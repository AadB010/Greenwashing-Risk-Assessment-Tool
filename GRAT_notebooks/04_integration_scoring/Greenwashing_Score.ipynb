{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a5d5b8",
   "metadata": {},
   "source": [
    "# Greenwashing Risk Score Calculation and Ensemble Analysis\n",
    "\n",
    "## Overview\n",
    "This module integrates all performance and communication components into the final greenwashing risk assessment using ensemble methodology. It calculates the Performance-Communication Gap (PCG) that defines classic greenwashing and applies systematic weight combinations to ensure robust scoring across methodological uncertainties.\n",
    "\n",
    "## Component Integration Process\n",
    "\n",
    "### Communication Dimension Calculation\n",
    "1. **Combined Sentiment Score**: Weighted combination (60% general environmental sentiment, 20% renewable energy sentiment, 20% climate emissions sentiment)\n",
    "2. **Combined Green Terms Score**: Weighted combination (70% term frequency, 30% vocabulary diversity)  \n",
    "3. **Green Communication Intensity**: Final combination (40% green terms, 60% sentiment)\n",
    "4. **Other Dimensions**: Calculated from NLP outputs:\n",
    "   - Substantiation Weakness: Quantification + Evidence + Aspirational intensities\n",
    "   - Language Vagueness: Vague + Hedge word intensities\n",
    "   - Temporal Orientation: Future focus + Timeline specificity\n",
    "   - Reporting Consistency: Cross-year similarity + High similarity ratios\n",
    "\n",
    "### Performance-Communication Gap Calculation\n",
    "- **Absolute Gap**: Green Communication Score - Performance Score\n",
    "- **Classic Greenwashing Detection**: Companies with above-median communication AND below-median performance\n",
    "- **Amplification Factor**: 1.5x multiplier applied to classic greenwashing pattern to reflect severity\n",
    "- **Year-wise Normalization**: Scores normalized within each year (2021, 2022) for temporal consistency\n",
    "\n",
    "## Ensemble Methodology\n",
    "- **Weight Constraints**: Individual weights 0.05-0.50, hierarchical ordering: Greenwashing Score > Substantiation Weakness > other components\n",
    "- **Valid Combinations**: 59,881 weight combinations meeting theoretical constraints\n",
    "- **Statistical Output**: Mean, median, standard deviation, quartiles, and range for each company-year observation\n",
    "- **Robustness**: Multiple weight scenarios ensure results aren't dependent on single weight specification\n",
    "\n",
    "## Key Output Variables\n",
    "- **Performance_Communication_Gap_Score**: Primary greenwashing risk measure (PCG)\n",
    "- **Component scores**: Individual dimension scores (0-100 scale)\n",
    "- **Ensemble statistics**: Complete uncertainty measures across all weight combinations\n",
    "- **Company rankings**: Relative positioning for both years\n",
    "\n",
    "## Data Integration\n",
    "Creates comprehensive master dataset combining performance scores (from Performance_Score_Calculations.ipynb), communication analyses (from all 5 NLP modules), and ensemble results with complete variable preservation for validation and further analysis.\n",
    "\n",
    "## Theoretical Foundation\n",
    "Implements the PCG approach where high environmental communication paired with poor environmental performance indicates greenwashing risk, with amplification factors revealing imbalances as recommended by composite index best practices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a62b08",
   "metadata": {},
   "source": [
    "## Loading and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load all Excel files\n",
    "df_density = pd.read_excel('data/NLP/Results/Communication_Score_df_Density.xlsx')\n",
    "df_context = pd.read_excel('data/NLP/Results/Communication_Score_df_Context.xlsx')\n",
    "df_similarity = pd.read_excel('data/NLP/Results/Similarity/similarity_analysis_results.xlsx')\n",
    "df_sentiment = pd.read_excel('data/NLP/Results/Overall_Sentiment_Analysis.xlsx')\n",
    "df_hedge_vague = pd.read_excel('data/NLP/Results/Communication_Score_df_Hedge_Vague.xlsx')\n",
    "df_topics = pd.read_excel('data/NLP/Results/Communication_Score_df_Topics.xlsx')\n",
    "df_topic_sentiment = pd.read_excel('data/NLP/Results/Topic_Weighted_Sentiment_Analysis.xlsx')\n",
    "\n",
    "# For each df print the name of the first column\n",
    "dataframes = [df_density, df_context, df_similarity, df_sentiment, df_hedge_vague, df_topics, df_topic_sentiment]\n",
    "for df in dataframes:\n",
    "    first_col = df.columns[0]\n",
    "    print(f\"First column: {first_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092c29e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant metrics from each dataframe\n",
    "density_metrics = df_density[['organization', 'year', 'gt_freq_pct', 'unique_gt_relative']].copy()\n",
    "\n",
    "context_metrics = df_context[['organization', 'year', 'temporal_past_pct', 'temporal_present_pct', \n",
    "                             'temporal_future_pct', 'quantification_intensity_score', \n",
    "                             'evidence_intensity_score', 'aspirational_intensity_score']].copy()\n",
    "\n",
    "similarity_metrics = df_similarity[['Company', 'TFIDF_Doc', 'Jaccard', 'SpaCy_HighSim_Ratio', 'SpaCy_Avg_Similarity']].copy()\n",
    "similarity_metrics.rename(columns={'Company': 'organization'}, inplace=True)\n",
    "\n",
    "sentiment_metrics = df_sentiment[['organization', 'year', 'avg_sentiment_score', 'sentiment_confidence', 'opportunity_ratio', 'risk_ratio'\n",
    "]].copy()\n",
    "\n",
    "hedge_vague_metrics = df_hedge_vague[['organization', 'year', 'hedge_intensity_score', \n",
    "                                     'vague_intensity_score', 'commitment_timeline_pct', 'total_unclear_density',\n",
    "                                     'combined_intensity_score']].copy()\n",
    "\n",
    "topics_metrics = df_topics[['organization', 'year', 'renewable_energy_density', 'climate_emissions_density']].copy()\n",
    "\n",
    "topic_sentiment_metrics = df_topic_sentiment[['organization', 'year', 'renewable_energy_avg_sentiment', \n",
    "                                             'climate_emissions_avg_sentiment']].copy()\n",
    "\n",
    "# Standardize organization names by replacing underscores with spaces\n",
    "density_metrics['organization'] = density_metrics['organization'].str.replace('_', ' ')\n",
    "context_metrics['organization'] = context_metrics['organization'].str.replace('_', ' ')\n",
    "sentiment_metrics['organization'] = sentiment_metrics['organization'].str.replace('_', ' ')\n",
    "hedge_vague_metrics['organization'] = hedge_vague_metrics['organization'].str.replace('_', ' ')\n",
    "topics_metrics['organization'] = topics_metrics['organization'].str.replace('_', ' ')\n",
    "topic_sentiment_metrics['organization'] = topic_sentiment_metrics['organization'].str.replace('_', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8eec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all dataframes\n",
    "df = density_metrics.merge(context_metrics, on=['organization', 'year'], how='outer')\n",
    "df = df.merge(sentiment_metrics, on=['organization', 'year'], how='outer')\n",
    "df = df.merge(hedge_vague_metrics, on=['organization', 'year'], how='outer')\n",
    "df = df.merge(topics_metrics, on=['organization', 'year'], how='outer')\n",
    "df = df.merge(topic_sentiment_metrics, on=['organization', 'year'], how='outer')\n",
    "df = df.merge(similarity_metrics, on='organization', how='outer')\n",
    "\n",
    "# Rename organization column to Organization\n",
    "df.rename(columns={'organization': 'Organization'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04bae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract first word or apply exceptions\n",
    "def simplify_org_name(name):\n",
    "    if name == 'Polska Grupa Energetyczna PGE SA':\n",
    "        return 'PGE'\n",
    "    elif name == 'AKENERJİ ELEKTRİK ÜRETİM A.Ş.':\n",
    "        return 'Akenerji'\n",
    "    else:\n",
    "        return name.split()[0]\n",
    "\n",
    "# Apply the function to the 'Organization' column\n",
    "df.loc[:, 'Organization'] = df['Organization'].apply(simplify_org_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b7716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values and show which metrics and organizations have them\n",
    "print(\"MISSING DATA ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get columns with NaN values\n",
    "cols_with_nan = df.columns[df.isnull().any()].tolist()\n",
    "\n",
    "if cols_with_nan:\n",
    "    print(f\"Metrics with NaN values: {cols_with_nan}\")\n",
    "    print()\n",
    "    \n",
    "    for col in cols_with_nan:\n",
    "        nan_rows = df[df[col].isnull()]\n",
    "        if not nan_rows.empty:\n",
    "            print(f\"Metric: {col}\")\n",
    "            print(f\"Organizations with missing data:\")\n",
    "            for _, row in nan_rows.iterrows():\n",
    "                if 'year' in df.columns:\n",
    "                    print(f\"  - {row['Organization']} ({row['year']})\")\n",
    "                else:\n",
    "                    print(f\"  - {row['Organization']}\")\n",
    "            print()\n",
    "else:\n",
    "    print(\"No NaN values found in the dataset.\")\n",
    "\n",
    "# Summary statistics\n",
    "total_cells = df.shape[0] * df.shape[1]\n",
    "nan_cells = df.isnull().sum().sum()\n",
    "print(f\"Total cells: {total_cells}\")\n",
    "print(f\"NaN cells: {nan_cells}\")\n",
    "print(f\"Missing data percentage: {(nan_cells/total_cells)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2edaad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned Excel with new structure\n",
    "ensemble_perf = pd.read_excel('data/Performance/ensemble_performance_scores.xlsx')\n",
    "\n",
    "# Confirm actual column names\n",
    "print(\"Columns:\", ensemble_perf.columns)\n",
    "\n",
    "# Fix name consistency (Ørsted → Orsted)\n",
    "ensemble_perf['Organization'] = ensemble_perf['Organization'].replace('Ørsted', 'Orsted')\n",
    "\n",
    "# Rename median_score to Performance_Score (data is already in long format)\n",
    "ensemble_perf = ensemble_perf.rename(columns={'median_score': 'Performance_Score'})\n",
    "\n",
    "# Merge with your main DataFrame\n",
    "df = df.merge(ensemble_perf[['Organization', 'year', 'Performance_Score']], on=['Organization', 'year'], how='left')\n",
    "\n",
    "# Diagnostics\n",
    "print(\"Performance scores added to df\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Performance scores available: {df['Performance_Score'].notna().sum()}/{len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c798bc",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a8ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "\n",
    "# Save df in Exscel in file path: data/Greenwashing Results/df.xlsx\n",
    "output_path = 'data/Greenwashing Results/df.xlsx'\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "df.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13136686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize scores per year (0-100 scale)\n",
    "def normalize_by_year(df, column):\n",
    "    normalized_col = f\"{column}\"\n",
    "    df[normalized_col] = df.groupby('year')[column].transform(\n",
    "        lambda x: (x - x.min()) / (x.max() - x.min()) * 100 if x.max() != x.min() else 50\n",
    "    )\n",
    "    return normalized_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11811a6b",
   "metadata": {},
   "source": [
    "## Aditional metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0047b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize similarity values\n",
    "normalize_by_year(df, 'TFIDF_Doc')\n",
    "normalize_by_year(df, 'Jaccard')\n",
    "normalize_by_year(df, 'SpaCy_Avg_Similarity')\n",
    "\n",
    "# Create similarity combined score (average of three similarity metrics)\n",
    "df['similarity_combined'] = (\n",
    "    (df['TFIDF_Doc'] + \n",
    "     df['Jaccard'] + \n",
    "     df['SpaCy_Avg_Similarity']) / 3\n",
    ").round(2)\n",
    "\n",
    "\n",
    "# Calculate an other additional metric\n",
    "# Add future vs past+present ratio\n",
    "df['future_vs_past_present_ratio'] = (\n",
    "    df['temporal_future_pct'] / \n",
    "    (df['temporal_past_pct'] + df['temporal_present_pct'])\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed8d1be",
   "metadata": {},
   "source": [
    "## Create df used for greenwashing score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a1d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of df\n",
    "greenwashing_df = df.copy()\n",
    "\n",
    "# Drop specified columns\n",
    "columns_to_drop = [\n",
    "    'temporal_past_pct', 'temporal_present_pct', 'temporal_future_pct',\n",
    "    'TFIDF_Doc', 'Jaccard', 'SpaCy_Avg_Similarity',\n",
    "    'sentiment_confidence', 'opportunity_ratio', 'risk_ratio',\n",
    "    'total_unclear_density', 'renewable_energy_density', 'climate_emissions_density'\n",
    "]\n",
    "greenwashing_df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# Normalize all numeric columns except 'year' and 'Performance_Score'\n",
    "numeric_cols = greenwashing_df.select_dtypes(include='number').columns\n",
    "numeric_cols = [col for col in numeric_cols if col != 'year'] # was: ...if col not in ['year', 'Performance_Score'] OR: if col != 'year'\n",
    "\n",
    "for col in numeric_cols:\n",
    "    normalize_by_year(greenwashing_df, col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbe7b76",
   "metadata": {},
   "source": [
    "# Calculation of Greenwashing Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b67e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined sentiment score\n",
    "\n",
    "# Calculate combined sentiment score\n",
    "greenwashing_df['combined_sentiment_score'] = (\n",
    "    0.6 * greenwashing_df['avg_sentiment_score'] +     \n",
    "    0.2 * greenwashing_df['renewable_energy_avg_sentiment'] +\n",
    "    0.2 * greenwashing_df['climate_emissions_avg_sentiment']\n",
    ").round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6fe2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined green term score\n",
    "\n",
    "# Calculate combined sentiment score\n",
    "greenwashing_df['combined_green_score'] = (\n",
    "    0.7 * greenwashing_df['gt_freq_pct'] +     \n",
    "    0.3 * greenwashing_df['unique_gt_relative']\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate communication score\n",
    "greenwashing_df['Green_Com_Score'] = (\n",
    "    0.4 * greenwashing_df['combined_green_score'] +\n",
    "    0.6 * greenwashing_df['combined_sentiment_score']\n",
    ").round(2)\n",
    "\n",
    "# Normalize Green_Com_Score per year to 0–100 scale\n",
    "normalize_by_year(greenwashing_df, 'Green_Com_Score')\n",
    "\n",
    "print(\"Communication score created\")\n",
    "print(f\"Mean score: {greenwashing_df['Green_Com_Score'].mean():.2f}\")\n",
    "print(f\"Score range: {greenwashing_df['Green_Com_Score'].min():.2f} - {greenwashing_df['Green_Com_Score'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da0e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute gap between performance and communication\n",
    "greenwashing_df['Greenwashing_Risk_Abs'] = (\n",
    "    greenwashing_df['Green_Com_Score'] - greenwashing_df['Performance_Score']\n",
    ").round(2)\n",
    "\n",
    "# Calculate yearly medians for classic greenwashing pattern\n",
    "yearly_medians = greenwashing_df.groupby('year').agg({\n",
    "    'Performance_Score': 'median',\n",
    "    'Green_Com_Score': 'median'\n",
    "})\n",
    "\n",
    "# Identify classic greenwashing pattern\n",
    "greenwashing_df['Classic_Greenwashing'] = greenwashing_df.apply(\n",
    "    lambda row: (row['Green_Com_Score'] > yearly_medians.loc[row['year'], 'Green_Com_Score']) and \n",
    "                (row['Performance_Score'] < yearly_medians.loc[row['year'], 'Performance_Score']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Apply 1.5x amplifier for classic greenwashing\n",
    "greenwashing_df['Amplified_Score'] = greenwashing_df.apply(\n",
    "    lambda row: row['Greenwashing_Risk_Abs'] * 1.5 if row['Classic_Greenwashing'] else row['Greenwashing_Risk_Abs'],\n",
    "    axis=1\n",
    ").round(2)\n",
    "\n",
    "# Normalize to 0–100 scale scale within each year\n",
    "normalize_by_year(greenwashing_df, 'Amplified_Score')\n",
    "greenwashing_df['Greenwashing_Score'] = greenwashing_df['Amplified_Score'].round(2)\n",
    "\n",
    "# Clean up intermediate columns\n",
    "greenwashing_df = greenwashing_df.drop(['Classic_Greenwashing'], axis=1) # kept 'Base_Hybrid_Score', 'Amplified_Score'\n",
    "\n",
    "print(\"Greenwashing score calculated\")\n",
    "print(f\"Mean score: {greenwashing_df['Greenwashing_Score'].mean():.2f}\")\n",
    "print(f\"Companies with classic greenwashing pattern: {greenwashing_df.apply(lambda row: (row['Green_Com_Score'] > yearly_medians.loc[row['year'], 'Green_Com_Score']) and (row['Performance_Score'] < yearly_medians.loc[row['year'], 'Performance_Score']), axis=1).sum()}\")\n",
    "\n",
    "# 2021 highest scores\n",
    "print(f\"\\n2021 HIGHEST GREENWASHING SCORES:\")\n",
    "highest_2021 = greenwashing_df[greenwashing_df['year'] == 2021].nlargest(10, 'Greenwashing_Score')[['Organization', 'Greenwashing_Score']]\n",
    "print(highest_2021.to_string(index=False))\n",
    "\n",
    "# 2022 highest scores  \n",
    "print(f\"\\n2022 HIGHEST GREENWASHING SCORES:\")\n",
    "highest_2022 = greenwashing_df[greenwashing_df['year'] == 2022].nlargest(10, 'Greenwashing_Score')[['Organization', 'Greenwashing_Score']]\n",
    "print(highest_2022.to_string(index=False))\n",
    "\n",
    "# Average scores per company across both years\n",
    "print(f\"\\nHIGHEST AVERAGE GREENWASHING SCORES (2021-2022):\")\n",
    "company_averages = greenwashing_df.groupby('Organization')['Greenwashing_Score'].agg(['mean', 'count']).reset_index()\n",
    "company_averages.columns = ['Organization', 'Avg_Greenwashing_Score', 'Years_Count']\n",
    "company_averages = company_averages[company_averages['Years_Count'] == 2]  # Only companies with data for both years\n",
    "highest_avg = company_averages.nlargest(10, 'Avg_Greenwashing_Score')[['Organization', 'Avg_Greenwashing_Score']]\n",
    "highest_avg['Avg_Greenwashing_Score'] = highest_avg['Avg_Greenwashing_Score'].round(2)\n",
    "print(highest_avg.to_string(index=False))\n",
    "\n",
    "print(f\"\\nCompanies with data for both years: {len(company_averages)}\")\n",
    "print(f\"Average greenwashing score across all companies (both years): {company_averages['Avg_Greenwashing_Score'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d1cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save performance communication gap data to Excel\n",
    "import os\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs('data/Greenwashing Results', exist_ok=True)\n",
    "\n",
    "# Save the dataframe to Excel\n",
    "greenwashing_df.to_excel('data/Greenwashing Results/performance_communication_gap.xlsx', \n",
    "                        index=False)\n",
    "\n",
    "print(\"Performance communication gap data saved to: data/Greenwashing Results/performance_communication_gap.xlsx\")\n",
    "print(f\"Saved {len(greenwashing_df)} rows and {len(greenwashing_df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d704c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greenwashing Score Quadrant Visualization\n",
    "# Communication Score vs Performance Score Analysis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Calculate medians for thresholds\n",
    "comm_median = greenwashing_df['Green_Com_Score'].median()\n",
    "perf_median = greenwashing_df['Performance_Score'].median()\n",
    "\n",
    "# Create quadrant classification\n",
    "def classify_quadrant(row):\n",
    "    comm_high = row['Green_Com_Score'] >= comm_median\n",
    "    perf_high = row['Performance_Score'] >= perf_median\n",
    "    \n",
    "    if comm_high and not perf_high:\n",
    "        return 'Potential_Greenwashing'\n",
    "    elif comm_high and perf_high:\n",
    "        return 'Green_Leaders'\n",
    "    elif not comm_high and not perf_high:\n",
    "        return 'Laggards'\n",
    "    else:\n",
    "        return 'Under_Communicators'\n",
    "\n",
    "greenwashing_df['Quadrant'] = greenwashing_df.apply(classify_quadrant, axis=1)\n",
    "\n",
    "# Define colors for quadrants\n",
    "colors = {'Potential_Greenwashing': 'red', 'Green_Leaders': 'green', \n",
    "          'Laggards': 'gray', 'Under_Communicators': 'blue'}\n",
    "\n",
    "# Filter data by year\n",
    "df_2021 = greenwashing_df[greenwashing_df['year'] == 2021]\n",
    "df_2022 = greenwashing_df[greenwashing_df['year'] == 2022]\n",
    "\n",
    "# Create the visualization function\n",
    "def create_quadrant_plot(data, year, ax):\n",
    "    \"\"\"Create a quadrant plot for the given year\"\"\"\n",
    "    \n",
    "    # Calculate year-specific medians\n",
    "    year_comm_median = data['Green_Com_Score'].median()\n",
    "    year_perf_median = data['Performance_Score'].median()\n",
    "    \n",
    "    # Plot each quadrant\n",
    "    for quadrant in data['Quadrant'].unique():\n",
    "        if pd.isna(quadrant):\n",
    "            continue\n",
    "        subset = data[data['Quadrant'] == quadrant]\n",
    "        ax.scatter(subset['Performance_Score'], subset['Green_Com_Score'], \n",
    "                  c=colors[quadrant], label=quadrant.replace('_', ' '), \n",
    "                  alpha=0.7, s=100, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    # Add company name annotations\n",
    "    for _, row in data.iterrows():\n",
    "        if pd.notna(row['Performance_Score']) and pd.notna(row['Green_Com_Score']):\n",
    "            ax.annotate(row['Organization'], \n",
    "                       (row['Performance_Score'], row['Green_Com_Score']),\n",
    "                       xytext=(5, 5), textcoords='offset points', \n",
    "                       fontsize=10, alpha=0.9, fontweight='bold')\n",
    "    \n",
    "    # Add median lines\n",
    "    ax.axvline(year_perf_median, color='gray', linestyle='--', alpha=0.5, linewidth=2)\n",
    "    ax.axhline(year_comm_median, color='gray', linestyle='--', alpha=0.5, linewidth=2)\n",
    "    \n",
    "    # Add quadrant labels\n",
    "    perf_range = ax.get_xlim()\n",
    "    comm_range = ax.get_ylim()\n",
    "    \n",
    "    # Potential Greenwashing (Top Left)\n",
    "    ax.text(perf_range[0] + (year_perf_median - perf_range[0])/2, \n",
    "            year_comm_median + (comm_range[1] - year_comm_median)/2,\n",
    "            'Potential\\nGreenwashing', ha='center', va='center', fontsize=11, \n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightcoral', alpha=0.7))\n",
    "    \n",
    "    # Green Leaders (Top Right)\n",
    "    ax.text(year_perf_median + (perf_range[1] - year_perf_median)/2, \n",
    "            year_comm_median + (comm_range[1] - year_comm_median)/2,\n",
    "            'Green\\nLeaders', ha='center', va='center', fontsize=11, \n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightgreen', alpha=0.7))\n",
    "    \n",
    "    # Laggards (Bottom Left)\n",
    "    ax.text(perf_range[0] + (year_perf_median - perf_range[0])/2, \n",
    "            comm_range[0] + (year_comm_median - comm_range[0])/2,\n",
    "            'Laggards', ha='center', va='center', fontsize=11, \n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightgray', alpha=0.7))\n",
    "    \n",
    "    # Under Communicators (Bottom Right)\n",
    "    ax.text(year_perf_median + (perf_range[1] - year_perf_median)/2, \n",
    "            comm_range[0] + (year_comm_median - comm_range[0])/2,\n",
    "            'Under\\nCommunicators', ha='center', va='center', fontsize=11, \n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightblue', alpha=0.7))\n",
    "    \n",
    "    # Customize axes\n",
    "    ax.set_xlabel('Performance Score', fontsize=14)\n",
    "    ax.set_ylabel('Communication Score', fontsize=14)\n",
    "    ax.set_title(f'Greenwashing Detection {year}', fontsize=16, fontweight='bold')\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(0, 1))\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(24, 10))\n",
    "\n",
    "# FIRST GRAPH: 2021\n",
    "if len(df_2021) > 0:\n",
    "    create_quadrant_plot(df_2021, 2021, axes[0])\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, 'No data available for 2021', \n",
    "                ha='center', va='center', transform=axes[0].transAxes, fontsize=14)\n",
    "    axes[0].set_title('Greenwashing Detection 2021', fontsize=16, fontweight='bold')\n",
    "\n",
    "# SECOND GRAPH: 2022\n",
    "if len(df_2022) > 0:\n",
    "    create_quadrant_plot(df_2022, 2022, axes[1])\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No data available for 2022', \n",
    "                ha='center', va='center', transform=axes[1].transAxes, fontsize=14)\n",
    "    axes[1].set_title('Greenwashing Detection 2022', fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed quadrant analysis\n",
    "print(\"GREENWASHING QUADRANT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for year in [2021, 2022]:\n",
    "    year_data = greenwashing_df[greenwashing_df['year'] == year]\n",
    "    if len(year_data) > 0:\n",
    "        print(f\"\\n{year} QUADRANT DISTRIBUTION:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        quadrant_counts = year_data['Quadrant'].value_counts()\n",
    "        for quadrant, count in quadrant_counts.items():\n",
    "            if pd.notna(quadrant):\n",
    "                print(f\"{quadrant.replace('_', ' ')}: {count} companies\")\n",
    "        \n",
    "        print(f\"\\nMedian Communication Score: {year_data['Green_Com_Score'].median():.2f}\")\n",
    "        print(f\"Median Performance Score: {year_data['Performance_Score'].median():.2f}\")\n",
    "        \n",
    "        # Show companies in each quadrant with their scores\n",
    "        print(f\"\\nDETAILED BREAKDOWN ({year}):\")\n",
    "        for quadrant in ['Potential_Greenwashing', 'Green_Leaders', 'Laggards', 'Under_Communicators']:\n",
    "            quad_data = year_data[year_data['Quadrant'] == quadrant]\n",
    "            if len(quad_data) > 0:\n",
    "                print(f\"\\n{quadrant.replace('_', ' ')} ({len(quad_data)} companies):\")\n",
    "                for _, row in quad_data.iterrows():\n",
    "                    print(f\"  {row['Organization']}: Com={row['Green_Com_Score']:.1f}, Perf={row['Performance_Score']:.1f}, Risk={row['Greenwashing_Score']:.1f}\")\n",
    "\n",
    "# Analysis of companies that changed quadrants\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"QUADRANT MOVEMENT ANALYSIS (2021 → 2022)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "companies_both_years = set(df_2021['Organization'].unique()) & set(df_2022['Organization'].unique())\n",
    "\n",
    "movements = []\n",
    "for company in companies_both_years:\n",
    "    quad_2021 = df_2021[df_2021['Organization'] == company]['Quadrant'].iloc[0]\n",
    "    quad_2022 = df_2022[df_2022['Organization'] == company]['Quadrant'].iloc[0]\n",
    "    \n",
    "    if quad_2021 != quad_2022:\n",
    "        movements.append({\n",
    "            'Company': company,\n",
    "            'From': quad_2021,\n",
    "            'To': quad_2022\n",
    "        })\n",
    "\n",
    "if movements:\n",
    "    print(f\"\\nCompanies that changed quadrants: {len(movements)}\")\n",
    "    for movement in movements:\n",
    "        print(f\"{movement['Company']}: {movement['From'].replace('_', ' ')} → {movement['To'].replace('_', ' ')}\")\n",
    "else:\n",
    "    print(\"\\nNo companies changed quadrants between 2021 and 2022\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"Overall median Communication Score: {greenwashing_df['Green_Com_Score'].median():.2f}\")\n",
    "print(f\"Overall median Performance Score: {greenwashing_df['Performance_Score'].median():.2f}\")\n",
    "print(f\"Overall median Greenwashing Score: {greenwashing_df['Greenwashing_Score'].median():.2f}\")\n",
    "\n",
    "print(f\"\\nCorrelation between Communication and Performance: {greenwashing_df['Green_Com_Score'].corr(greenwashing_df['Performance_Score']):.3f}\")\n",
    "print(f\"Correlation between Communication and Greenwashing Risk: {greenwashing_df['Green_Com_Score'].corr(greenwashing_df['Greenwashing_Score']):.3f}\")\n",
    "print(f\"Correlation between Performance and Greenwashing Risk: {greenwashing_df['Performance_Score'].corr(greenwashing_df['Greenwashing_Score']):.3f}\")\n",
    "\n",
    "# Print companies with highest risk scores in each quadrant\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"HIGHEST RISK COMPANIES BY QUADRANT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for quadrant in ['Potential_Greenwashing', 'Green_Leaders', 'Laggards', 'Under_Communicators']:\n",
    "    quad_data = greenwashing_df[greenwashing_df['Quadrant'] == quadrant]\n",
    "    if len(quad_data) > 0:\n",
    "        top_risk = quad_data.nlargest(3, 'Greenwashing_Score')\n",
    "        print(f\"\\n{quadrant.replace('_', ' ')} - Top Risk:\")\n",
    "        for _, row in top_risk.iterrows():\n",
    "            print(f\"  {row['Organization']} ({row['year']}): Risk={row['Greenwashing_Score']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde79db2",
   "metadata": {},
   "source": [
    "### Additional components for greenwashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e245100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# COMPONENT 1: SUBSTANTIATION WEAKNESS SCORE\n",
    "# Quantification (30%) + Evidence (35%) + Aspirational (35%)\n",
    "# Higher scores = higher greenwashing risk\n",
    "# ==========================================\n",
    "\n",
    "greenwashing_df['Substantiation_Weakness'] = (\n",
    "    0.30 * (100 - greenwashing_df['quantification_intensity_score']) +     # Reversed: higher quantification = lower risk\n",
    "    0.35 * (100 - greenwashing_df['evidence_intensity_score']) +          # Reversed: higher evidence = lower risk  \n",
    "    0.35 * greenwashing_df['aspirational_intensity_score']                # Normal: higher aspirational = higher risk\n",
    ").round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b1d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPONENT 2: LANGUAGE VAGUENESS SCORE\n",
    "# Vague (70%) + Hedge (30%)\n",
    "# Higher scores = higher greenwashing risk\n",
    "# ==========================================\n",
    "\n",
    "greenwashing_df['Language_Vagueness'] = (\n",
    "    0.70 * greenwashing_df['vague_intensity_score'] +                     # Normal: higher vagueness = higher risk\n",
    "    0.30 * (100 - greenwashing_df['hedge_intensity_score'])               # Reversed: higher hedging = lower risk (per UCLA study)\n",
    ").round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdb9253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPONENT 3: TEMPORAL ORIENTATION SCORE  \n",
    "# Future orientation (60%) + Timeline specificity (40%)\n",
    "# Higher scores = higher greenwashing risk\n",
    "# ==========================================\n",
    "\n",
    "greenwashing_df['Temporal_Orientation'] = (\n",
    "    0.60 * greenwashing_df['future_vs_past_present_ratio'] +             # Normal: higher future focus = higher risk\n",
    "    0.40 * (100 - greenwashing_df['commitment_timeline_pct'])            # Reversed: more specific timelines = lower risk\n",
    ").round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c111883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPONENT 4: REPORTING CONSISTENCY SCORE\n",
    "# Overall similarity (70%) + High similarity ratio (30%)  \n",
    "# Higher scores = higher greenwashing risk\n",
    "# ==========================================\n",
    "\n",
    "greenwashing_df['Reporting_Consistency'] = (\n",
    "    0.70 * greenwashing_df['similarity_combined'] +                      # Normal: higher similarity = higher risk\n",
    "    0.30 * greenwashing_df['SpaCy_HighSim_Ratio']                       # Normal: more identical sentences = higher risk\n",
    ").round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a2122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARDIZE EACH COMPONENT BY YEAR\n",
    "# ==========================================\n",
    "\n",
    "normalize_by_year(greenwashing_df, 'Substantiation_Weakness')\n",
    "normalize_by_year(greenwashing_df, 'Language_Vagueness')\n",
    "normalize_by_year(greenwashing_df, 'Temporal_Orientation')\n",
    "normalize_by_year(greenwashing_df, 'Reporting_Consistency')\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# SUMMARY STATISTICS BY YEAR\n",
    "# ==========================================\n",
    "\n",
    "print(\"GREENWASHING COMPONENTS SUMMARY BY YEAR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Component 1: Substantiation Weakness\n",
    "print(f\"\\nSUBSTANTIATION WEAKNESS\")\n",
    "for year in [2021, 2022]:\n",
    "    year_data = greenwashing_df[greenwashing_df['year'] == year]\n",
    "    print(f\"\\n{year}:\")\n",
    "    print(f\"  Mean Score: {year_data['Substantiation_Weakness'].mean():.2f}\")\n",
    "    print(f\"  Highest Risk Companies:\")\n",
    "    top_subst = year_data.nlargest(5, 'Substantiation_Weakness')[['Organization', 'Substantiation_Weakness']]\n",
    "    for i, (idx, row) in enumerate(top_subst.iterrows(), 1):\n",
    "        print(f\"    {i}. {row['Organization']}: {row['Substantiation_Weakness']:.2f}\")\n",
    "\n",
    "# Component 2: Language Vagueness\n",
    "print(f\"\\nLANGUAGE VAGUENESS\")\n",
    "for year in [2021, 2022]:\n",
    "    year_data = greenwashing_df[greenwashing_df['year'] == year]\n",
    "    print(f\"\\n{year}:\")\n",
    "    print(f\"  Mean Score: {year_data['Language_Vagueness'].mean():.2f}\")\n",
    "    print(f\"  Highest Risk Companies:\")\n",
    "    top_lang = year_data.nlargest(5, 'Language_Vagueness')[['Organization', 'Language_Vagueness']]\n",
    "    for i, (idx, row) in enumerate(top_lang.iterrows(), 1):\n",
    "        print(f\"    {i}. {row['Organization']}: {row['Language_Vagueness']:.2f}\")\n",
    "\n",
    "# Component 3: Temporal Orientation  \n",
    "print(f\"\\nTEMPORAL ORIENTATION\")\n",
    "for year in [2021, 2022]:\n",
    "    year_data = greenwashing_df[greenwashing_df['year'] == year]\n",
    "    print(f\"\\n{year}:\")\n",
    "    print(f\"  Mean Score: {year_data['Temporal_Orientation'].mean():.2f}\")\n",
    "    print(f\"  Highest Risk Companies:\")\n",
    "    top_temp = year_data.nlargest(5, 'Temporal_Orientation')[['Organization', 'Temporal_Orientation']]\n",
    "    for i, (idx, row) in enumerate(top_temp.iterrows(), 1):\n",
    "        print(f\"    {i}. {row['Organization']}: {row['Temporal_Orientation']:.2f}\")\n",
    "\n",
    "# Component 4: Reporting Consistency\n",
    "print(f\"\\nREPORTING CONSISTENCY\")\n",
    "for year in [2021, 2022]:\n",
    "    year_data = greenwashing_df[greenwashing_df['year'] == year]\n",
    "    print(f\"\\n{year}:\")\n",
    "    print(f\"  Mean Score: {year_data['Reporting_Consistency'].mean():.2f}\")\n",
    "    print(f\"  Highest Risk Companies:\")\n",
    "    top_cons = year_data.nlargest(5, 'Reporting_Consistency')[['Organization', 'Reporting_Consistency']]\n",
    "    for i, (idx, row) in enumerate(top_cons.iterrows(), 1):\n",
    "        print(f\"    {i}. {row['Organization']}: {row['Reporting_Consistency']:.2f}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"Component calculations and standardization complete!\")\n",
    "print(\"Higher scores indicate higher greenwashing risk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735cf78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "greenwashing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a2839",
   "metadata": {},
   "source": [
    "### Export and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffec87ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Create Comprehensive Greenwashing Results Export\n",
    "\n",
    "print(\"CREATING COMPREHENSIVE GREENWASHING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: Create comprehensive results DataFrame with all variables\n",
    "# ==========================================\n",
    "\n",
    "print(\"Preparing comprehensive greenwashing dataset...\")\n",
    "\n",
    "# Create comprehensive dataset with all variables and clear naming\n",
    "comprehensive_greenwashing = pd.DataFrame({\n",
    "    # ============= IDENTIFICATION =============\n",
    "    'Company': greenwashing_df['Organization'],\n",
    "    'Year': greenwashing_df['year'],\n",
    "    \n",
    "    # ============= MAIN SCORES =============\n",
    "    'Performance_Communication_Gap_Score': greenwashing_df['Greenwashing_Score'],\n",
    "    'Performance_Score': greenwashing_df['Performance_Score'], \n",
    "    'Green_Communication_Score': greenwashing_df['Green_Com_Score'],\n",
    "    'Performance_Communication_Absolute_Gap': greenwashing_df['Greenwashing_Risk_Abs'],\n",
    "    # 'Performance_Communication_Gap_Amplified': greenwashing_df['Amplified_Score'],\n",
    "    \n",
    "    # ============= COMPONENT SCORES (0-100 each) =============\n",
    "    'Component_Substantiation_Weakness_Score': greenwashing_df['Substantiation_Weakness'],\n",
    "    'Component_Language_Vagueness_Score': greenwashing_df['Language_Vagueness'], \n",
    "    'Component_Temporal_Orientation_Score': greenwashing_df['Temporal_Orientation'],\n",
    "    'Component_Reporting_Consistency_Score': greenwashing_df['Reporting_Consistency'],\n",
    "    \n",
    "    # ============= GREEN TERM ANALYSIS =============\n",
    "    'Green_Terms_Frequency_Pct': df['gt_freq_pct'],\n",
    "    'Green_Terms_Unique_Relative': df['unique_gt_relative'],\n",
    "    'Green_Terms_Combined_Score': greenwashing_df['combined_green_score'],\n",
    "    \n",
    "    # ============= SENTIMENT ANALYSIS =============\n",
    "    'Overall_Sentiment_Score': df['avg_sentiment_score'],\n",
    "    'Renewable_Energy_Sentiment': df['renewable_energy_avg_sentiment'],\n",
    "    'Climate_Emissions_Sentiment': df['climate_emissions_avg_sentiment'],\n",
    "    'Combined_Sentiment_Score': greenwashing_df['combined_sentiment_score'],\n",
    "    \n",
    "    # ============= CONTEXT & SUBSTANTIATION =============\n",
    "    # Quantification, Evidence, Aspirational (used in Substantiation Weakness)\n",
    "    'Quantification_Intensity_Score': df['quantification_intensity_score'],\n",
    "    'Evidence_Intensity_Score': df['evidence_intensity_score'], \n",
    "    'Aspirational_Intensity_Score': df['aspirational_intensity_score'],\n",
    "    \n",
    "    # ============= LANGUAGE VAGUENESS METRICS =============\n",
    "    # Hedging, Vague language (used in Language Vagueness)\n",
    "    'Hedge_Intensity_Score': df['hedge_intensity_score'],\n",
    "    'Vague_Intensity_Score': df['vague_intensity_score'],\n",
    "    'Combined_Unclear_Intensity_Score': greenwashing_df['combined_intensity_score'],\n",
    "    'Commitment_Timeline_Pct': df['commitment_timeline_pct'],\n",
    "    \n",
    "    # ============= TEMPORAL ORIENTATION METRICS =============\n",
    "    # Future vs past/present focus (used in Temporal Orientation)\n",
    "    'Future_vs_Past_Present_Ratio': df['future_vs_past_present_ratio'],\n",
    "    \n",
    "    # ============= SIMILARITY/CONSISTENCY METRICS =============\n",
    "    # Document similarity (used in Reporting Consistency)\n",
    "    'TFIDF_Document_Similarity': df['TFIDF_Doc'],\n",
    "    'Jaccard_Similarity': df['Jaccard'],\n",
    "    'SpaCy_Average_Similarity': df['SpaCy_Avg_Similarity'],\n",
    "    'SpaCy_High_Similarity_Ratio': df['SpaCy_HighSim_Ratio'],\n",
    "    'Similarity_Combined_Score': greenwashing_df['similarity_combined'],\n",
    "})\n",
    "\n",
    "# Round all numeric columns\n",
    "numeric_columns = comprehensive_greenwashing.select_dtypes(include=[np.number]).columns\n",
    "comprehensive_greenwashing[numeric_columns] = comprehensive_greenwashing[numeric_columns].round(3)\n",
    "\n",
    "# Sort by Company and Year\n",
    "comprehensive_greenwashing = comprehensive_greenwashing.sort_values(['Company', 'Year']).reset_index(drop=True)\n",
    "\n",
    "print(f\"✓ Comprehensive dataset created: {comprehensive_greenwashing.shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: Create component breakdown analysis\n",
    "# ==========================================\n",
    "\n",
    "print(\"Creating component breakdown analysis...\")\n",
    "\n",
    "# Component weights used in calculations (for documentation)\n",
    "component_weights = pd.DataFrame({\n",
    "    'Component': [\n",
    "        'Substantiation Weakness - Quantification',\n",
    "        'Substantiation Weakness - Evidence', \n",
    "        'Substantiation Weakness - Aspirational',\n",
    "        'Language Vagueness - Vague Language',\n",
    "        'Language Vagueness - Hedge Language',\n",
    "        'Temporal Orientation - Future vs Past/Present',\n",
    "        'Temporal Orientation - Timeline Specificity',\n",
    "        'Reporting Consistency - Overall Similarity',\n",
    "        'Reporting Consistency - High Similarity Ratio',\n",
    "        'Green Communication - Green Terms',\n",
    "        'Green Communication - Sentiment'\n",
    "    ],\n",
    "    'Weight': [0.30, 0.35, 0.35, 0.70, 0.30, 0.60, 0.40, 0.70, 0.30, 0.40, 0.60],\n",
    "    'Direction': [\n",
    "        'Reversed (higher quantification = lower risk)',\n",
    "        'Reversed (higher evidence = lower risk)',\n",
    "        'Normal (higher aspirational = higher risk)',\n",
    "        'Normal (higher vagueness = higher risk)',\n",
    "        'Reversed (higher hedging = lower risk)',\n",
    "        'Normal (higher future focus = higher risk)',\n",
    "        'Reversed (more specific timelines = lower risk)',\n",
    "        'Normal (higher similarity = higher risk)',\n",
    "        'Normal (more identical sentences = higher risk)',\n",
    "        'Normal (higher green terms = higher communication)',\n",
    "        'Normal (higher sentiment = higher communication)'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: Create summary statistics by year\n",
    "# ==========================================\n",
    "\n",
    "print(\"Calculating summary statistics by year...\")\n",
    "\n",
    "summary_stats_2021 = comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2021].describe()\n",
    "summary_stats_2022 = comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2022].describe()\n",
    "\n",
    "# Component score means by year\n",
    "component_summary = pd.DataFrame({\n",
    "    'Component': [\n",
    "        'Greenwashing Risk Score',\n",
    "        'Green Communication Score', \n",
    "        'Substantiation Weakness',\n",
    "        'Language Vagueness',\n",
    "        'Temporal Orientation',\n",
    "        'Reporting Consistency'\n",
    "    ],\n",
    "    'Mean_2021': [\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2021]['Performance_Communication_Gap_Score'].mean(),\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2021]['Green_Communication_Score'].mean(),\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2021]['Component_Substantiation_Weakness_Score'].mean(),\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2021]['Component_Language_Vagueness_Score'].mean(),\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2021]['Component_Temporal_Orientation_Score'].mean(),\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2021]['Component_Reporting_Consistency_Score'].mean()\n",
    "    ],\n",
    "    'Mean_2022': [\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2022]['Performance_Communication_Gap_Score'].mean(),\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2022]['Green_Communication_Score'].mean(),\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2022]['Component_Substantiation_Weakness_Score'].mean(),\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2022]['Component_Language_Vagueness_Score'].mean(),\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2022]['Component_Temporal_Orientation_Score'].mean(),\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2022]['Component_Reporting_Consistency_Score'].mean()\n",
    "    ]\n",
    "}).round(2)\n",
    "\n",
    "# ==========================================\n",
    "# STEP 4: Export to Excel with multiple tabs\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\nExporting comprehensive greenwashing results to Excel...\")\n",
    "output_path = \"data/Greenwashing Results/comprehensive_greenwashing_results.xlsx\"\n",
    "\n",
    "try:\n",
    "    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "        # Main comprehensive dataset\n",
    "        comprehensive_greenwashing.to_excel(writer, sheet_name='Comprehensive_Results', index=False)\n",
    "        \n",
    "        # Separate years for easier analysis\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2021].to_excel(\n",
    "            writer, sheet_name='Results_2021', index=False)\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2022].to_excel(\n",
    "            writer, sheet_name='Results_2022', index=False)\n",
    "        \n",
    "        # Component weights and methodology\n",
    "        component_weights.to_excel(writer, sheet_name='Component_Methodology', index=False)\n",
    "        \n",
    "        # Component summary statistics\n",
    "        component_summary.to_excel(writer, sheet_name='Component_Summary', index=False)\n",
    "        \n",
    "        # Detailed statistics by year\n",
    "        summary_stats_2021.to_excel(writer, sheet_name='Stats_2021')\n",
    "        summary_stats_2022.to_excel(writer, sheet_name='Stats_2022')\n",
    "        \n",
    "        # Data dictionary\n",
    "        data_dict = pd.DataFrame({\n",
    "            'Column_Name': comprehensive_greenwashing.columns,\n",
    "            'Category': [\n",
    "                'Identification' if col in ['Company', 'Year'] else\n",
    "                'Main Scores' if any(x in col for x in ['Performance_Communication_Gap_Score', 'Performance_Score', 'Green_Communication_Score', 'Absolute_Gap', 'Amplified']) else\n",
    "                'Component Scores' if 'Component_' in col else\n",
    "                'Green Terms Analysis' if any(x in col for x in ['Green_Terms', 'combined_green_score']) else\n",
    "                'Sentiment Analysis' if any(x in col for x in ['Sentiment', 'sentiment']) else\n",
    "                'Substantiation Metrics' if any(x in col for x in ['Quantification', 'Evidence', 'Aspirational']) else\n",
    "                'Language Vagueness Metrics' if any(x in col for x in ['Hedge', 'Vague', 'Unclear', 'Timeline']) else\n",
    "                'Temporal Metrics' if 'Future_vs_Past' in col else\n",
    "                'Similarity Metrics' if any(x in col for x in ['Similarity', 'TFIDF', 'Jaccard', 'SpaCy']) else\n",
    "                'Other' for col in comprehensive_greenwashing.columns\n",
    "            ],\n",
    "            'Description': [\n",
    "                'Company identifier' if col == 'Company' else\n",
    "                'Year (2021 or 2022)' if col == 'Year' else\n",
    "                '(Amplified) Performance Communication Ga[ score (0-100, higher = more risk)' if col == 'Performance_Communication_Gap_Score' else\n",
    "                'Environmental performance score from ensemble analysis' if col == 'Performance_Score' else\n",
    "                'Green communication intensity score (0-100)' if col == 'Green_Communication_Score' else\n",
    "                'Absolute gap between communication and performance' if col == 'Performance_Communication_Absolute_Gap' else\n",
    "                # 'Amplified risk score with classic greenwashing penalty' if col == 'Greenwashing_Risk_Amplified' else\n",
    "                'Component score: Weakness of substantiation (0-100, higher = more risk)' if col == 'Component_Substantiation_Weakness_Score' else\n",
    "                'Component score: Language Vagueness (0-100, higher = more risk)' if col == 'Component_Language_Vagueness_Score' else\n",
    "                'Component score: Temporal orientation (0-100, higher = more risk)' if col == 'Component_Temporal_Orientation_Score' else\n",
    "                'Component score: Reporting consistency (0-100, higher = more risk)' if col == 'Component_Reporting_Consistency_Score' else\n",
    "                'Frequency of green terms as percentage of total words' if col == 'Green_Terms_Frequency_Pct' else\n",
    "                'Unique green terms relative to document length' if col == 'Green_Terms_Unique_Relative' else\n",
    "                'Combined score from green term frequency and uniqueness' if col == 'Green_Terms_Combined_Score' else\n",
    "                'Overall sentiment score across all text' if col == 'Overall_Sentiment_Score' else\n",
    "                'Average sentiment in renewable energy discussions' if col == 'Renewable_Energy_Sentiment' else\n",
    "                'Average sentiment in climate/emissions discussions' if col == 'Climate_Emissions_Sentiment' else\n",
    "                'Weighted combination of all sentiment scores' if col == 'Combined_Sentiment_Score' else\n",
    "                'Intensity of quantitative claims and metrics' if col == 'Quantification_Intensity_Score' else\n",
    "                'Intensity of evidence-based statements' if col == 'Evidence_Intensity_Score' else\n",
    "                'Intensity of aspirational/future-oriented language' if col == 'Aspirational_Intensity_Score' else\n",
    "                'Intensity of hedging language (uncertainty markers)' if col == 'Hedge_Intensity_Score' else\n",
    "                'Intensity of vague, non-specific language' if col == 'Vague_Intensity_Score' else\n",
    "                'Combined score of unclear language patterns' if col == 'Combined_Unclear_Intensity_Score' else\n",
    "                'Percentage of commitments with specific timelines' if col == 'Commitment_Timeline_Pct' else\n",
    "                'Ratio of future-focused vs past/present language' if col == 'Future_vs_Past_Present_Ratio' else\n",
    "                'TF-IDF based document similarity score' if col == 'TFIDF_Document_Similarity' else\n",
    "                'Jaccard similarity coefficient between documents' if col == 'Jaccard_Similarity' else\n",
    "                'SpaCy semantic similarity average' if col == 'SpaCy_Average_Similarity' else\n",
    "                'Ratio of highly similar sentences (SpaCy >0.8)' if col == 'SpaCy_High_Similarity_Ratio' else\n",
    "                'Combined similarity score from multiple methods' if col == 'Similarity_Combined_Score' else\n",
    "                f'Variable: {col}' for col in comprehensive_greenwashing.columns\n",
    "            ],\n",
    "            'Scale': [\n",
    "                'Text' if col in ['Company'] else\n",
    "                'Integer (2021, 2022)' if col == 'Year' else\n",
    "                '0-100 (normalized by year)' if 'Score' in col or 'Component_' in col else\n",
    "                'Percentage (0-100)' if 'Pct' in col else\n",
    "                'Ratio (0+)' if 'Ratio' in col else\n",
    "                'Normalized (0-100)' if any(x in col for x in ['Intensity', 'Similarity', 'Combined']) else\n",
    "                'Score (-100 to +100)' if 'Sentiment' in col else\n",
    "                'Continuous' for col in comprehensive_greenwashing.columns\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        data_dict.to_excel(writer, sheet_name='Data_Dictionary', index=False)\n",
    "    \n",
    "    print(f\"✓ Comprehensive results exported successfully to: {output_path}\")\n",
    "    print(f\"  - Comprehensive_Results: Complete dataset ({comprehensive_greenwashing.shape[0]} rows, {comprehensive_greenwashing.shape[1]} columns)\")\n",
    "    print(f\"  - Results_2021: 2021 data only\")\n",
    "    print(f\"  - Results_2022: 2022 data only\") \n",
    "    print(f\"  - Component_Methodology: Weights and calculation methods\")\n",
    "    print(f\"  - Component_Summary: Component score means by year\")\n",
    "    print(f\"  - Stats_2021/2022: Detailed descriptive statistics\")\n",
    "    print(f\"  - Data_Dictionary: Complete variable descriptions\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error exporting comprehensive results: {e}\")\n",
    "    print(\"Comprehensive dataset created successfully in memory as 'comprehensive_greenwashing'\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 5: Display key summary information\n",
    "# ==========================================\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPREHENSIVE GREENWASHING DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n📊 DATASET OVERVIEW:\")\n",
    "print(f\"  Total records: {len(comprehensive_greenwashing):,}\")\n",
    "print(f\"  Companies: {comprehensive_greenwashing['Company'].nunique()}\")\n",
    "print(f\"  Years: {sorted(comprehensive_greenwashing['Year'].unique())}\")\n",
    "print(f\"  Variables: {comprehensive_greenwashing.shape[1]}\")\n",
    "\n",
    "print(f\"\\n📈 VARIABLE CATEGORIES:\")\n",
    "main_scores = [col for col in comprehensive_greenwashing.columns if any(x in col for x in ['Performance_Communication_Gap_Score', 'Performance_Score', 'Green_Communication_Score', 'Absolute_Gap', 'Amplified'])]\n",
    "components = [col for col in comprehensive_greenwashing.columns if 'Component_' in col]\n",
    "green_terms = [col for col in comprehensive_greenwashing.columns if 'Green_Terms' in col or 'combined_green_score' in col]\n",
    "sentiment = [col for col in comprehensive_greenwashing.columns if 'Sentiment' in col or 'sentiment' in col]\n",
    "substantiation = [col for col in comprehensive_greenwashing.columns if any(x in col for x in ['Quantification', 'Evidence', 'Aspirational'])]\n",
    "language = [col for col in comprehensive_greenwashing.columns if any(x in col for x in ['Hedge', 'Vague', 'Unclear', 'Timeline'])]\n",
    "temporal = [col for col in comprehensive_greenwashing.columns if 'Future_vs_Past' in col]\n",
    "similarity = [col for col in comprehensive_greenwashing.columns if any(x in col for x in ['Similarity', 'TFIDF', 'Jaccard', 'SpaCy'])]\n",
    "\n",
    "print(f\"  Main Scores: {len(main_scores)} variables\")\n",
    "print(f\"  Component Scores: {len(components)} variables\")\n",
    "print(f\"  Green Terms Analysis: {len(green_terms)} variables\")\n",
    "print(f\"  Sentiment Analysis: {len(sentiment)} variables\")\n",
    "print(f\"  Substantiation Metrics: {len(substantiation)} variables\")\n",
    "print(f\"  Language Metrics: {len(language)} variables\")\n",
    "print(f\"  Temporal Metrics: {len(temporal)} variables\")\n",
    "print(f\"  Similarity/Consistency: {len(similarity)} variables\")\n",
    "\n",
    "print(f\"\\n🎯 KEY STATISTICS:\")\n",
    "print(f\"  Mean Greenwashing Risk (2021): {comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2021]['Performance_Communication_Gap_Score'].mean():.2f}\")\n",
    "print(f\"  Mean Greenwashing Risk (2022): {comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2022]['Performance_Communication_Gap_Score'].mean():.2f}\")\n",
    "print(f\"  Companies with both years: {len(comprehensive_greenwashing.groupby('Company').filter(lambda x: len(x) == 2)['Company'].unique())}\")\n",
    "\n",
    "print(f\"\\n💾 FILES CREATED:\")\n",
    "print(f\"  Comprehensive dataset: {output_path}\")\n",
    "print(f\"  (8 sheets with complete data, methodology, and documentation)\")\n",
    "\n",
    "print(f\"\\n🔬 READY FOR ENSEMBLE ANALYSIS:\")\n",
    "print(f\"  All component scores calculated and normalized\")\n",
    "print(f\"  All underlying variables preserved\")\n",
    "print(f\"  Complete documentation provided\")\n",
    "\n",
    "print(f\"\\nVariable 'comprehensive_greenwashing' is available in memory for immediate use\")\n",
    "print(\"Proceed to ensemble analysis with confidence that all data is preserved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984de21",
   "metadata": {},
   "source": [
    "# Final Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f7a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ENSEMBLE GREENWASHING SCORE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: Generate valid weight combinations\n",
    "# ==========================================\n",
    "\n",
    "print(\"Generating valid weight combinations...\")\n",
    "print(\"Constraints:\")\n",
    "print(\"- Individual weights: 0.05 ≤ w ≤ 0.50\")\n",
    "print(\"- Greenwashing_Score > all other components\")\n",
    "print(\"- Substantiation_Weakness > Language_Vagueness, Temporal_Orientation, Reporting_Consistency\")\n",
    "print(\"- All weights sum to 1.0\")\n",
    "print()\n",
    "\n",
    "# Create weight ranges (2 decimal precision as requested)\n",
    "weight_range = np.arange(0.05, 0.51, 0.01)\n",
    "weight_range = np.round(weight_range, 2)\n",
    "\n",
    "valid_combinations = []\n",
    "total_combinations = 0\n",
    "\n",
    "# Generate all possible combinations\n",
    "for w_gw in weight_range:  # Greenwashing_Score weight\n",
    "    for w_sub in weight_range:  # Substantiation_Weakness weight\n",
    "        for w_lang in weight_range:  # Language_Vagueness weight\n",
    "            for w_temp in weight_range:  # Temporal_Orientation weight\n",
    "                total_combinations += 1\n",
    "                \n",
    "                # Calculate remaining weight for Reporting_Consistency\n",
    "                w_rep = 1.0 - (w_gw + w_sub + w_lang + w_temp)\n",
    "                w_rep = round(w_rep, 2)\n",
    "                \n",
    "                # Check if all constraints are satisfied\n",
    "                if (0.05 <= w_rep <= 0.50 and  # Reporting_Consistency in valid range\n",
    "                    w_gw > w_sub and           # Greenwashing_Score > Substantiation_Weakness\n",
    "                    w_gw > w_lang and          # Greenwashing_Score > Language_Vagueness \n",
    "                    w_gw > w_temp and          # Greenwashing_Score > Temporal_Orientation\n",
    "                    w_gw > w_rep and           # Greenwashing_Score > Reporting_Consistency\n",
    "                    w_sub > w_lang and         # Substantiation_Weakness > Language_Vagueness\n",
    "                    w_sub > w_temp and         # Substantiation_Weakness > Temporal_Orientation  \n",
    "                    w_sub > w_rep and          # Substantiation_Weakness > Reporting_Consistency\n",
    "                    abs(w_gw + w_sub + w_lang + w_temp + w_rep - 1.0) < 0.001):  # Sum ≈ 1.0\n",
    "                    \n",
    "                    valid_combinations.append({\n",
    "                        'w_greenwashing': w_gw,\n",
    "                        'w_substantiation': w_sub,\n",
    "                        'w_language': w_lang,\n",
    "                        'w_temporal': w_temp,\n",
    "                        'w_reporting': w_rep\n",
    "                    })\n",
    "\n",
    "print(f\"Total combinations tested: {total_combinations:,}\")\n",
    "print(f\"Valid combinations found: {len(valid_combinations):,}\")\n",
    "print(f\"Percentage valid: {len(valid_combinations)/total_combinations*100:.2f}%\")\n",
    "\n",
    "if len(valid_combinations) == 0:\n",
    "    print(\"ERROR: No valid weight combinations found. Check constraints.\")\n",
    "else:\n",
    "    # ==========================================\n",
    "    # STEP 2: Calculate scores for all combinations\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\nCalculating scores for {len(valid_combinations):,} weight combinations...\")\n",
    "    \n",
    "    # Initialize storage for all results\n",
    "    all_results = []\n",
    "    \n",
    "    # Progress bar for weight combinations\n",
    "    for i, weights in enumerate(tqdm(valid_combinations, desc=\"Processing combinations\")):\n",
    "        \n",
    "        # Calculate final score for this weight combination\n",
    "        final_scores = (\n",
    "            weights['w_greenwashing'] * greenwashing_df['Greenwashing_Score'] +\n",
    "            weights['w_substantiation'] * greenwashing_df['Substantiation_Weakness'] +\n",
    "            weights['w_language'] * greenwashing_df['Language_Vagueness'] +\n",
    "            weights['w_temporal'] * greenwashing_df['Temporal_Orientation'] +\n",
    "            weights['w_reporting'] * greenwashing_df['Reporting_Consistency']\n",
    "        ).round(2)\n",
    "        \n",
    "        # Store results for this combination\n",
    "        combination_result = {\n",
    "            'combination_id': i,\n",
    "            'weights': weights,\n",
    "            'scores': final_scores.tolist(),\n",
    "            'organizations': greenwashing_df['Organization'].tolist(),\n",
    "            'years': greenwashing_df['year'].tolist(),\n",
    "            'summary_stats': {\n",
    "                'mean': final_scores.mean(),\n",
    "                'median': final_scores.median(),\n",
    "                'std': final_scores.std(),\n",
    "                'min': final_scores.min(),\n",
    "                'max': final_scores.max(),\n",
    "                'q25': final_scores.quantile(0.25),\n",
    "                'q75': final_scores.quantile(0.75)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        all_results.append(combination_result)\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 3: Create ensemble summary\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"\\nCreating ensemble summary...\")\n",
    "    \n",
    "    # Extract all scores into matrix format\n",
    "    n_combinations = len(all_results)\n",
    "    n_observations = len(greenwashing_df)\n",
    "    \n",
    "    # Matrix: rows = combinations, columns = observations\n",
    "    score_matrix = np.array([result['scores'] for result in all_results])\n",
    "    \n",
    "    # Calculate statistics across all combinations for each observation\n",
    "    ensemble_stats = pd.DataFrame({\n",
    "        'Organization': greenwashing_df['Organization'],\n",
    "        'year': greenwashing_df['year'],\n",
    "        'mean_score': np.mean(score_matrix, axis=0),\n",
    "        'median_score': np.median(score_matrix, axis=0),\n",
    "        'std_score': np.std(score_matrix, axis=0),\n",
    "        'min_score': np.min(score_matrix, axis=0),\n",
    "        'max_score': np.max(score_matrix, axis=0),\n",
    "        'q25_score': np.percentile(score_matrix, 25, axis=0),\n",
    "        'q75_score': np.percentile(score_matrix, 75, axis=0),\n",
    "        'iqr_score': np.percentile(score_matrix, 75, axis=0) - np.percentile(score_matrix, 25, axis=0),\n",
    "        'range_score': np.max(score_matrix, axis=0) - np.min(score_matrix, axis=0)\n",
    "    }).round(2)\n",
    "    \n",
    "    # Weight distribution analysis\n",
    "    weight_analysis = pd.DataFrame([result['weights'] for result in all_results])\n",
    "    weight_stats = {\n",
    "        'weight_ranges': {\n",
    "            'greenwashing': [weight_analysis['w_greenwashing'].min(), weight_analysis['w_greenwashing'].max()],\n",
    "            'substantiation': [weight_analysis['w_substantiation'].min(), weight_analysis['w_substantiation'].max()], \n",
    "            'language': [weight_analysis['w_language'].min(), weight_analysis['w_language'].max()],\n",
    "            'temporal': [weight_analysis['w_temporal'].min(), weight_analysis['w_temporal'].max()],\n",
    "            'reporting': [weight_analysis['w_reporting'].min(), weight_analysis['w_reporting'].max()]\n",
    "        },\n",
    "        'weight_means': weight_analysis.mean().to_dict(),\n",
    "        'weight_stds': weight_analysis.std().to_dict()\n",
    "    }\n",
    "    \n",
    "    # Overall ensemble statistics\n",
    "    overall_stats = {\n",
    "        'total_combinations': n_combinations,\n",
    "        'total_observations': n_observations,\n",
    "        'score_stability': {\n",
    "            'mean_std_across_combinations': ensemble_stats['std_score'].mean(),\n",
    "            'max_std_across_combinations': ensemble_stats['std_score'].max(),\n",
    "            'mean_range_across_combinations': ensemble_stats['range_score'].mean(),\n",
    "            'companies_with_high_uncertainty': len(ensemble_stats[ensemble_stats['std_score'] > ensemble_stats['std_score'].quantile(0.9)])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 4: Calculate company averages across both years\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"Calculating company averages across years...\")\n",
    "    \n",
    "    # Company averages for median scores\n",
    "    company_averages_median = ensemble_stats.groupby('Organization').agg({\n",
    "        'median_score': ['mean', 'count']\n",
    "    }).round(2)\n",
    "    company_averages_median.columns = ['Avg_Median_Score', 'Years_Count']\n",
    "    company_averages_median = company_averages_median.reset_index()\n",
    "    company_averages_median = company_averages_median[company_averages_median['Years_Count'] == 2]  # Only companies with both years\n",
    "    \n",
    "    # Company averages for mean scores\n",
    "    company_averages_mean = ensemble_stats.groupby('Organization').agg({\n",
    "        'mean_score': ['mean', 'count']\n",
    "    }).round(2)\n",
    "    company_averages_mean.columns = ['Avg_Mean_Score', 'Years_Count']\n",
    "    company_averages_mean = company_averages_mean.reset_index()\n",
    "    company_averages_mean = company_averages_mean[company_averages_mean['Years_Count'] == 2]  # Only companies with both years\n",
    "    \n",
    "    # Combine company averages\n",
    "    company_averages = pd.merge(company_averages_median[['Organization', 'Avg_Median_Score']], \n",
    "                               company_averages_mean[['Organization', 'Avg_Mean_Score']], \n",
    "                               on='Organization') \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d399bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# ==========================================\n",
    "# STEP 5: Save results\n",
    "# ==========================================\n",
    "if len(valid_combinations) == 0:\n",
    "    print(\"ERROR: No valid weight combinations found. Check constraints.\")\n",
    "else:    \n",
    "    print(f\"\\nSaving results...\")\n",
    "    \n",
    "    # Define file path\n",
    "    output_path = \"data/Greenwashing Results/ensemble_results_summary_ensforperf.xlsx\"\n",
    "    \n",
    "    # Save key results as Excel for easy viewing\n",
    "    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "        ensemble_stats.to_excel(writer, sheet_name='Ensemble_Scores', index=False)\n",
    "        company_averages.to_excel(writer, sheet_name='Company_Averages', index=False)\n",
    "        \n",
    "        # Summary statistics sheet\n",
    "        summary_df = pd.DataFrame([\n",
    "            ['Total Combinations', overall_stats['total_combinations']],\n",
    "            ['Total Observations', overall_stats['total_observations']],\n",
    "            ['Mean Std Across Combinations', overall_stats['score_stability']['mean_std_across_combinations']],\n",
    "            ['Max Std Across Combinations', overall_stats['score_stability']['max_std_across_combinations']],\n",
    "            ['Mean Range Across Combinations', overall_stats['score_stability']['mean_range_across_combinations']],\n",
    "            ['High Uncertainty Companies', overall_stats['score_stability']['companies_with_high_uncertainty']]\n",
    "        ], columns=['Metric', 'Value'])\n",
    "        summary_df.to_excel(writer, sheet_name='Summary_Stats', index=False)\n",
    "    \n",
    "    print(\"Applying Excel formatting...\")\n",
    "    \n",
    "    # Load the workbook for formatting\n",
    "    wb = load_workbook(output_path)\n",
    "    \n",
    "    # Define grey fill for alternating rows\n",
    "    grey_fill = PatternFill(start_color=\"D9D9D9\", end_color=\"D9D9D9\", fill_type=\"solid\")\n",
    "    \n",
    "    # Format each sheet\n",
    "    for sheet_name in wb.sheetnames:\n",
    "        ws = wb[sheet_name]\n",
    "        \n",
    "        # Auto-adjust column widths based on the longest string in each column\n",
    "        for col in ws.columns:\n",
    "            max_length = 0\n",
    "            col_letter = get_column_letter(col[0].column)\n",
    "            for cell in col:\n",
    "                if cell.value:\n",
    "                    max_length = max(max_length, len(str(cell.value)))\n",
    "            ws.column_dimensions[col_letter].width = max_length + 3  # Add padding\n",
    "        \n",
    "        # Apply alternating row colors\n",
    "        if sheet_name in ['Ensemble_Scores', 'Company_Averages']:\n",
    "            # These sheets have company names in column A - alternate by company\n",
    "            prev_company = None\n",
    "            use_grey = False\n",
    "            for row in range(2, ws.max_row + 1):\n",
    "                current_company = ws[f\"A{row}\"].value  # Column A has the company names\n",
    "                if current_company != prev_company:\n",
    "                    use_grey = not use_grey\n",
    "                    prev_company = current_company\n",
    "                \n",
    "                if use_grey:\n",
    "                    for col in range(1, ws.max_column + 1):\n",
    "                        ws.cell(row=row, column=col).fill = grey_fill\n",
    "        else:\n",
    "            # Summary_Stats sheet - simple alternating rows\n",
    "            for row in range(2, ws.max_row + 1):\n",
    "                if row % 2 == 0:  # Even rows get grey background\n",
    "                    for col in range(1, ws.max_column + 1):\n",
    "                        ws.cell(row=row, column=col).fill = grey_fill\n",
    "    \n",
    "    # Save the final formatted workbook\n",
    "    wb.save(output_path)\n",
    "    \n",
    "    print(f\"Results saved and formatted:\")\n",
    "    print(f\"- Summary tables: data/Greenwashing Results/ensemble_results_summary_ensforperf.xlsx\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 6: Display results by year and averages\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ENSEMBLE GREENWASHING SCORE RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nOVERALL STATISTICS:\")\n",
    "    print(f\"Total weight combinations tested: {overall_stats['total_combinations']:,}\")\n",
    "    print(f\"Mean uncertainty (std) across all companies: {overall_stats['score_stability']['mean_std_across_combinations']:.2f}\")\n",
    "    print(f\"Maximum uncertainty (std) for any company: {overall_stats['score_stability']['max_std_across_combinations']:.2f}\")\n",
    "    print(f\"Companies with high score uncertainty (>90th percentile): {overall_stats['score_stability']['companies_with_high_uncertainty']}\")\n",
    "    \n",
    "    # Results by year - 2021\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(\"HIGHEST RISK COMPANIES - 2021\")\n",
    "    print(\"=\"*40)\n",
    "    print(\"(Ranked by median ensemble score)\")\n",
    "    \n",
    "    ensemble_2021 = ensemble_stats[ensemble_stats['year'] == 2021]\n",
    "    top_risk_2021 = ensemble_2021.nlargest(10, 'median_score')[['Organization', 'median_score', 'mean_score', 'std_score', 'min_score', 'max_score']]\n",
    "    print(top_risk_2021.to_string(index=False))\n",
    "    \n",
    "    # Results by year - 2022\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(\"HIGHEST RISK COMPANIES - 2022\")\n",
    "    print(\"=\"*40)\n",
    "    print(\"(Ranked by median ensemble score)\")\n",
    "    \n",
    "    ensemble_2022 = ensemble_stats[ensemble_stats['year'] == 2022]\n",
    "    top_risk_2022 = ensemble_2022.nlargest(10, 'median_score')[['Organization', 'median_score', 'mean_score', 'std_score', 'min_score', 'max_score']]\n",
    "    print(top_risk_2022.to_string(index=False))\n",
    "    \n",
    "    # Company averages across both years\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(\"HIGHEST RISK COMPANIES - AVERAGE (2021-2022)\")\n",
    "    print(\"=\"*40)\n",
    "    print(\"(Companies with data for both years)\")\n",
    "    \n",
    "    top_avg_companies = company_averages.nlargest(14, 'Avg_Median_Score')\n",
    "    print(top_avg_companies.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nCompanies with data for both years: {len(company_averages)}\")\n",
    "    \n",
    "    # Most uncertain companies by year\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(\"MOST UNCERTAIN COMPANIES BY YEAR\")\n",
    "    print(\"=\"*40)\n",
    "    print(\"(Highest std across weight combinations)\")\n",
    "    \n",
    "    print(f\"\\n2021 - Most Uncertain:\")\n",
    "    most_uncertain_2021 = ensemble_2021.nlargest(5, 'std_score')[['Organization', 'median_score', 'mean_score', 'std_score', 'min_score', 'max_score']]\n",
    "    print(most_uncertain_2021.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n2022 - Most Uncertain:\")\n",
    "    most_uncertain_2022 = ensemble_2022.nlargest(5, 'std_score')[['Organization', 'median_score', 'mean_score', 'std_score', 'min_score', 'max_score']]\n",
    "    print(most_uncertain_2022.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"ANALYSIS COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Use 'ensemble_stats' DataFrame for individual company/year scores\")\n",
    "    print(\"Use 'company_averages' DataFrame for cross-year company averages\")\n",
    "    print(\"All results saved to files for further analysis\")\n",
    "\n",
    "print(\"\\nEnsemble analysis complete!\")\n",
    "print(\"\\nKey variables created:\")\n",
    "print(\"- ensemble_stats: Individual company scores by year\")\n",
    "print(\"- company_averages: Company averages across both years\") \n",
    "print(\"- weight_analysis: All valid weight combinations (in memory)\")\n",
    "print(\"- all_results: Complete results for all combinations\")\n",
    "print(\"\\nFormatted results saved to: data/Greenwashing Results/ensemble_results_summary_ensforperf.xlsx\")\n",
    "print(\"(3 sheets: Ensemble_Scores, Company_Averages, Summary_Stats)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4478364",
   "metadata": {},
   "source": [
    "### Final Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eb941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Create Master Greenwashing Dataset - Combining All Results\n",
    "\n",
    "print(\"CREATING MASTER GREENWASHING DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Combining comprehensive communication results with ensemble statistics...\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: Load Comprehensive Greenwashing Results\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\nLoading comprehensive greenwashing results...\")\n",
    "\n",
    "try:\n",
    "    # Load comprehensive results\n",
    "    comp_greenwashing = pd.read_excel(\"data/Greenwashing Results/comprehensive_greenwashing_results.xlsx\", \n",
    "                                     sheet_name='Comprehensive_Results')\n",
    "    \n",
    "    print(f\"✓ Comprehensive data loaded: {len(comp_greenwashing)} records\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading comprehensive results: {e}\")\n",
    "    print(\"Using in-memory comprehensive_greenwashing data...\")\n",
    "    comp_greenwashing = comprehensive_greenwashing.copy()\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: Load Ensemble Greenwashing Results\n",
    "# ==========================================\n",
    "\n",
    "print(\"Loading ensemble greenwashing results...\")\n",
    "\n",
    "try:\n",
    "    ensemble_greenwashing = pd.read_excel(\"data/Greenwashing Results/ensemble_results_summary_ensforperf.xlsx\", \n",
    "                                         sheet_name='Ensemble_Scores')\n",
    "    \n",
    "    print(f\"✓ Ensemble data loaded: {len(ensemble_greenwashing)} records\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading ensemble results: {e}\")\n",
    "    print(\"Using in-memory ensemble_stats data...\")\n",
    "    ensemble_greenwashing = ensemble_stats.copy()\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2.5: Load Sensitivity Analysis Results\n",
    "# ==========================================\n",
    "\n",
    "print(\"Loading sensitivity analysis results...\")\n",
    "\n",
    "try:\n",
    "    # Load company sensitivity metrics from existing sensitivity analysis\n",
    "    company_sensitivity = pd.read_excel(\"data/Greenwashing Results/sensitivity_analysis/communication_sensitivity_results.xlsx\", \n",
    "                                       sheet_name='Company_Sensitivity')\n",
    "    company_sensitivity.rename(columns={'Organization': 'Company'}, inplace=True)\n",
    "    \n",
    "    # Load scenario impact summary \n",
    "    scenario_impact = pd.read_excel(\"data/Greenwashing Results/sensitivity_analysis/communication_sensitivity_results.xlsx\", \n",
    "                                   sheet_name='Scenario_Impact')\n",
    "    \n",
    "    print(f\"✓ Sensitivity data loaded:\")\n",
    "    print(f\"  - Company sensitivity: {len(company_sensitivity)} companies\")\n",
    "    print(f\"  - Scenario impact: {len(scenario_impact)} scenarios\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load sensitivity analysis results: {e}\")\n",
    "    print(\"Continuing without sensitivity data...\")\n",
    "    company_sensitivity = pd.DataFrame()\n",
    "    scenario_impact = pd.DataFrame()\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: Standardize and Rename Columns\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\nStandardizing column names...\")\n",
    "\n",
    "# Rename comprehensive data columns with clear, informative names\n",
    "if not comp_greenwashing.empty:\n",
    "    comprehensive_renamed = comp_greenwashing.rename(columns={\n",
    "        'Company': 'Company',\n",
    "        'Year': 'Year',\n",
    "        \n",
    "        # ============= MAIN GREENWASHING SCORES =============\n",
    "        'Performance_Communication_Gap_Score': 'Performance_Communication_Gap_Score',\n",
    "        'Performance_Score': 'Performance_Score',\n",
    "        'Green_Communication_Score': 'Green_Com_Score',\n",
    "        'Greenwashing_Risk_Absolute_Gap': 'Greenwashing_Risk_Absolute_Gap',\n",
    "        'Greenwashing_Risk_Amplified': 'Greenwashing_Risk_Amplified',\n",
    "        \n",
    "        # ============= COMMUNICATION COMPONENT SCORES =============\n",
    "        'Component_Substantiation_Weakness_Score': 'Substantiation_Weakness',\n",
    "        'Component_Language_Vagueness_Score': 'Language_Vagueness',\n",
    "        'Component_Temporal_Orientation_Score': 'Temporal_Orientation',\n",
    "        'Component_Reporting_Consistency_Score': 'Reporting_Consistency',\n",
    "        \n",
    "        # ============= UNDERLYING RAW COMMUNICATION VARIABLES =============\n",
    "        'Green_Terms_Frequency_Pct': 'gt_freq_pct',\n",
    "        'Green_Terms_Unique_Relative': 'unique_gt_relative',\n",
    "        'Green_Terms_Combined_Score': 'combined_green_score',\n",
    "        \n",
    "        'Overall_Sentiment_Score': 'avg_sentiment_score',\n",
    "        'Renewable_Energy_Sentiment': 'renewable_energy_avg_sentiment',\n",
    "        'Climate_Emissions_Sentiment': 'climate_emissions_avg_sentiment',\n",
    "        'Combined_Sentiment_Score': 'combined_sentiment_score',\n",
    "        \n",
    "        'Quantification_Intensity_Score': 'quantification_intensity_score',\n",
    "        'Evidence_Intensity_Score': 'evidence_intensity_score',\n",
    "        'Aspirational_Intensity_Score': 'aspirational_intensity_score',\n",
    "        \n",
    "        'Hedge_Intensity_Score': 'hedge_density',\n",
    "        'Vague_Intensity_Score': 'vague_density',\n",
    "        'Combined_Unclear_Intensity_Score': 'combined_unclear_intensity',\n",
    "        'Commitment_Timeline_Pct': 'commitment_timeline_pct',\n",
    "        \n",
    "        'Future_vs_Past_Present_Ratio': 'temporal_future_pct',\n",
    "        \n",
    "        'TFIDF_Document_Similarity': 'TFIDF_similarity',\n",
    "        'Jaccard_Similarity': 'Jaccard_similarity',\n",
    "        'SpaCy_Average_Similarity': 'SpaCy_avg_similarity',\n",
    "        'SpaCy_High_Similarity_Ratio': 'SpaCy_HighSim_Ratio',\n",
    "        'Similarity_Combined_Score': 'similarity_combined'\n",
    "    })\n",
    "else:\n",
    "    comprehensive_renamed = pd.DataFrame()\n",
    "\n",
    "# Rename ensemble data columns with Greenwashing_ prefix\n",
    "if not ensemble_greenwashing.empty:\n",
    "    ensemble_renamed = ensemble_greenwashing.rename(columns={\n",
    "        'Organization': 'Company',\n",
    "        'year': 'Year',\n",
    "        'mean_score': 'Ens_Greenwashing_mean_score',\n",
    "        'median_score': 'Ens_Greenwashing_median_score',\n",
    "        'std_score': 'Ens_Greenwashing_std_score',\n",
    "        'min_score': 'Ens_Greenwashing_min_score',\n",
    "        'max_score': 'Ens_Greenwashing_max_score',\n",
    "        'q25_score': 'Ens_Greenwashing_q25_score',\n",
    "        'q75_score': 'Ens_Greenwashing_q75_score',\n",
    "        'iqr_score': 'Ens_Greenwashing_iqr_score',\n",
    "        'range_score': 'Ens_Greenwashing_range_score'\n",
    "    })\n",
    "else:\n",
    "    ensemble_renamed = pd.DataFrame()\n",
    "\n",
    "print(f\"✓ Column standardization complete\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 4: Merge Datasets\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\nMerging comprehensive and ensemble data...\")\n",
    "\n",
    "if not comprehensive_renamed.empty and not ensemble_renamed.empty:\n",
    "    # First merge comprehensive and ensemble data\n",
    "    master_greenwashing = pd.merge(\n",
    "        comprehensive_renamed, \n",
    "        ensemble_renamed, \n",
    "        on=['Company', 'Year'], \n",
    "        how='outer',\n",
    "        suffixes=('', '_ensemble')\n",
    "    )\n",
    "    \n",
    "    # Then add sensitivity data (company-level, no year dimension)\n",
    "if not company_sensitivity.empty:\n",
    "    # Rename sensitivity columns with Sens_ prefix\n",
    "    sensitivity_renamed = company_sensitivity.rename(columns={\n",
    "        'CV_Percent': 'Sens_CV_Percent',\n",
    "        'Score_Range': 'Sens_Score_Range',\n",
    "        'MAD': 'Sens_MAD',\n",
    "        'Avg_Rank_Shift': 'Sens_Avg_Rank_Shift',\n",
    "        'Max_Rank_Shift': 'Sens_Max_Rank_Shift',\n",
    "        'Sensitivity_Level': 'Sens_Sensitivity_Level'\n",
    "    })\n",
    "    \n",
    "    sensitivity_cols = ['Company', 'Sens_CV_Percent', 'Sens_Score_Range', 'Sens_MAD', 'Sens_Avg_Rank_Shift', \n",
    "                       'Sens_Max_Rank_Shift', 'Sens_Sensitivity_Level']\n",
    "    master_greenwashing = pd.merge(\n",
    "        master_greenwashing,\n",
    "        sensitivity_renamed[sensitivity_cols],\n",
    "        on='Company',\n",
    "        how='left'\n",
    "    )\n",
    "    print(f\"✓ Sensitivity data merged: {len(sensitivity_cols)-1} sensitivity variables added\")\n",
    "    \n",
    "    print(f\"✓ Merge complete: {len(master_greenwashing)} records\")\n",
    "    print(f\"  - Companies: {master_greenwashing['Company'].nunique()}\")\n",
    "    print(f\"  - Years: {sorted(master_greenwashing['Year'].unique())}\")\n",
    "    \n",
    "elif not comprehensive_renamed.empty:\n",
    "    master_greenwashing = comprehensive_renamed.copy()\n",
    "    print(\"Using comprehensive data only (ensemble data not available)\")\n",
    "    \n",
    "elif not ensemble_renamed.empty:\n",
    "    master_greenwashing = ensemble_renamed.copy()\n",
    "    print(\"Using ensemble data only (comprehensive data not available)\")\n",
    "    \n",
    "else:\n",
    "    print(\"Error: No data available from either source\")\n",
    "    master_greenwashing = pd.DataFrame()\n",
    "\n",
    "# ==========================================\n",
    "# STEP 5: Final Data Organization and Validation\n",
    "# ==========================================\n",
    "\n",
    "if not master_greenwashing.empty:\n",
    "    # Sort by Company and Year for consistency\n",
    "    master_greenwashing = master_greenwashing.sort_values(['Company', 'Year']).reset_index(drop=True)\n",
    "    \n",
    "    # Round numerical columns for clean display\n",
    "    numeric_columns = master_greenwashing.select_dtypes(include=[np.number]).columns\n",
    "    master_greenwashing[numeric_columns] = master_greenwashing[numeric_columns].round(3)\n",
    "    \n",
    "    print(f\"\\nFinal dataset structure:\")\n",
    "    print(f\"  - Shape: {master_greenwashing.shape}\")\n",
    "    print(f\"  - Companies: {master_greenwashing['Company'].nunique()}\")\n",
    "    print(f\"  - Years covered: {sorted(master_greenwashing['Year'].unique())}\")\n",
    "    print(f\"  - Component score columns: {len([col for col in master_greenwashing.columns if col in ['Green_Com_Score', 'Substantiation_Weakness', 'Language_Vagueness', 'Temporal_Orientation', 'Reporting_Consistency']])}\")\n",
    "    print(f\"  - Ensemble statistic columns: {len([col for col in master_greenwashing.columns if 'Ens_Greenwashing_' in col])}\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 6: Export Master Dataset\n",
    "# ==========================================\n",
    "\n",
    "print(f\"\\nExporting master greenwashing dataset...\")\n",
    "\n",
    "if not master_greenwashing.empty:\n",
    "    output_path = \"data/Greenwashing Results/master_greenwashing_dataset.xlsx\"\n",
    "    \n",
    "    try:\n",
    "        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "            # Main dataset\n",
    "            master_greenwashing.to_excel(writer, sheet_name='Master_Greenwashing_Data', index=False)\n",
    "            \n",
    "            # Create data dictionary\n",
    "            data_dict = pd.DataFrame({\n",
    "                'Column_Name': master_greenwashing.columns,\n",
    "                'Data_Type': [str(master_greenwashing[col].dtype) for col in master_greenwashing.columns],\n",
    "                'Category': [\n",
    "                    'Identification' if col in ['Company', 'Year'] else\n",
    "                    'Main Greenwashing Scores' if col in ['Performance_Communication_Gap_Score', 'Performance_Score', 'Greenwashing_Risk_Absolute_Gap', 'Greenwashing_Risk_Amplified'] else\n",
    "                    'Communication Component Scores' if col in ['Green_Com_Score', 'Substantiation_Weakness', 'Language_Vagueness', 'Temporal_Orientation', 'Reporting_Consistency'] else\n",
    "                    'Greenwashing Ensemble Statistics' if any(x in col for x in ['Ens_Greenwashing_mean', 'Ens_Greenwashing_median', 'Ens_Greenwashing_std', 'Ens_Greenwashing_min', 'Ens_Greenwashing_max', 'Ens_Greenwashing_q25', 'Ens_Greenwashing_q75', 'Ens_Greenwashing_iqr', 'Ens_Greenwashing_range']) else\n",
    "                    'Sensitivity Analysis' if col in ['Sens_CV_Percent', 'Sens_Score_Range', 'Sens_MAD', 'Sens_Avg_Rank_Shift', 'Sens_Max_Rank_Shift', 'Sens_Sensitivity_Level'] else\n",
    "                    'Green Terms Analysis' if col in ['gt_freq_pct', 'unique_gt_relative', 'combined_green_score'] else\n",
    "                    'Sentiment Analysis' if col in ['avg_sentiment_score', 'renewable_energy_avg_sentiment', 'climate_emissions_avg_sentiment', 'combined_sentiment_score'] else\n",
    "                    'Substantiation Variables' if col in ['quantification_intensity_score', 'evidence_intensity_score', 'aspirational_intensity_score'] else\n",
    "                    'Language Vagueness Variables' if col in ['hedge_density', 'vague_density', 'combined_unclear_intensity', 'commitment_timeline_pct'] else\n",
    "                    'Temporal Variables' if col in ['temporal_future_pct'] else\n",
    "                    'Similarity Variables' if col in ['TFIDF_similarity', 'Jaccard_similarity', 'SpaCy_avg_similarity', 'SpaCy_HighSim_Ratio', 'similarity_combined'] else\n",
    "                    'Other' for col in master_greenwashing.columns\n",
    "                ],\n",
    "                'Description': [\n",
    "                    'Company name' if col == 'Company' else\n",
    "                    'Year (2021 or 2022)' if col == 'Year' else\n",
    "                    '(Amplified) Performance Communication Gap Score (0-100, higher = more risk)' if col == 'Performance_Communication_Gap_Score' else\n",
    "                    'Environmental performance score from performance analysis' if col == 'Performance_Score' else\n",
    "                    'Combined green communication intensity score (0-100)' if col == 'Green_Com_Score' else\n",
    "                    'Absolute gap between communication and performance scores' if col == 'Greenwashing_Risk_Absolute_Gap' else\n",
    "                    'Amplified risk score with classic greenwashing penalty' if col == 'Greenwashing_Risk_Amplified' else\n",
    "                    'Component score: Weakness of substantiation (0-100, higher = more risk)' if col == 'Substantiation_Weakness' else\n",
    "                    'Component score: Language Vagueness (0-100, higher = more risk)' if col == 'Language_Vagueness' else\n",
    "                    'Component score: Temporal orientation (0-100, higher = more risk)' if col == 'Temporal_Orientation' else\n",
    "                    'Component score: Reporting consistency (0-100, higher = more risk)' if col == 'Reporting_Consistency' else\n",
    "                    'Ensemble median greenwashing score across all weight combinations' if col == 'Ens_Greenwashing_median_score' else\n",
    "                    'Ensemble mean greenwashing score across all weight combinations' if col == 'Ens_Greenwashing_mean_score' else\n",
    "                    'Ensemble standard deviation of greenwashing scores' if col == 'Ens_Greenwashing_std_score' else\n",
    "                    'Ensemble minimum greenwashing score across all weight combinations' if col == 'Ens_Greenwashing_min_score' else\n",
    "                    'Ensemble maximum greenwashing score across all weight combinations' if col == 'Ens_Greenwashing_max_score' else\n",
    "                    'Ensemble 25th percentile greenwashing score' if col == 'Ens_Greenwashing_q25_score' else\n",
    "                    'Ensemble 75th percentile greenwashing score' if col == 'Ens_Greenwashing_q75_score' else\n",
    "                    'Ensemble interquartile range of greenwashing scores' if col == 'Ens_Greenwashing_iqr_score' else\n",
    "                    'Ensemble range (max-min) of greenwashing scores' if col == 'Ens_Greenwashing_range_score' else\n",
    "                    'Coefficient of variation across weight scenarios (%)' if col == 'Sens_CV_Percent' else\n",
    "                    'Score range across all weight scenarios' if col == 'Sens_Score_Range' else\n",
    "                    'Mean absolute deviation from baseline' if col == 'Sens_MAD' else\n",
    "                    'Average ranking shift across scenarios' if col == 'Sens_Avg_Rank_Shift' else\n",
    "                    'Maximum ranking shift across scenarios' if col == 'Sens_Max_Rank_Shift' else\n",
    "                    'Sensitivity level classification (High/Moderate/Low)' if col == 'Sens_Sensitivity_Level' else\n",
    "                    'Green term frequency as percentage of total words' if col == 'gt_freq_pct' else\n",
    "                    'Unique green terms relative to document length (vocabulary diversity)' if col == 'unique_gt_relative' else\n",
    "                    'Combined score from green term frequency and uniqueness' if col == 'combined_green_score' else\n",
    "                    'Average sentiment score across all text' if col == 'avg_sentiment_score' else\n",
    "                    'Average sentiment in renewable energy discussions' if col == 'renewable_energy_avg_sentiment' else\n",
    "                    'Average sentiment in climate/emissions discussions' if col == 'climate_emissions_avg_sentiment' else\n",
    "                    'Weighted combination of all sentiment scores' if col == 'combined_sentiment_score' else\n",
    "                    'Intensity of quantitative claims and metrics' if col == 'quantification_intensity_score' else\n",
    "                    'Intensity of evidence-based statements' if col == 'evidence_intensity_score' else\n",
    "                    'Intensity of aspirational/future-oriented language' if col == 'aspirational_intensity_score' else\n",
    "                    'Vague language percentage (unclear, non-specific terms)' if col == 'vague_density' else\n",
    "                    'Hedge word percentage (uncertainty markers)' if col == 'hedge_density' else\n",
    "                    'Combined unclear language intensity score' if col == 'combined_unclear_intensity' else\n",
    "                    'Percentage of commitments with specific timelines' if col == 'commitment_timeline_pct' else\n",
    "                    'Future orientation percentage (future vs past/present focus)' if col == 'temporal_future_pct' else\n",
    "                    'TF-IDF based document similarity score' if col == 'TFIDF_similarity' else\n",
    "                    'Jaccard similarity coefficient between documents' if col == 'Jaccard_similarity' else\n",
    "                    'SpaCy semantic similarity average' if col == 'SpaCy_avg_similarity' else\n",
    "                    'Ratio of highly similar sentences (SpaCy >0.8)' if col == 'SpaCy_HighSim_Ratio' else\n",
    "                    'Year-over-year combined similarity score from multiple methods' if col == 'similarity_combined' else\n",
    "                    f'Variable: {col}' for col in master_greenwashing.columns\n",
    "                ],\n",
    "                'Source': [\n",
    "                    'Both' if col in ['Company', 'Year'] else\n",
    "                    'Communication Analysis' if any(x in col for x in ['Green_Com_Score', 'Substantiation_Weakness', 'Language_Vagueness', 'Temporal_Orientation', 'Reporting_Consistency', 'gt_freq_pct', 'unique_gt_relative', 'sentiment', 'quantification', 'evidence', 'aspirational', 'vague', 'hedge', 'timeline', 'temporal', 'similarity', 'TFIDF', 'Jaccard', 'SpaCy']) else\n",
    "                    'Performance Analysis' if col == 'Performance_Score' else\n",
    "                    'Greenwashing Analysis' if any(x in col for x in ['Performance_Communication_Gap_Score', 'Greenwashing_Risk', 'Amplified']) else\n",
    "                    'Ensemble Analysis' if 'Ens_Greenwashing_' in col else\n",
    "                    'Sensitivity Analysis' if col in ['Sens_CV_Percent', 'Sens_Score_Range', 'Sens_MAD', 'Sens_Avg_Rank_Shift', 'Sens_Max_Rank_Shift', 'Sens_Sensitivity_Level'] else\n",
    "                    'Unknown' for col in master_greenwashing.columns\n",
    "                ],\n",
    "                'Scale': [\n",
    "                    'Text' if col == 'Company' else\n",
    "                    'Integer (2021, 2022)' if col == 'Year' else\n",
    "                    '0-100 (normalized by year)' if any(x in col for x in ['Score', 'Weakness', 'Vagueness', 'Orientation', 'Consistency']) else\n",
    "                    'Percentage (0-100)' if any(x in col for x in ['pct', 'density']) else\n",
    "                    'Ratio (0+)' if 'Ratio' in col else\n",
    "                    'Score (-100 to +100)' if 'sentiment' in col else\n",
    "                    'Normalized (0-100)' if any(x in col for x in ['intensity', 'similarity', 'combined']) else\n",
    "                    'Ensemble Statistics' if 'Ens_Greenwashing_' in col else\n",
    "                    'Sensitivity Metrics' if col in ['Sens_CV_Percent', 'Sens_Score_Range', 'Sens_MAD', 'Sens_Avg_Rank_Shift', 'Sens_Max_Rank_Shift'] else\n",
    "                    'Categorical' if col == 'Sens_Sensitivity_Level' else\n",
    "                    'Continuous' for col in master_greenwashing.columns\n",
    "                ]\n",
    "            })\n",
    "            \n",
    "            data_dict.to_excel(writer, sheet_name='Data_Dictionary', index=False)\n",
    "            \n",
    "            # Summary statistics by year\n",
    "            if 'Year' in master_greenwashing.columns:\n",
    "                summary_2021 = master_greenwashing[master_greenwashing['Year'] == 2021].describe()\n",
    "                summary_2022 = master_greenwashing[master_greenwashing['Year'] == 2022].describe()\n",
    "                \n",
    "                summary_2021.to_excel(writer, sheet_name='Summary_Stats_2021')\n",
    "                summary_2022.to_excel(writer, sheet_name='Summary_Stats_2022')\n",
    "                \n",
    "            # Create component analysis sheet\n",
    "            component_cols = ['Green_Com_Score', 'Substantiation_Weakness', 'Language_Vagueness', \n",
    "                            'Temporal_Orientation', 'Reporting_Consistency']\n",
    "            if all(col in master_greenwashing.columns for col in component_cols):\n",
    "                component_analysis = pd.DataFrame({\n",
    "                    'Component': component_cols,\n",
    "                    'Mean_2021': [master_greenwashing[master_greenwashing['Year'] == 2021][col].mean() \n",
    "                                 for col in component_cols],\n",
    "                    'Mean_2022': [master_greenwashing[master_greenwashing['Year'] == 2022][col].mean() \n",
    "                                 for col in component_cols],\n",
    "                    'Std_2021': [master_greenwashing[master_greenwashing['Year'] == 2021][col].std() \n",
    "                                for col in component_cols],\n",
    "                    'Std_2022': [master_greenwashing[master_greenwashing['Year'] == 2022][col].std() \n",
    "                                for col in component_cols]\n",
    "                }).round(3)\n",
    "                \n",
    "                component_analysis.to_excel(writer, sheet_name='Component_Analysis', index=False)\n",
    "            \n",
    "            # Add sensitivity analysis summary\n",
    "            if not scenario_impact.empty:\n",
    "                scenario_impact.to_excel(writer, sheet_name='Sensitivity_Summary', index=False)\n",
    "        \n",
    "        print(f\"✓ Master dataset exported successfully to: {output_path}\")\n",
    "        print(f\"  - Master_Greenwashing_Data: Complete dataset ({master_greenwashing.shape[0]} rows, {master_greenwashing.shape[1]} columns)\")\n",
    "        print(f\"  - Data_Dictionary: Column descriptions and sources\")\n",
    "        print(f\"  - Summary_Stats_2021: Descriptive statistics for 2021\")\n",
    "        print(f\"  - Summary_Stats_2022: Descriptive statistics for 2022\")\n",
    "        print(f\"  - Component_Analysis: Component score analysis by year\")\n",
    "        if not scenario_impact.empty:\n",
    "            print(f\"  - Sensitivity_Summary: Sensitivity analysis scenario impacts\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting master dataset: {e}\")\n",
    "else:\n",
    "    print(\"Cannot export: Master dataset is empty\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 7: Display Final Summary\n",
    "# ==========================================\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"MASTER GREENWASHING DATASET CREATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not master_greenwashing.empty:\n",
    "    print(f\"\\nDATASET OVERVIEW:\")\n",
    "    print(f\"  Total records: {len(master_greenwashing):,}\")\n",
    "    print(f\"  Companies: {master_greenwashing['Company'].nunique()}\")\n",
    "    print(f\"  Years: {sorted(master_greenwashing['Year'].unique())}\")\n",
    "    \n",
    "    print(f\"\\nVARIABLE CATEGORIES:\")\n",
    "    \n",
    "    # Count variables by category\n",
    "    main_scores = [col for col in master_greenwashing.columns if col in ['Performance_Communication_Gap_Score', 'Performance_Score', 'Greenwashing_Risk_Absolute_Gap', 'Greenwashing_Risk_Amplified']]\n",
    "    component_scores = [col for col in master_greenwashing.columns if col in ['Green_Com_Score', 'Substantiation_Weakness', 'Language_Vagueness', 'Temporal_Orientation', 'Reporting_Consistency']]\n",
    "    ensemble_stats = [col for col in master_greenwashing.columns if 'Ens_Greenwashing_' in col and any(x in col for x in ['mean', 'median', 'std', 'min', 'max', 'q25', 'q75', 'iqr', 'range'])]\n",
    "    sensitivity_vars = [col for col in master_greenwashing.columns if col in ['Sens_CV_Percent', 'Sens_Score_Range', 'Sens_MAD', 'Sens_Avg_Rank_Shift', 'Sens_Max_Rank_Shift', 'Sens_Sensitivity_Level']]    \n",
    "    raw_variables = [col for col in master_greenwashing.columns if col in ['gt_freq_pct', 'unique_gt_relative', 'avg_sentiment_score', 'renewable_energy_avg_sentiment', 'climate_emissions_avg_sentiment', 'quantification_intensity_score', 'evidence_intensity_score', 'aspirational_intensity_score', 'vague_density', 'hedge_density', 'temporal_future_pct', 'commitment_timeline_pct', 'similarity_combined', 'SpaCy_HighSim_Ratio']]\n",
    "    \n",
    "    print(f\"  Main Greenwashing Scores: {len(main_scores)} variables\")\n",
    "    print(f\"  Communication Component Scores: {len(component_scores)} variables\")\n",
    "    print(f\"  Greenwashing Ensemble Statistics: {len(ensemble_stats)} variables\") \n",
    "    print(f\"  Sensitivity Analysis Variables: {len(sensitivity_vars)} variables\")\n",
    "    print(f\"  Underlying Raw Communication Variables: {len(raw_variables)} variables\")\n",
    "    print(f\"  Total Variables: {master_greenwashing.shape[1]}\")\n",
    "    \n",
    "    print(f\"\\nKEY STATISTICS:\")\n",
    "    if 'Ens_Greenwashing_median_score' in master_greenwashing.columns:\n",
    "        print(f\"  Mean Greenwashing Risk (2021): {master_greenwashing[master_greenwashing['Year'] == 2021]['Ens_Greenwashing_median_score'].mean():.2f}\")\n",
    "        print(f\"  Mean Greenwashing Risk (2022): {master_greenwashing[master_greenwashing['Year'] == 2022]['Ens_Greenwashing_median_score'].mean():.2f}\")\n",
    "    \n",
    "    companies_both_years = len(master_greenwashing.groupby('Company').filter(lambda x: len(x) == 2)['Company'].unique())\n",
    "    print(f\"  Companies with both years: {companies_both_years}\")\n",
    "    \n",
    "    print(f\"\\nSENSITIVITY ANALYSIS:\")\n",
    "    if 'Sens_CV_Percent' in master_greenwashing.columns:\n",
    "        high_sensitivity = len(master_greenwashing[master_greenwashing['Sens_CV_Percent'] > 15]) // 2  # Divide by 2 since each company appears twice\n",
    "        moderate_sensitivity = len(master_greenwashing[(master_greenwashing['Sens_CV_Percent'] > 5) & (master_greenwashing['Sens_CV_Percent'] <= 15)]) // 2\n",
    "        low_sensitivity = len(master_greenwashing[master_greenwashing['Sens_CV_Percent'] <= 5]) // 2\n",
    "        \n",
    "        print(f\"  High sensitivity companies: {high_sensitivity}\")\n",
    "        print(f\"  Moderate sensitivity companies: {moderate_sensitivity}\")\n",
    "        print(f\"  Low sensitivity companies: {low_sensitivity}\")\n",
    "        print(f\"  Average CV across companies: {master_greenwashing['Sens_CV_Percent'].mean():.2f}%\")\n",
    "    \n",
    "    print(f\"\\nKEY VARIABLES CONFIRMED:\")\n",
    "    key_vars = ['Green_Com_Score', 'Substantiation_Weakness', 'Language_Vagueness', 'Temporal_Orientation', 'Reporting_Consistency',\n",
    "                'Ens_Greenwashing_median_score', 'Sens_CV_Percent', 'Sens_Sensitivity_Level', 'gt_freq_pct', 'unique_gt_relative', 'avg_sentiment_score', 'similarity_combined']\n",
    "    \n",
    "    for var in key_vars:\n",
    "        status = \"YES\" if var in master_greenwashing.columns else \"NO\"\n",
    "        print(f\"  {status} {var}\")\n",
    "    \n",
    "    print(f\"\\nFILES CREATED:\")\n",
    "    print(f\"  Master dataset: data/Greenwashing Results/master_greenwashing_dataset.xlsx\")\n",
    "    sheet_count = 6 if not scenario_impact.empty else 5\n",
    "    print(f\"  ({sheet_count} sheets: Master_Greenwashing_Data, Data_Dictionary, Summary_Stats_2021, Summary_Stats_2022, Component_Analysis\" + (\", Sensitivity_Summary\" if not scenario_impact.empty else \"\") + \")\")\n",
    "    \n",
    "    print(f\"\\nREADY FOR ANALYSIS:\")\n",
    "    print(f\"  All communication variables are now in one dataset\")\n",
    "    print(f\"  Both individual component scores and ensemble statistics included\")\n",
    "    print(f\"  Sensitivity analysis metrics integrated\")\n",
    "    print(f\"  Clear variable names with sources and descriptions\")\n",
    "    print(f\"  Complete data dictionary and summary statistics provided\")\n",
    "    \n",
    "    print(f\"\\nTOP 5 HIGHEST RISK COMPANIES BY ENSEMBLE SCORE:\")\n",
    "    if 'Ens_Greenwashing_median_score' in master_greenwashing.columns:\n",
    "        for year in [2021, 2022]:\n",
    "            year_data = master_greenwashing[master_greenwashing['Year'] == year]\n",
    "            if not year_data.empty:\n",
    "                top_risk = year_data.nlargest(5, 'Ens_Greenwashing_median_score')[['Company', 'Ens_Greenwashing_median_score']]\n",
    "                print(f\"\\n  {year}:\")\n",
    "                for i, (_, row) in enumerate(top_risk.iterrows(), 1):\n",
    "                    print(f\"    {i}. {row['Company']}: {row['Ens_Greenwashing_median_score']:.2f}\")\n",
    "else:\n",
    "    print(\"Master dataset creation failed - check input files\")\n",
    "\n",
    "print(f\"\\nVariable 'master_greenwashing' is available in memory for immediate use\")\n",
    "print(\"Ready for comprehensive greenwashing risk analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d727189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Export Master Greenwashing Data as CSV\n",
    "# Place this AFTER the master dataset creation cell\n",
    "\n",
    "print(\"EXPORTING MASTER GREENWASHING DATA AS CSV\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Load the Master_Greenwashing_Data sheet from the Excel file\n",
    "    print(\"Loading Master_Greenwashing_Data sheet...\")\n",
    "    \n",
    "    master_greenwashing_csv = pd.read_excel(\"data/Greenwashing Results/master_greenwashing_dataset.xlsx\", \n",
    "                                           sheet_name='Master_Greenwashing_Data')\n",
    "    \n",
    "    print(f\"✓ Data loaded successfully:\")\n",
    "    print(f\"  - Shape: {master_greenwashing_csv.shape}\")\n",
    "    print(f\"  - Companies: {master_greenwashing_csv['Company'].nunique()}\")\n",
    "    print(f\"  - Years: {sorted(master_greenwashing_csv['Year'].unique())}\")\n",
    "    \n",
    "    # Export as CSV\n",
    "    output_csv_path = \"data/Greenwashing Results/master_greenwashing_data.csv\"\n",
    "    master_greenwashing_csv.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    print(f\"\\n✓ CSV export successful!\")\n",
    "    print(f\"  - File saved to: {output_csv_path}\")\n",
    "    print(f\"  - Records exported: {len(master_greenwashing_csv):,}\")\n",
    "    print(f\"  - Variables exported: {master_greenwashing_csv.shape[1]}\")\n",
    "    \n",
    "    # Display file size info\n",
    "    import os\n",
    "    file_size = os.path.getsize(output_csv_path) / (1024 * 1024)  # Convert to MB\n",
    "    print(f\"  - File size: {file_size:.2f} MB\")\n",
    "    \n",
    "    # Show first few rows and column names for verification\n",
    "    print(f\"\\nFIRST 3 ROWS PREVIEW:\")\n",
    "    print(master_greenwashing_csv.head(3).to_string())\n",
    "    \n",
    "    print(f\"\\nCOLUMN NAMES ({len(master_greenwashing_csv.columns)} total):\")\n",
    "    for i, col in enumerate(master_greenwashing_csv.columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "    \n",
    "    print(f\"\\nREADY FOR UPLOAD:\")\n",
    "    print(f\"  You can now upload: {output_csv_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error exporting CSV: {e}\")\n",
    "    print(\"Trying to use in-memory data instead...\")\n",
    "    \n",
    "    try:\n",
    "        # Fallback: use the in-memory master_greenwashing DataFrame\n",
    "        if 'master_greenwashing' in locals() and not master_greenwashing.empty:\n",
    "            output_csv_path = \"data/Greenwashing Results/master_greenwashing_data.csv\"\n",
    "            master_greenwashing.to_csv(output_csv_path, index=False)\n",
    "            \n",
    "            print(f\"✓ CSV export successful using in-memory data!\")\n",
    "            print(f\"  - File saved to: {output_csv_path}\")\n",
    "            print(f\"  - Records exported: {len(master_greenwashing):,}\")\n",
    "            print(f\"  - Variables exported: {master_greenwashing.shape[1]}\")\n",
    "            \n",
    "            # Display file size info\n",
    "            import os\n",
    "            file_size = os.path.getsize(output_csv_path) / (1024 * 1024)  # Convert to MB\n",
    "            print(f\"  - File size: {file_size:.2f} MB\")\n",
    "            \n",
    "        else:\n",
    "            print(\"No master_greenwashing data available in memory\")\n",
    "            \n",
    "    except Exception as e2:\n",
    "        print(f\"Fallback also failed: {e2}\")\n",
    "\n",
    "print(\"\\nCSV export process complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ffb4c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
