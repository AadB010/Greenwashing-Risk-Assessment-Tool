{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d720608a",
   "metadata": {},
   "source": [
    "# External Validation Using Documented Greenwashing Cases\n",
    "\n",
    "## Overview\n",
    "This module conducts external validation of the GRAT by testing its ability to distinguish between companies with documented greenwashing accusations and those with clean environmental records. It implements the validation approach outlined in the methodology using real-world \"ground truth\" data to assess discriminant validity.\n",
    "\n",
    "## Validation Framework\n",
    "**Data source**: Ensemble analysis results (`ensemble_results_summary_ensforperf.xlsx`, Company_Averages sheet)\n",
    "**Primary metric**: Average median greenwashing scores across 2021-2022 to reflect consistent risk patterns rather than yearly fluctuations\n",
    "**Validation approach**: Systematic comparison between known positive cases and clean record companies using non-parametric statistical testing\n",
    "\n",
    "## Company Classification Groups\n",
    "Based on systematic online search examining all 14 sample companies between January 2020-December 2023:\n",
    "\n",
    "### Known Positive Cases (n=3)\n",
    "- **CEZ**: Documented greenwashing accusations from credible sources\n",
    "- **Ørsted**: Documented environmental claims controversies  \n",
    "- **PGE**: Documented greenwashing-related criticisms\n",
    "- **Classification basis**: Multi-language search across Greenpeace offices, investigative outlets, consumer protection agencies, and energy sector publications\n",
    "\n",
    "### Clean Record Companies (n=11)\n",
    "- **All remaining companies**: No documented greenwashing accusations found within the timeframe\n",
    "- **Historical cases excluded**: Past accusations outside 2020-2023 timeframe classified as clean record\n",
    "\n",
    "## Statistical Testing Methodology\n",
    "**Primary test**: Mann-Whitney U test (Wilcoxon rank-sum test)\n",
    "- **Hypothesis**: H₁: Known positive cases score significantly higher than clean record companies (one-tailed test)\n",
    "- **Justification**: Non-parametric test appropriate for small samples without normal distribution assumptions\n",
    "- **Effect size**: Rank-biserial correlation (r) calculated as r = 1 - (2U)/(n₁ × n₂)\n",
    "\n",
    "## Key Validation Metrics Calculated\n",
    "1. **Group score distributions**: Median, mean, and range for both validation groups\n",
    "2. **Statistical significance**: p-value testing at α = 0.05 significance level\n",
    "3. **Effect size interpretation**: Small (≥0.1), medium (≥0.3), or large (≥0.5) effects\n",
    "4. **Ranking analysis**: Position of known positive cases within overall company rankings\n",
    "5. **Threshold analysis**: Number of known positive cases scoring above clean record median\n",
    "\n",
    "## Critical Statistical Limitations\n",
    "**Sample size constraints**: Only 14 total observations create severe statistical power limitations\n",
    "**Central Limit Theorem requirements**: Mann-Whitney U test effectiveness compromised with such small samples\n",
    "**Validation interpretation**: Results should be considered **indicative only**, not conclusive evidence\n",
    "**Random phenomenon possibility**: Observed patterns could represent chance variation between 2021-2022 rather than systematic GRAT accuracy\n",
    "\n",
    "## External Validation Outputs\n",
    "- **Discriminant validity assessment**: Whether GRAT can separate documented cases from clean records\n",
    "- **Effect size quantification**: Magnitude of difference between validation groups  \n",
    "- **Ranking validation**: Positional analysis of known cases within risk score distribution\n",
    "- **Statistical confidence measures**: p-values and confidence intervals (with appropriate caveats about small sample limitations)\n",
    "\n",
    "This validation provides preliminary evidence for GRAT effectiveness while acknowledging that larger datasets would enable substantially more robust statistical validation with meaningful power for definitive conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0656c248",
   "metadata": {},
   "source": [
    "# Known Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da549176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known Case Validation Analysis\n",
    "# Framework Validation Using Mann-Whitney U Test\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "file_path = \"data/Greenwashing Results/ensemble_results_summary_ensforperf.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name=\"Company_Averages\")\n",
    "\n",
    "print(\"Data loaded successfully:\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1668662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and prepare the data\n",
    "# Use the Avg_Median_Score as our primary validation metric\n",
    "companies = df['Organization'].tolist()\n",
    "scores = df['Avg_Median_Score'].tolist()\n",
    "\n",
    "print(\"Companies and their average median scores:\")\n",
    "for company, score in zip(companies, scores):\n",
    "    print(f\"{company}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ba9513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define validation groups based on systematic search results\n",
    "known_positive_cases = ['CEZ', 'Orsted', 'PGE']  # Companies with documented greenwashing accusations\n",
    "clean_record_companies = [company for company in companies if company not in known_positive_cases]\n",
    "\n",
    "print(\"=== VALIDATION GROUPS ===\")\n",
    "print(f\"\\nKnown Positive Cases (n={len(known_positive_cases)}):\")\n",
    "for company in known_positive_cases:\n",
    "    idx = companies.index(company)\n",
    "    print(f\"  {company}: {scores[idx]:.2f}\")\n",
    "\n",
    "print(f\"\\nClean Record Companies (n={len(clean_record_companies)}):\")\n",
    "for company in clean_record_companies:\n",
    "    idx = companies.index(company)\n",
    "    print(f\"  {company}: {scores[idx]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e4325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract scores for statistical testing\n",
    "positive_scores = [scores[companies.index(company)] for company in known_positive_cases]\n",
    "clean_scores = [scores[companies.index(company)] for company in clean_record_companies]\n",
    "\n",
    "print(\"=== SCORE DISTRIBUTIONS ===\")\n",
    "print(f\"\\nKnown Positive Cases:\")\n",
    "print(f\"  Scores: {positive_scores}\")\n",
    "print(f\"  Median: {np.median(positive_scores):.2f}\")\n",
    "print(f\"  Mean: {np.mean(positive_scores):.2f}\")\n",
    "print(f\"  Range: {min(positive_scores):.2f} - {max(positive_scores):.2f}\")\n",
    "\n",
    "print(f\"\\nClean Record Companies:\")\n",
    "print(f\"  Scores: {clean_scores}\")\n",
    "print(f\"  Median: {np.median(clean_scores):.2f}\")\n",
    "print(f\"  Mean: {np.mean(clean_scores):.2f}\")\n",
    "print(f\"  Range: {min(clean_scores):.2f} - {max(clean_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95cc55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Mann-Whitney U test\n",
    "# H0: No difference between groups\n",
    "# H1: Known positive cases have higher scores (one-tailed test)\n",
    "\n",
    "statistic, p_value = mannwhitneyu(positive_scores, clean_scores, alternative='greater')\n",
    "\n",
    "# Calculate effect size (rank-biserial correlation)\n",
    "n1, n2 = len(positive_scores), len(clean_scores)\n",
    "r = 1 - (2 * statistic) / (n1 * n2)\n",
    "\n",
    "print(\"=== MANN-WHITNEY U TEST RESULTS ===\")\n",
    "print(f\"\\nU-statistic: {statistic}\")\n",
    "print(f\"p-value (one-tailed): {p_value:.4f}\")\n",
    "print(f\"Effect size (r): {r:.3f}\")\n",
    "\n",
    "# Effect size interpretation\n",
    "if abs(r) >= 0.5:\n",
    "    effect_interpretation = \"Large effect\"\n",
    "elif abs(r) >= 0.3:\n",
    "    effect_interpretation = \"Medium effect\"\n",
    "elif abs(r) >= 0.1:\n",
    "    effect_interpretation = \"Small effect\"\n",
    "else:\n",
    "    effect_interpretation = \"Negligible effect\"\n",
    "\n",
    "print(f\"Effect size interpretation: {effect_interpretation}\")\n",
    "\n",
    "# Statistical significance\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    significance = \"Statistically significant\"\n",
    "else:\n",
    "    significance = \"Not statistically significant\"\n",
    "\n",
    "print(f\"Statistical significance (α = {alpha}): {significance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e854198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional validation metrics\n",
    "# Company rankings and individual case analysis\n",
    "\n",
    "# Create combined dataset with rankings\n",
    "all_data = list(zip(companies, scores))\n",
    "all_data.sort(key=lambda x: x[1], reverse=True) # Sort by score (highest first)\n",
    "\n",
    "print(\"=== COMPANY RANKINGS ===\")\n",
    "print(\"Rank | Company | Score | Group\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "positive_ranks = []\n",
    "for rank, (company, score) in enumerate(all_data, 1):\n",
    "    group = \"Known Positive\" if company in known_positive_cases else \"Clean Record\"\n",
    "    print(f\"{rank:2d}   | {company:<8s} | {score:5.2f} | {group}\")\n",
    "    \n",
    "    if company in known_positive_cases:\n",
    "        positive_ranks.append(rank)\n",
    "\n",
    "print(f\"\\nKnown positive cases rank positions: {positive_ranks}\")\n",
    "print(f\"All known positive cases in top half (≤7): {all(rank <= 7 for rank in positive_ranks)}\")\n",
    "\n",
    "# Count how many known positive cases score above clean record median\n",
    "clean_median = np.median(clean_scores)\n",
    "positive_above_clean_median = sum(1 for score in positive_scores if score > clean_median)\n",
    "print(f\"\\nKnown positive cases scoring above clean record median ({clean_median:.2f}): {positive_above_clean_median}/{len(positive_scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
