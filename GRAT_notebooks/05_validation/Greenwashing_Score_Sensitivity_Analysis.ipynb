{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c0ac7d",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis for Greenwashing Risk Assessment\n",
    "\n",
    "## Overview\n",
    "This module extends the main greenwashing scoring with comprehensive sensitivity analysis to test methodological robustness. It examines how sub-component weight variations affect final scores and company rankings, addressing theoretical uncertainty about optimal weight allocations within communication dimensions.\n",
    "\n",
    "## Sensitivity Scenarios (17 total)\n",
    "- **Baseline**: Literature-based weights as established in main analysis\n",
    "- **Systematic variations**: ±10% adjustments for most components, ±5% for substantiation weakness (preserving Evidence ≥ Aspirational > Quantified hierarchy)\n",
    "- **Component coverage**: All sub-components within sentiment, green terms, substantiation, language vagueness, temporal orientation, and reporting consistency dimensions\n",
    "- **Hierarchy preservation**: Weight modifications maintain theoretical priorities while testing robustness boundaries\n",
    "\n",
    "## Enhanced Ensemble Processing\n",
    "**Per-scenario analysis**: Each of 17 scenarios runs through complete ensemble methodology with all 59,881 valid weight combinations\n",
    "**Computational scope**: Total analysis spans ~1 million weight combination tests across all scenarios\n",
    "**Consistency maintenance**: Same methodology as main analysis ensures comparable results\n",
    "\n",
    "## Sensitivity-Specific Metrics Added\n",
    "\n",
    "### Ranking Stability Analysis\n",
    "- **R̄S (R-bar-S)**: Average ranking shift across scenarios compared to baseline rankings\n",
    "- **Robustness classification**: R̄S < 2.0 (high), 2.0-4.0 (moderate), >4.0 (low robustness)\n",
    "- **Cross-year tracking**: Ranking sensitivity measured separately for 2021 and 2022\n",
    "\n",
    "### Score Variability Metrics  \n",
    "- **MAD (Mean Absolute Deviation)**: Average score shifts from baseline values across scenarios\n",
    "- **CV (Coefficient of Variation)**: Standardized variability measure enabling comparison across score levels\n",
    "- **Score Range**: Maximum uncertainty span (max score - min score) for each company\n",
    "- **High sensitivity threshold**: Companies with CV >15% or score range >10 points flagged for careful interpretation\n",
    "\n",
    "### Company-Level Sensitivity Classification\n",
    "- **Sensitivity_Level**: High/Moderate/Low classification based on combined CV, score range, and ranking shift metrics\n",
    "- **Max_Rank_Shift**: Largest ranking change experienced across all scenarios\n",
    "- **Avg_Rank_Shift**: Average ranking movement indicating overall positional stability\n",
    "\n",
    "## Statistical Framework\n",
    "Follows Saisana et al. (2005) methodology for composite index sensitivity testing, distinguishing between components affecting rankings versus absolute score values. Results identify both methodological elements causing largest disruptions and companies requiring cautious interpretation due to weight sensitivity.\n",
    "\n",
    "## Key Sensitivity Output Variables\n",
    "- **Sens_CV_Percent**: Coefficient of variation across weight scenarios (%)\n",
    "- **Sens_Score_Range**: Score range across all weight scenarios  \n",
    "- **Sens_MAD**: Mean absolute deviation from baseline\n",
    "- **Sens_Avg_Rank_Shift**: Average ranking shift across scenarios\n",
    "- **Sens_Max_Rank_Shift**: Maximum ranking shift across scenarios\n",
    "- **Sens_Sensitivity_Level**: Sensitivity classification (High/Moderate/Low)\n",
    "\n",
    "This analysis validates GRAT robustness by revealing which sub-component weights most influence results and identifying companies whose assessments remain stable versus those requiring methodological attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a62b08",
   "metadata": {},
   "source": [
    "## Loading and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load all Excel files\n",
    "df_density = pd.read_excel('data/NLP/Results/Communication_Score_df_Density.xlsx')\n",
    "df_context = pd.read_excel('data/NLP/Results/Communication_Score_df_Context.xlsx')\n",
    "df_similarity = pd.read_excel('data/NLP/Results/Similarity/similarity_analysis_results.xlsx')\n",
    "df_sentiment = pd.read_excel('data/NLP/Results/Overall_Sentiment_Analysis.xlsx')\n",
    "df_hedge_vague = pd.read_excel('data/NLP/Results/Communication_Score_df_Hedge_Vague.xlsx')\n",
    "df_topics = pd.read_excel('data/NLP/Results/Communication_Score_df_Topics.xlsx')\n",
    "df_topic_sentiment = pd.read_excel('data/NLP/Results/Topic_Weighted_Sentiment_Analysis.xlsx')\n",
    "\n",
    "# For each df print the name of the first column\n",
    "dataframes = [df_density, df_context, df_similarity, df_sentiment, df_hedge_vague, df_topics, df_topic_sentiment]\n",
    "for df in dataframes:\n",
    "    first_col = df.columns[0]\n",
    "    print(f\"First column: {first_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092c29e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant metrics from each dataframe\n",
    "density_metrics = df_density[['organization', 'year', 'gt_freq_pct', 'unique_gt_relative']].copy()\n",
    "\n",
    "context_metrics = df_context[['organization', 'year', 'temporal_past_pct', 'temporal_present_pct', \n",
    "                             'temporal_future_pct', 'quantification_intensity_score', \n",
    "                             'evidence_intensity_score', 'aspirational_intensity_score']].copy()\n",
    "\n",
    "similarity_metrics = df_similarity[['Company', 'TFIDF_Doc', 'Jaccard', 'SpaCy_HighSim_Ratio', 'SpaCy_Avg_Similarity']].copy()\n",
    "similarity_metrics.rename(columns={'Company': 'organization'}, inplace=True)\n",
    "\n",
    "sentiment_metrics = df_sentiment[['organization', 'year', 'avg_sentiment_score', 'sentiment_confidence', 'opportunity_ratio', 'risk_ratio'\n",
    "]].copy()\n",
    "\n",
    "hedge_vague_metrics = df_hedge_vague[['organization', 'year', 'hedge_intensity_score', \n",
    "                                     'vague_intensity_score', 'commitment_timeline_pct', 'total_unclear_density',\n",
    "                                     'combined_intensity_score']].copy()\n",
    "\n",
    "topics_metrics = df_topics[['organization', 'year', 'renewable_energy_density', 'climate_emissions_density']].copy()\n",
    "\n",
    "topic_sentiment_metrics = df_topic_sentiment[['organization', 'year', 'renewable_energy_avg_sentiment', \n",
    "                                             'climate_emissions_avg_sentiment']].copy()\n",
    "\n",
    "# Standardize organization names by replacing underscores with spaces\n",
    "density_metrics['organization'] = density_metrics['organization'].str.replace('_', ' ')\n",
    "context_metrics['organization'] = context_metrics['organization'].str.replace('_', ' ')\n",
    "sentiment_metrics['organization'] = sentiment_metrics['organization'].str.replace('_', ' ')\n",
    "hedge_vague_metrics['organization'] = hedge_vague_metrics['organization'].str.replace('_', ' ')\n",
    "topics_metrics['organization'] = topics_metrics['organization'].str.replace('_', ' ')\n",
    "topic_sentiment_metrics['organization'] = topic_sentiment_metrics['organization'].str.replace('_', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8eec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all dataframes\n",
    "df = density_metrics.merge(context_metrics, on=['organization', 'year'], how='outer')\n",
    "df = df.merge(sentiment_metrics, on=['organization', 'year'], how='outer')\n",
    "df = df.merge(hedge_vague_metrics, on=['organization', 'year'], how='outer')\n",
    "df = df.merge(topics_metrics, on=['organization', 'year'], how='outer')\n",
    "df = df.merge(topic_sentiment_metrics, on=['organization', 'year'], how='outer')\n",
    "df = df.merge(similarity_metrics, on='organization', how='outer')\n",
    "\n",
    "# Rename organization column to Organization\n",
    "df.rename(columns={'organization': 'Organization'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04bae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract first word or apply exceptions\n",
    "def simplify_org_name(name):\n",
    "    if name == 'Polska Grupa Energetyczna PGE SA':\n",
    "        return 'PGE'\n",
    "    elif name == 'AKENERJİ ELEKTRİK ÜRETİM A.Ş.':\n",
    "        return 'Akenerji'\n",
    "    else:\n",
    "        return name.split()[0]\n",
    "\n",
    "# Apply the function to the 'Organization' column\n",
    "df.loc[:, 'Organization'] = df['Organization'].apply(simplify_org_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b7716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values and show which metrics and organizations have them\n",
    "print(\"MISSING DATA ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get columns with NaN values\n",
    "cols_with_nan = df.columns[df.isnull().any()].tolist()\n",
    "\n",
    "if cols_with_nan:\n",
    "    print(f\"Metrics with NaN values: {cols_with_nan}\")\n",
    "    print()\n",
    "    \n",
    "    for col in cols_with_nan:\n",
    "        nan_rows = df[df[col].isnull()]\n",
    "        if not nan_rows.empty:\n",
    "            print(f\"Metric: {col}\")\n",
    "            print(f\"Organizations with missing data:\")\n",
    "            for _, row in nan_rows.iterrows():\n",
    "                if 'year' in df.columns:\n",
    "                    print(f\"  - {row['Organization']} ({row['year']})\")\n",
    "                else:\n",
    "                    print(f\"  - {row['Organization']}\")\n",
    "            print()\n",
    "else:\n",
    "    print(\"No NaN values found in the dataset.\")\n",
    "\n",
    "# Summary statistics\n",
    "total_cells = df.shape[0] * df.shape[1]\n",
    "nan_cells = df.isnull().sum().sum()\n",
    "print(f\"Total cells: {total_cells}\")\n",
    "print(f\"NaN cells: {nan_cells}\")\n",
    "print(f\"Missing data percentage: {(nan_cells/total_cells)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2edaad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned Excel with new structure\n",
    "ensemble_perf = pd.read_excel('data/Performance/ensemble_performance_scores.xlsx')\n",
    "\n",
    "# Confirm actual column names\n",
    "print(\"Columns:\", ensemble_perf.columns)\n",
    "\n",
    "# Fix name consistency (Ørsted → Orsted)\n",
    "ensemble_perf['Organization'] = ensemble_perf['Organization'].replace('Ørsted', 'Orsted')\n",
    "\n",
    "# Rename median_score to Performance_Score (data is already in long format)\n",
    "ensemble_perf = ensemble_perf.rename(columns={'median_score': 'Performance_Score'})\n",
    "\n",
    "# Merge with your main DataFrame\n",
    "df = df.merge(ensemble_perf[['Organization', 'year', 'Performance_Score']], on=['Organization', 'year'], how='left')\n",
    "\n",
    "# Diagnostics\n",
    "print(\"Performance scores added to df\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Performance scores available: {df['Performance_Score'].notna().sum()}/{len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c798bc",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13136686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize scores per year (0-100 scale)\n",
    "def normalize_by_year(df, column):\n",
    "    normalized_col = f\"{column}\"\n",
    "    df[normalized_col] = df.groupby('year')[column].transform(\n",
    "        lambda x: (x - x.min()) / (x.max() - x.min()) * 100 if x.max() != x.min() else 50\n",
    "    )\n",
    "    return normalized_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11811a6b",
   "metadata": {},
   "source": [
    "## Aditional metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0047b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize similarity values\n",
    "normalize_by_year(df, 'TFIDF_Doc')\n",
    "normalize_by_year(df, 'Jaccard')\n",
    "normalize_by_year(df, 'SpaCy_Avg_Similarity')\n",
    "\n",
    "# Create similarity combined score (average of three similarity metrics)\n",
    "df['similarity_combined'] = (\n",
    "    (df['TFIDF_Doc'] + \n",
    "     df['Jaccard'] + \n",
    "     df['SpaCy_Avg_Similarity']) / 3\n",
    ").round(2)\n",
    "\n",
    "\n",
    "# Calculate an other additional metric\n",
    "# Add future vs past+present ratio\n",
    "df['future_vs_past_present_ratio'] = (\n",
    "    df['temporal_future_pct'] / \n",
    "    (df['temporal_past_pct'] + df['temporal_present_pct'])\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed8d1be",
   "metadata": {},
   "source": [
    "## Create df used for greenwashing score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a1d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of df\n",
    "greenwashing_df = df.copy()\n",
    "\n",
    "# Drop specified columns\n",
    "columns_to_drop = [\n",
    "    'temporal_past_pct', 'temporal_present_pct', 'temporal_future_pct',\n",
    "    'TFIDF_Doc', 'Jaccard', 'SpaCy_Avg_Similarity',\n",
    "    'sentiment_confidence', 'opportunity_ratio', 'risk_ratio',\n",
    "    'total_unclear_density', 'renewable_energy_density', 'climate_emissions_density'\n",
    "]\n",
    "greenwashing_df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# Normalize all numeric columns except 'year' and 'Performance_Score'\n",
    "numeric_cols = greenwashing_df.select_dtypes(include='number').columns\n",
    "numeric_cols = [col for col in numeric_cols if col != 'year'] # was: ...if col not in ['year', 'Performance_Score'] OR: if col != 'year'\n",
    "\n",
    "for col in numeric_cols:\n",
    "    normalize_by_year(greenwashing_df, col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbe7b76",
   "metadata": {},
   "source": [
    "# Calculation of Greenwashing Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b67e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined sentiment score\n",
    "\n",
    "# Calculate combined sentiment score\n",
    "greenwashing_df['combined_sentiment_score'] = (\n",
    "    0.6 * greenwashing_df['avg_sentiment_score'] +     \n",
    "    0.2 * greenwashing_df['renewable_energy_avg_sentiment'] +\n",
    "    0.2 * greenwashing_df['climate_emissions_avg_sentiment']\n",
    ").round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6fe2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined green term score\n",
    "\n",
    "# Calculate combined sentiment score\n",
    "greenwashing_df['combined_green_score'] = (\n",
    "    0.7 * greenwashing_df['gt_freq_pct'] +     \n",
    "    0.3 * greenwashing_df['unique_gt_relative']\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate communication score\n",
    "greenwashing_df['Green_Com_Score'] = (\n",
    "    0.4 * greenwashing_df['combined_green_score'] +\n",
    "    0.6 * greenwashing_df['combined_sentiment_score']\n",
    ").round(2)\n",
    "\n",
    "# Normalize Green_Com_Score per year to 0–100 scale\n",
    "normalize_by_year(greenwashing_df, 'Green_Com_Score')\n",
    "\n",
    "print(\"Communication score created\")\n",
    "print(f\"Mean score: {greenwashing_df['Green_Com_Score'].mean():.2f}\")\n",
    "print(f\"Score range: {greenwashing_df['Green_Com_Score'].min():.2f} - {greenwashing_df['Green_Com_Score'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da0e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute gap between performance and communication\n",
    "greenwashing_df['Greenwashing_Risk_Abs'] = (\n",
    "    greenwashing_df['Green_Com_Score'] - greenwashing_df['Performance_Score']\n",
    ").round(2)\n",
    "\n",
    "# Calculate yearly medians for classic greenwashing pattern\n",
    "yearly_medians = greenwashing_df.groupby('year').agg({\n",
    "    'Performance_Score': 'median',\n",
    "    'Green_Com_Score': 'median'\n",
    "})\n",
    "\n",
    "# Identify classic greenwashing pattern\n",
    "greenwashing_df['Classic_Greenwashing'] = greenwashing_df.apply(\n",
    "    lambda row: (row['Green_Com_Score'] > yearly_medians.loc[row['year'], 'Green_Com_Score']) and \n",
    "                (row['Performance_Score'] < yearly_medians.loc[row['year'], 'Performance_Score']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Apply 1.5x amplifier for classic greenwashing\n",
    "greenwashing_df['Amplified_Score'] = greenwashing_df.apply(\n",
    "    lambda row: row['Greenwashing_Risk_Abs'] * 1.5 if row['Classic_Greenwashing'] else row['Greenwashing_Risk_Abs'],\n",
    "    axis=1\n",
    ").round(2)\n",
    "\n",
    "# Normalize to 0–100 scale scale within each year\n",
    "normalize_by_year(greenwashing_df, 'Amplified_Score')\n",
    "greenwashing_df['Greenwashing_Score'] = greenwashing_df['Amplified_Score'].round(2)\n",
    "\n",
    "# Clean up intermediate columns\n",
    "greenwashing_df = greenwashing_df.drop(['Classic_Greenwashing'], axis=1) # kept 'Base_Hybrid_Score', 'Amplified_Score'\n",
    "\n",
    "print(\"Greenwashing score calculated\")\n",
    "print(f\"Mean score: {greenwashing_df['Greenwashing_Score'].mean():.2f}\")\n",
    "print(f\"Companies with classic greenwashing pattern: {greenwashing_df.apply(lambda row: (row['Green_Com_Score'] > yearly_medians.loc[row['year'], 'Green_Com_Score']) and (row['Performance_Score'] < yearly_medians.loc[row['year'], 'Performance_Score']), axis=1).sum()}\")\n",
    "\n",
    "# 2021 highest scores\n",
    "print(f\"\\n2021 HIGHEST GREENWASHING SCORES:\")\n",
    "highest_2021 = greenwashing_df[greenwashing_df['year'] == 2021].nlargest(10, 'Greenwashing_Score')[['Organization', 'Greenwashing_Score']]\n",
    "print(highest_2021.to_string(index=False))\n",
    "\n",
    "# 2022 highest scores  \n",
    "print(f\"\\n2022 HIGHEST GREENWASHING SCORES:\")\n",
    "highest_2022 = greenwashing_df[greenwashing_df['year'] == 2022].nlargest(10, 'Greenwashing_Score')[['Organization', 'Greenwashing_Score']]\n",
    "print(highest_2022.to_string(index=False))\n",
    "\n",
    "# Average scores per company across both years\n",
    "print(f\"\\nHIGHEST AVERAGE GREENWASHING SCORES (2021-2022):\")\n",
    "company_averages = greenwashing_df.groupby('Organization')['Greenwashing_Score'].agg(['mean', 'count']).reset_index()\n",
    "company_averages.columns = ['Organization', 'Avg_Greenwashing_Score', 'Years_Count']\n",
    "company_averages = company_averages[company_averages['Years_Count'] == 2]  # Only companies with data for both years\n",
    "highest_avg = company_averages.nlargest(10, 'Avg_Greenwashing_Score')[['Organization', 'Avg_Greenwashing_Score']]\n",
    "highest_avg['Avg_Greenwashing_Score'] = highest_avg['Avg_Greenwashing_Score'].round(2)\n",
    "print(highest_avg.to_string(index=False))\n",
    "\n",
    "print(f\"\\nCompanies with data for both years: {len(company_averages)}\")\n",
    "print(f\"Average greenwashing score across all companies (both years): {company_averages['Avg_Greenwashing_Score'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d1cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save performance communication gap data to Excel\n",
    "import os\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs('data/Greenwashing Results', exist_ok=True)\n",
    "\n",
    "# Save the dataframe to Excel\n",
    "greenwashing_df.to_excel('data/Greenwashing Results/performance_communication_gap.xlsx', \n",
    "                        index=False)\n",
    "\n",
    "print(\"Performance communication gap data saved to: data/Greenwashing Results/performance_communication_gap.xlsx\")\n",
    "print(f\"Saved {len(greenwashing_df)} rows and {len(greenwashing_df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d704c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greenwashing Score Quadrant Visualization\n",
    "# Communication Score vs Performance Score Analysis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Calculate medians for thresholds\n",
    "comm_median = greenwashing_df['Green_Com_Score'].median()\n",
    "perf_median = greenwashing_df['Performance_Score'].median()\n",
    "\n",
    "# Create quadrant classification\n",
    "def classify_quadrant(row):\n",
    "    comm_high = row['Green_Com_Score'] >= comm_median\n",
    "    perf_high = row['Performance_Score'] >= perf_median\n",
    "    \n",
    "    if comm_high and not perf_high:\n",
    "        return 'Potential_Greenwashing'\n",
    "    elif comm_high and perf_high:\n",
    "        return 'Green_Leaders'\n",
    "    elif not comm_high and not perf_high:\n",
    "        return 'Laggards'\n",
    "    else:\n",
    "        return 'Under_Communicators'\n",
    "\n",
    "greenwashing_df['Quadrant'] = greenwashing_df.apply(classify_quadrant, axis=1)\n",
    "\n",
    "# Define colors for quadrants\n",
    "colors = {'Potential_Greenwashing': 'red', 'Green_Leaders': 'green', \n",
    "          'Laggards': 'gray', 'Under_Communicators': 'blue'}\n",
    "\n",
    "# Filter data by year\n",
    "df_2021 = greenwashing_df[greenwashing_df['year'] == 2021]\n",
    "df_2022 = greenwashing_df[greenwashing_df['year'] == 2022]\n",
    "\n",
    "# Create the visualization function\n",
    "def create_quadrant_plot(data, year, ax):\n",
    "    \"\"\"Create a quadrant plot for the given year\"\"\"\n",
    "    \n",
    "    # Calculate year-specific medians\n",
    "    year_comm_median = data['Green_Com_Score'].median()\n",
    "    year_perf_median = data['Performance_Score'].median()\n",
    "    \n",
    "    # Plot each quadrant\n",
    "    for quadrant in data['Quadrant'].unique():\n",
    "        if pd.isna(quadrant):\n",
    "            continue\n",
    "        subset = data[data['Quadrant'] == quadrant]\n",
    "        ax.scatter(subset['Performance_Score'], subset['Green_Com_Score'], \n",
    "                  c=colors[quadrant], label=quadrant.replace('_', ' '), \n",
    "                  alpha=0.7, s=100, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    # Add company name annotations\n",
    "    for _, row in data.iterrows():\n",
    "        if pd.notna(row['Performance_Score']) and pd.notna(row['Green_Com_Score']):\n",
    "            ax.annotate(row['Organization'], \n",
    "                       (row['Performance_Score'], row['Green_Com_Score']),\n",
    "                       xytext=(5, 5), textcoords='offset points', \n",
    "                       fontsize=10, alpha=0.9, fontweight='bold')\n",
    "    \n",
    "    # Add median lines\n",
    "    ax.axvline(year_perf_median, color='gray', linestyle='--', alpha=0.5, linewidth=2)\n",
    "    ax.axhline(year_comm_median, color='gray', linestyle='--', alpha=0.5, linewidth=2)\n",
    "    \n",
    "    # Add quadrant labels\n",
    "    perf_range = ax.get_xlim()\n",
    "    comm_range = ax.get_ylim()\n",
    "    \n",
    "    # Potential Greenwashing (Top Left)\n",
    "    ax.text(perf_range[0] + (year_perf_median - perf_range[0])/2, \n",
    "            year_comm_median + (comm_range[1] - year_comm_median)/2,\n",
    "            'Potential\\nGreenwashing', ha='center', va='center', fontsize=11, \n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightcoral', alpha=0.7))\n",
    "    \n",
    "    # Green Leaders (Top Right)\n",
    "    ax.text(year_perf_median + (perf_range[1] - year_perf_median)/2, \n",
    "            year_comm_median + (comm_range[1] - year_comm_median)/2,\n",
    "            'Green\\nLeaders', ha='center', va='center', fontsize=11, \n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightgreen', alpha=0.7))\n",
    "    \n",
    "    # Laggards (Bottom Left)\n",
    "    ax.text(perf_range[0] + (year_perf_median - perf_range[0])/2, \n",
    "            comm_range[0] + (year_comm_median - comm_range[0])/2,\n",
    "            'Laggards', ha='center', va='center', fontsize=11, \n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightgray', alpha=0.7))\n",
    "    \n",
    "    # Under Communicators (Bottom Right)\n",
    "    ax.text(year_perf_median + (perf_range[1] - year_perf_median)/2, \n",
    "            comm_range[0] + (year_comm_median - comm_range[0])/2,\n",
    "            'Under\\nCommunicators', ha='center', va='center', fontsize=11, \n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightblue', alpha=0.7))\n",
    "    \n",
    "    # Customize axes\n",
    "    ax.set_xlabel('Performance Score', fontsize=14)\n",
    "    ax.set_ylabel('Communication Score', fontsize=14)\n",
    "    ax.set_title(f'Greenwashing Detection {year}', fontsize=16, fontweight='bold')\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(0, 1))\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(24, 10))\n",
    "\n",
    "# FIRST GRAPH: 2021\n",
    "if len(df_2021) > 0:\n",
    "    create_quadrant_plot(df_2021, 2021, axes[0])\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, 'No data available for 2021', \n",
    "                ha='center', va='center', transform=axes[0].transAxes, fontsize=14)\n",
    "    axes[0].set_title('Greenwashing Detection 2021', fontsize=16, fontweight='bold')\n",
    "\n",
    "# SECOND GRAPH: 2022\n",
    "if len(df_2022) > 0:\n",
    "    create_quadrant_plot(df_2022, 2022, axes[1])\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No data available for 2022', \n",
    "                ha='center', va='center', transform=axes[1].transAxes, fontsize=14)\n",
    "    axes[1].set_title('Greenwashing Detection 2022', fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed quadrant analysis\n",
    "print(\"GREENWASHING QUADRANT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for year in [2021, 2022]:\n",
    "    year_data = greenwashing_df[greenwashing_df['year'] == year]\n",
    "    if len(year_data) > 0:\n",
    "        print(f\"\\n{year} QUADRANT DISTRIBUTION:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        quadrant_counts = year_data['Quadrant'].value_counts()\n",
    "        for quadrant, count in quadrant_counts.items():\n",
    "            if pd.notna(quadrant):\n",
    "                print(f\"{quadrant.replace('_', ' ')}: {count} companies\")\n",
    "        \n",
    "        print(f\"\\nMedian Communication Score: {year_data['Green_Com_Score'].median():.2f}\")\n",
    "        print(f\"Median Performance Score: {year_data['Performance_Score'].median():.2f}\")\n",
    "        \n",
    "        # Show companies in each quadrant with their scores\n",
    "        print(f\"\\nDETAILED BREAKDOWN ({year}):\")\n",
    "        for quadrant in ['Potential_Greenwashing', 'Green_Leaders', 'Laggards', 'Under_Communicators']:\n",
    "            quad_data = year_data[year_data['Quadrant'] == quadrant]\n",
    "            if len(quad_data) > 0:\n",
    "                print(f\"\\n{quadrant.replace('_', ' ')} ({len(quad_data)} companies):\")\n",
    "                for _, row in quad_data.iterrows():\n",
    "                    print(f\"  {row['Organization']}: Com={row['Green_Com_Score']:.1f}, Perf={row['Performance_Score']:.1f}, Risk={row['Greenwashing_Score']:.1f}\")\n",
    "\n",
    "# Analysis of companies that changed quadrants\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"QUADRANT MOVEMENT ANALYSIS (2021 → 2022)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "companies_both_years = set(df_2021['Organization'].unique()) & set(df_2022['Organization'].unique())\n",
    "\n",
    "movements = []\n",
    "for company in companies_both_years:\n",
    "    quad_2021 = df_2021[df_2021['Organization'] == company]['Quadrant'].iloc[0]\n",
    "    quad_2022 = df_2022[df_2022['Organization'] == company]['Quadrant'].iloc[0]\n",
    "    \n",
    "    if quad_2021 != quad_2022:\n",
    "        movements.append({\n",
    "            'Company': company,\n",
    "            'From': quad_2021,\n",
    "            'To': quad_2022\n",
    "        })\n",
    "\n",
    "if movements:\n",
    "    print(f\"\\nCompanies that changed quadrants: {len(movements)}\")\n",
    "    for movement in movements:\n",
    "        print(f\"{movement['Company']}: {movement['From'].replace('_', ' ')} → {movement['To'].replace('_', ' ')}\")\n",
    "else:\n",
    "    print(\"\\nNo companies changed quadrants between 2021 and 2022\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"Overall median Communication Score: {greenwashing_df['Green_Com_Score'].median():.2f}\")\n",
    "print(f\"Overall median Performance Score: {greenwashing_df['Performance_Score'].median():.2f}\")\n",
    "print(f\"Overall median Greenwashing Score: {greenwashing_df['Greenwashing_Score'].median():.2f}\")\n",
    "\n",
    "print(f\"\\nCorrelation between Communication and Performance: {greenwashing_df['Green_Com_Score'].corr(greenwashing_df['Performance_Score']):.3f}\")\n",
    "print(f\"Correlation between Communication and Greenwashing Risk: {greenwashing_df['Green_Com_Score'].corr(greenwashing_df['Greenwashing_Score']):.3f}\")\n",
    "print(f\"Correlation between Performance and Greenwashing Risk: {greenwashing_df['Performance_Score'].corr(greenwashing_df['Greenwashing_Score']):.3f}\")\n",
    "\n",
    "# Print companies with highest risk scores in each quadrant\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"HIGHEST RISK COMPANIES BY QUADRANT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for quadrant in ['Potential_Greenwashing', 'Green_Leaders', 'Laggards', 'Under_Communicators']:\n",
    "    quad_data = greenwashing_df[greenwashing_df['Quadrant'] == quadrant]\n",
    "    if len(quad_data) > 0:\n",
    "        top_risk = quad_data.nlargest(3, 'Greenwashing_Score')\n",
    "        print(f\"\\n{quadrant.replace('_', ' ')} - Top Risk:\")\n",
    "        for _, row in top_risk.iterrows():\n",
    "            print(f\"  {row['Organization']} ({row['year']}): Risk={row['Greenwashing_Score']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde79db2",
   "metadata": {},
   "source": [
    "### Additional components for greenwashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e245100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# COMPONENT 1: SUBSTANTIATION WEAKNESS SCORE\n",
    "# Quantification (30%) + Evidence (35%) + Aspirational (35%)\n",
    "# Higher scores = higher greenwashing risk\n",
    "# ==========================================\n",
    "\n",
    "greenwashing_df['Substantiation_Weakness'] = (\n",
    "    0.30 * (100 - greenwashing_df['quantification_intensity_score']) +     # Reversed: higher quantification = lower risk\n",
    "    0.35 * (100 - greenwashing_df['evidence_intensity_score']) +          # Reversed: higher evidence = lower risk  \n",
    "    0.35 * greenwashing_df['aspirational_intensity_score']                # Normal: higher aspirational = higher risk\n",
    ").round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b1d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPONENT 2: LANGUAGE VAGUENESS SCORE\n",
    "# Vague (70%) + Hedge (30%)\n",
    "# Higher scores = higher greenwashing risk\n",
    "# ==========================================\n",
    "\n",
    "greenwashing_df['Language_Vagueness'] = (\n",
    "    0.70 * greenwashing_df['vague_intensity_score'] +                     # Normal: higher vagueness = higher risk\n",
    "    0.30 * (100 - greenwashing_df['hedge_intensity_score'])               # Reversed: higher hedging = lower risk (per UCLA study)\n",
    ").round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdb9253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPONENT 3: TEMPORAL ORIENTATION SCORE  \n",
    "# Future orientation (60%) + Timeline specificity (40%)\n",
    "# Higher scores = higher greenwashing risk\n",
    "# ==========================================\n",
    "\n",
    "greenwashing_df['Temporal_Orientation'] = (\n",
    "    0.60 * greenwashing_df['future_vs_past_present_ratio'] +             # Normal: higher future focus = higher risk\n",
    "    0.40 * (100 - greenwashing_df['commitment_timeline_pct'])            # Reversed: more specific timelines = lower risk\n",
    ").round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c111883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPONENT 4: REPORTING CONSISTENCY SCORE\n",
    "# Overall similarity (70%) + High similarity ratio (30%)  \n",
    "# Higher scores = higher greenwashing risk\n",
    "# ==========================================\n",
    "\n",
    "greenwashing_df['Reporting_Consistency'] = (\n",
    "    0.70 * greenwashing_df['similarity_combined'] +                      # Normal: higher similarity = higher risk\n",
    "    0.30 * greenwashing_df['SpaCy_HighSim_Ratio']                       # Normal: more identical sentences = higher risk\n",
    ").round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a2122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARDIZE EACH COMPONENT BY YEAR\n",
    "# ==========================================\n",
    "\n",
    "normalize_by_year(greenwashing_df, 'Substantiation_Weakness')\n",
    "normalize_by_year(greenwashing_df, 'Language_Vagueness')\n",
    "normalize_by_year(greenwashing_df, 'Temporal_Orientation')\n",
    "normalize_by_year(greenwashing_df, 'Reporting_Consistency')\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# SUMMARY STATISTICS BY YEAR\n",
    "# ==========================================\n",
    "\n",
    "print(\"GREENWASHING COMPONENTS SUMMARY BY YEAR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Component 1: Substantiation Weakness\n",
    "print(f\"\\nSUBSTANTIATION WEAKNESS\")\n",
    "for year in [2021, 2022]:\n",
    "    year_data = greenwashing_df[greenwashing_df['year'] == year]\n",
    "    print(f\"\\n{year}:\")\n",
    "    print(f\"  Mean Score: {year_data['Substantiation_Weakness'].mean():.2f}\")\n",
    "    print(f\"  Highest Risk Companies:\")\n",
    "    top_subst = year_data.nlargest(5, 'Substantiation_Weakness')[['Organization', 'Substantiation_Weakness']]\n",
    "    for i, (idx, row) in enumerate(top_subst.iterrows(), 1):\n",
    "        print(f\"    {i}. {row['Organization']}: {row['Substantiation_Weakness']:.2f}\")\n",
    "\n",
    "# Component 2: Language Vagueness\n",
    "print(f\"\\nLANGUAGE VAGUENESS\")\n",
    "for year in [2021, 2022]:\n",
    "    year_data = greenwashing_df[greenwashing_df['year'] == year]\n",
    "    print(f\"\\n{year}:\")\n",
    "    print(f\"  Mean Score: {year_data['Language_Vagueness'].mean():.2f}\")\n",
    "    print(f\"  Highest Risk Companies:\")\n",
    "    top_lang = year_data.nlargest(5, 'Language_Vagueness')[['Organization', 'Language_Vagueness']]\n",
    "    for i, (idx, row) in enumerate(top_lang.iterrows(), 1):\n",
    "        print(f\"    {i}. {row['Organization']}: {row['Language_Vagueness']:.2f}\")\n",
    "\n",
    "# Component 3: Temporal Orientation  \n",
    "print(f\"\\nTEMPORAL ORIENTATION\")\n",
    "for year in [2021, 2022]:\n",
    "    year_data = greenwashing_df[greenwashing_df['year'] == year]\n",
    "    print(f\"\\n{year}:\")\n",
    "    print(f\"  Mean Score: {year_data['Temporal_Orientation'].mean():.2f}\")\n",
    "    print(f\"  Highest Risk Companies:\")\n",
    "    top_temp = year_data.nlargest(5, 'Temporal_Orientation')[['Organization', 'Temporal_Orientation']]\n",
    "    for i, (idx, row) in enumerate(top_temp.iterrows(), 1):\n",
    "        print(f\"    {i}. {row['Organization']}: {row['Temporal_Orientation']:.2f}\")\n",
    "\n",
    "# Component 4: Reporting Consistency\n",
    "print(f\"\\nREPORTING CONSISTENCY\")\n",
    "for year in [2021, 2022]:\n",
    "    year_data = greenwashing_df[greenwashing_df['year'] == year]\n",
    "    print(f\"\\n{year}:\")\n",
    "    print(f\"  Mean Score: {year_data['Reporting_Consistency'].mean():.2f}\")\n",
    "    print(f\"  Highest Risk Companies:\")\n",
    "    top_cons = year_data.nlargest(5, 'Reporting_Consistency')[['Organization', 'Reporting_Consistency']]\n",
    "    for i, (idx, row) in enumerate(top_cons.iterrows(), 1):\n",
    "        print(f\"    {i}. {row['Organization']}: {row['Reporting_Consistency']:.2f}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"Component calculations and standardization complete!\")\n",
    "print(\"Higher scores indicate higher greenwashing risk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735cf78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "greenwashing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a2839",
   "metadata": {},
   "source": [
    "### Export and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffec87ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Create Comprehensive Greenwashing Results Export\n",
    "\n",
    "print(\"CREATING COMPREHENSIVE GREENWASHING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: Create comprehensive results DataFrame with all variables\n",
    "# ==========================================\n",
    "\n",
    "print(\"Preparing comprehensive greenwashing dataset...\")\n",
    "\n",
    "# Create comprehensive dataset with all variables and clear naming\n",
    "comprehensive_greenwashing = pd.DataFrame({\n",
    "    # ============= IDENTIFICATION =============\n",
    "    'Company': greenwashing_df['Organization'],\n",
    "    'Year': greenwashing_df['year'],\n",
    "    \n",
    "    # ============= MAIN SCORES =============\n",
    "    'Performance_Communication_Gap_Score': greenwashing_df['Greenwashing_Score'],\n",
    "    'Performance_Score': greenwashing_df['Performance_Score'], \n",
    "    'Green_Communication_Score': greenwashing_df['Green_Com_Score'],\n",
    "    'Performance_Communication_Absolute_Gap': greenwashing_df['Greenwashing_Risk_Abs'],\n",
    "    # 'Performance_Communication_Gap_Amplified': greenwashing_df['Amplified_Score'],\n",
    "    \n",
    "    # ============= COMPONENT SCORES (0-100 each) =============\n",
    "    'Component_Substantiation_Weakness_Score': greenwashing_df['Substantiation_Weakness'],\n",
    "    'Component_Language_Vagueness_Score': greenwashing_df['Language_Vagueness'], \n",
    "    'Component_Temporal_Orientation_Score': greenwashing_df['Temporal_Orientation'],\n",
    "    'Component_Reporting_Consistency_Score': greenwashing_df['Reporting_Consistency'],\n",
    "    \n",
    "    # ============= GREEN TERM ANALYSIS =============\n",
    "    'Green_Terms_Frequency_Pct': df['gt_freq_pct'],\n",
    "    'Green_Terms_Unique_Relative': df['unique_gt_relative'],\n",
    "    'Green_Terms_Combined_Score': greenwashing_df['combined_green_score'],\n",
    "    \n",
    "    # ============= SENTIMENT ANALYSIS =============\n",
    "    'Overall_Sentiment_Score': df['avg_sentiment_score'],\n",
    "    'Renewable_Energy_Sentiment': df['renewable_energy_avg_sentiment'],\n",
    "    'Climate_Emissions_Sentiment': df['climate_emissions_avg_sentiment'],\n",
    "    'Combined_Sentiment_Score': greenwashing_df['combined_sentiment_score'],\n",
    "    \n",
    "    # ============= CONTEXT & SUBSTANTIATION =============\n",
    "    # Quantification, Evidence, Aspirational (used in Substantiation Weakness)\n",
    "    'Quantification_Intensity_Score': df['quantification_intensity_score'],\n",
    "    'Evidence_Intensity_Score': df['evidence_intensity_score'], \n",
    "    'Aspirational_Intensity_Score': df['aspirational_intensity_score'],\n",
    "    \n",
    "    # ============= LANGUAGE VAGUENESS METRICS =============\n",
    "    # Hedging, Vague language (used in Language Vagueness)\n",
    "    'Hedge_Intensity_Score': df['hedge_intensity_score'],\n",
    "    'Vague_Intensity_Score': df['vague_intensity_score'],\n",
    "    'Combined_Unclear_Intensity_Score': greenwashing_df['combined_intensity_score'],\n",
    "    'Commitment_Timeline_Pct': df['commitment_timeline_pct'],\n",
    "    \n",
    "    # ============= TEMPORAL ORIENTATION METRICS =============\n",
    "    # Future vs past/present focus (used in Temporal Orientation)\n",
    "    'Future_vs_Past_Present_Ratio': df['future_vs_past_present_ratio'],\n",
    "    \n",
    "    # ============= SIMILARITY/CONSISTENCY METRICS =============\n",
    "    # Document similarity (used in Reporting Consistency)\n",
    "    'TFIDF_Document_Similarity': df['TFIDF_Doc'],\n",
    "    'Jaccard_Similarity': df['Jaccard'],\n",
    "    'SpaCy_Average_Similarity': df['SpaCy_Avg_Similarity'],\n",
    "    'SpaCy_High_Similarity_Ratio': df['SpaCy_HighSim_Ratio'],\n",
    "    'Similarity_Combined_Score': greenwashing_df['similarity_combined'],\n",
    "})\n",
    "\n",
    "# Round all numeric columns\n",
    "numeric_columns = comprehensive_greenwashing.select_dtypes(include=[np.number]).columns\n",
    "comprehensive_greenwashing[numeric_columns] = comprehensive_greenwashing[numeric_columns].round(3)\n",
    "\n",
    "# Sort by Company and Year\n",
    "comprehensive_greenwashing = comprehensive_greenwashing.sort_values(['Company', 'Year']).reset_index(drop=True)\n",
    "\n",
    "print(f\"✓ Comprehensive dataset created: {comprehensive_greenwashing.shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: Create component breakdown analysis\n",
    "# ==========================================\n",
    "\n",
    "print(\"Creating component breakdown analysis...\")\n",
    "\n",
    "# Component weights used in calculations (for documentation)\n",
    "component_weights = pd.DataFrame({\n",
    "    'Component': [\n",
    "        'Substantiation Weakness - Quantification',\n",
    "        'Substantiation Weakness - Evidence', \n",
    "        'Substantiation Weakness - Aspirational',\n",
    "        'Language Vagueness - Vague Language',\n",
    "        'Language Vagueness - Hedge Language',\n",
    "        'Temporal Orientation - Future vs Past/Present',\n",
    "        'Temporal Orientation - Timeline Specificity',\n",
    "        'Reporting Consistency - Overall Similarity',\n",
    "        'Reporting Consistency - High Similarity Ratio',\n",
    "        'Green Communication - Green Terms',\n",
    "        'Green Communication - Sentiment'\n",
    "    ],\n",
    "    'Weight': [0.30, 0.35, 0.35, 0.70, 0.30, 0.60, 0.40, 0.70, 0.30, 0.40, 0.60],\n",
    "    'Direction': [\n",
    "        'Reversed (higher quantification = lower risk)',\n",
    "        'Reversed (higher evidence = lower risk)',\n",
    "        'Normal (higher aspirational = higher risk)',\n",
    "        'Normal (higher vagueness = higher risk)',\n",
    "        'Reversed (higher hedging = lower risk)',\n",
    "        'Normal (higher future focus = higher risk)',\n",
    "        'Reversed (more specific timelines = lower risk)',\n",
    "        'Normal (higher similarity = higher risk)',\n",
    "        'Normal (more identical sentences = higher risk)',\n",
    "        'Normal (higher green terms = higher communication)',\n",
    "        'Normal (higher sentiment = higher communication)'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: Create summary statistics by year\n",
    "# ==========================================\n",
    "\n",
    "print(\"Calculating summary statistics by year...\")\n",
    "\n",
    "summary_stats_2021 = comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2021].describe()\n",
    "summary_stats_2022 = comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2022].describe()\n",
    "\n",
    "# Component score means by year\n",
    "component_summary = pd.DataFrame({\n",
    "    'Component': [\n",
    "        'Greenwashing Risk Score',\n",
    "        'Green Communication Score', \n",
    "        'Substantiation Weakness',\n",
    "        'Language Vagueness',\n",
    "        'Temporal Orientation',\n",
    "        'Reporting Consistency'\n",
    "    ],\n",
    "    'Mean_2021': [\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2021]['Performance_Communication_Gap_Score'].mean(),\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2021]['Green_Communication_Score'].mean(),\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2021]['Component_Substantiation_Weakness_Score'].mean(),\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2021]['Component_Language_Vagueness_Score'].mean(),\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2021]['Component_Temporal_Orientation_Score'].mean(),\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2021]['Component_Reporting_Consistency_Score'].mean()\n",
    "    ],\n",
    "    'Mean_2022': [\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2022]['Performance_Communication_Gap_Score'].mean(),\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2022]['Green_Communication_Score'].mean(),\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2022]['Component_Substantiation_Weakness_Score'].mean(),\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2022]['Component_Language_Vagueness_Score'].mean(),\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2022]['Component_Temporal_Orientation_Score'].mean(),\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2022]['Component_Reporting_Consistency_Score'].mean()\n",
    "    ]\n",
    "}).round(2)\n",
    "\n",
    "# ==========================================\n",
    "# STEP 4: Export to Excel with multiple tabs\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\nExporting comprehensive greenwashing results to Excel...\")\n",
    "output_path = \"data/Greenwashing Results/comprehensive_greenwashing_results.xlsx\"\n",
    "\n",
    "try:\n",
    "    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "        # Main comprehensive dataset\n",
    "        comprehensive_greenwashing.to_excel(writer, sheet_name='Comprehensive_Results', index=False)\n",
    "        \n",
    "        # Separate years for easier analysis\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2021].to_excel(\n",
    "            writer, sheet_name='Results_2021', index=False)\n",
    "        comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2022].to_excel(\n",
    "            writer, sheet_name='Results_2022', index=False)\n",
    "        \n",
    "        # Component weights and methodology\n",
    "        component_weights.to_excel(writer, sheet_name='Component_Methodology', index=False)\n",
    "        \n",
    "        # Component summary statistics\n",
    "        component_summary.to_excel(writer, sheet_name='Component_Summary', index=False)\n",
    "        \n",
    "        # Detailed statistics by year\n",
    "        summary_stats_2021.to_excel(writer, sheet_name='Stats_2021')\n",
    "        summary_stats_2022.to_excel(writer, sheet_name='Stats_2022')\n",
    "        \n",
    "        # Data dictionary\n",
    "        data_dict = pd.DataFrame({\n",
    "            'Column_Name': comprehensive_greenwashing.columns,\n",
    "            'Category': [\n",
    "                'Identification' if col in ['Company', 'Year'] else\n",
    "                'Main Scores' if any(x in col for x in ['Performance_Communication_Gap_Score', 'Performance_Score', 'Green_Communication_Score', 'Absolute_Gap', 'Amplified']) else\n",
    "                'Component Scores' if 'Component_' in col else\n",
    "                'Green Terms Analysis' if any(x in col for x in ['Green_Terms', 'combined_green_score']) else\n",
    "                'Sentiment Analysis' if any(x in col for x in ['Sentiment', 'sentiment']) else\n",
    "                'Substantiation Metrics' if any(x in col for x in ['Quantification', 'Evidence', 'Aspirational']) else\n",
    "                'Language Vagueness Metrics' if any(x in col for x in ['Hedge', 'Vague', 'Unclear', 'Timeline']) else\n",
    "                'Temporal Metrics' if 'Future_vs_Past' in col else\n",
    "                'Similarity Metrics' if any(x in col for x in ['Similarity', 'TFIDF', 'Jaccard', 'SpaCy']) else\n",
    "                'Other' for col in comprehensive_greenwashing.columns\n",
    "            ],\n",
    "            'Description': [\n",
    "                'Company identifier' if col == 'Company' else\n",
    "                'Year (2021 or 2022)' if col == 'Year' else\n",
    "                '(Amplified) Performance Communication Ga[ score (0-100, higher = more risk)' if col == 'Performance_Communication_Gap_Score' else\n",
    "                'Environmental performance score from ensemble analysis' if col == 'Performance_Score' else\n",
    "                'Green communication intensity score (0-100)' if col == 'Green_Communication_Score' else\n",
    "                'Absolute gap between communication and performance' if col == 'Performance_Communication_Absolute_Gap' else\n",
    "                # 'Amplified risk score with classic greenwashing penalty' if col == 'Greenwashing_Risk_Amplified' else\n",
    "                'Component score: Weakness of substantiation (0-100, higher = more risk)' if col == 'Component_Substantiation_Weakness_Score' else\n",
    "                'Component score: Language Vagueness (0-100, higher = more risk)' if col == 'Component_Language_Vagueness_Score' else\n",
    "                'Component score: Temporal orientation (0-100, higher = more risk)' if col == 'Component_Temporal_Orientation_Score' else\n",
    "                'Component score: Reporting consistency (0-100, higher = more risk)' if col == 'Component_Reporting_Consistency_Score' else\n",
    "                'Frequency of green terms as percentage of total words' if col == 'Green_Terms_Frequency_Pct' else\n",
    "                'Unique green terms relative to document length' if col == 'Green_Terms_Unique_Relative' else\n",
    "                'Combined score from green term frequency and uniqueness' if col == 'Green_Terms_Combined_Score' else\n",
    "                'Overall sentiment score across all text' if col == 'Overall_Sentiment_Score' else\n",
    "                'Average sentiment in renewable energy discussions' if col == 'Renewable_Energy_Sentiment' else\n",
    "                'Average sentiment in climate/emissions discussions' if col == 'Climate_Emissions_Sentiment' else\n",
    "                'Weighted combination of all sentiment scores' if col == 'Combined_Sentiment_Score' else\n",
    "                'Intensity of quantitative claims and metrics' if col == 'Quantification_Intensity_Score' else\n",
    "                'Intensity of evidence-based statements' if col == 'Evidence_Intensity_Score' else\n",
    "                'Intensity of aspirational/future-oriented language' if col == 'Aspirational_Intensity_Score' else\n",
    "                'Intensity of hedging language (uncertainty markers)' if col == 'Hedge_Intensity_Score' else\n",
    "                'Intensity of vague, non-specific language' if col == 'Vague_Intensity_Score' else\n",
    "                'Combined score of unclear language patterns' if col == 'Combined_Unclear_Intensity_Score' else\n",
    "                'Percentage of commitments with specific timelines' if col == 'Commitment_Timeline_Pct' else\n",
    "                'Ratio of future-focused vs past/present language' if col == 'Future_vs_Past_Present_Ratio' else\n",
    "                'TF-IDF based document similarity score' if col == 'TFIDF_Document_Similarity' else\n",
    "                'Jaccard similarity coefficient between documents' if col == 'Jaccard_Similarity' else\n",
    "                'SpaCy semantic similarity average' if col == 'SpaCy_Average_Similarity' else\n",
    "                'Ratio of highly similar sentences (SpaCy >0.8)' if col == 'SpaCy_High_Similarity_Ratio' else\n",
    "                'Combined similarity score from multiple methods' if col == 'Similarity_Combined_Score' else\n",
    "                f'Variable: {col}' for col in comprehensive_greenwashing.columns\n",
    "            ],\n",
    "            'Scale': [\n",
    "                'Text' if col in ['Company'] else\n",
    "                'Integer (2021, 2022)' if col == 'Year' else\n",
    "                '0-100 (normalized by year)' if 'Score' in col or 'Component_' in col else\n",
    "                'Percentage (0-100)' if 'Pct' in col else\n",
    "                'Ratio (0+)' if 'Ratio' in col else\n",
    "                'Normalized (0-100)' if any(x in col for x in ['Intensity', 'Similarity', 'Combined']) else\n",
    "                'Score (-100 to +100)' if 'Sentiment' in col else\n",
    "                'Continuous' for col in comprehensive_greenwashing.columns\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        data_dict.to_excel(writer, sheet_name='Data_Dictionary', index=False)\n",
    "    \n",
    "    print(f\"✓ Comprehensive results exported successfully to: {output_path}\")\n",
    "    print(f\"  - Comprehensive_Results: Complete dataset ({comprehensive_greenwashing.shape[0]} rows, {comprehensive_greenwashing.shape[1]} columns)\")\n",
    "    print(f\"  - Results_2021: 2021 data only\")\n",
    "    print(f\"  - Results_2022: 2022 data only\") \n",
    "    print(f\"  - Component_Methodology: Weights and calculation methods\")\n",
    "    print(f\"  - Component_Summary: Component score means by year\")\n",
    "    print(f\"  - Stats_2021/2022: Detailed descriptive statistics\")\n",
    "    print(f\"  - Data_Dictionary: Complete variable descriptions\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error exporting comprehensive results: {e}\")\n",
    "    print(\"Comprehensive dataset created successfully in memory as 'comprehensive_greenwashing'\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 5: Display key summary information\n",
    "# ==========================================\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPREHENSIVE GREENWASHING DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nDATASET OVERVIEW:\")\n",
    "print(f\"  Total records: {len(comprehensive_greenwashing):,}\")\n",
    "print(f\"  Companies: {comprehensive_greenwashing['Company'].nunique()}\")\n",
    "print(f\"  Years: {sorted(comprehensive_greenwashing['Year'].unique())}\")\n",
    "print(f\"  Variables: {comprehensive_greenwashing.shape[1]}\")\n",
    "\n",
    "print(f\"\\nVARIABLE CATEGORIES:\")\n",
    "main_scores = [col for col in comprehensive_greenwashing.columns if any(x in col for x in ['Performance_Communication_Gap_Score', 'Performance_Score', 'Green_Communication_Score', 'Absolute_Gap', 'Amplified'])]\n",
    "components = [col for col in comprehensive_greenwashing.columns if 'Component_' in col]\n",
    "green_terms = [col for col in comprehensive_greenwashing.columns if 'Green_Terms' in col or 'combined_green_score' in col]\n",
    "sentiment = [col for col in comprehensive_greenwashing.columns if 'Sentiment' in col or 'sentiment' in col]\n",
    "substantiation = [col for col in comprehensive_greenwashing.columns if any(x in col for x in ['Quantification', 'Evidence', 'Aspirational'])]\n",
    "language = [col for col in comprehensive_greenwashing.columns if any(x in col for x in ['Hedge', 'Vague', 'Unclear', 'Timeline'])]\n",
    "temporal = [col for col in comprehensive_greenwashing.columns if 'Future_vs_Past' in col]\n",
    "similarity = [col for col in comprehensive_greenwashing.columns if any(x in col for x in ['Similarity', 'TFIDF', 'Jaccard', 'SpaCy'])]\n",
    "\n",
    "print(f\"  Main Scores: {len(main_scores)} variables\")\n",
    "print(f\"  Component Scores: {len(components)} variables\")\n",
    "print(f\"  Green Terms Analysis: {len(green_terms)} variables\")\n",
    "print(f\"  Sentiment Analysis: {len(sentiment)} variables\")\n",
    "print(f\"  Substantiation Metrics: {len(substantiation)} variables\")\n",
    "print(f\"  Language Metrics: {len(language)} variables\")\n",
    "print(f\"  Temporal Metrics: {len(temporal)} variables\")\n",
    "print(f\"  Similarity/Consistency: {len(similarity)} variables\")\n",
    "\n",
    "print(f\"\\nKEY STATISTICS:\")\n",
    "print(f\"  Mean Greenwashing Risk (2021): {comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2021]['Performance_Communication_Gap_Score'].mean():.2f}\")\n",
    "print(f\"  Mean Greenwashing Risk (2022): {comprehensive_greenwashing[comprehensive_greenwashing['Year'] == 2022]['Performance_Communication_Gap_Score'].mean():.2f}\")\n",
    "print(f\"  Companies with both years: {len(comprehensive_greenwashing.groupby('Company').filter(lambda x: len(x) == 2)['Company'].unique())}\")\n",
    "\n",
    "print(f\"\\nFILES CREATED:\")\n",
    "print(f\"  Comprehensive dataset: {output_path}\")\n",
    "print(f\"  (8 sheets with complete data, methodology, and documentation)\")\n",
    "\n",
    "print(f\"\\nREADY FOR ENSEMBLE ANALYSIS:\")\n",
    "print(f\"  All component scores calculated and normalized\")\n",
    "print(f\"  All underlying variables preserved\")\n",
    "print(f\"  Complete documentation provided\")\n",
    "\n",
    "print(f\"\\nVariable 'comprehensive_greenwashing' is available in memory for immediate use\")\n",
    "print(\"Proceed to ensemble analysis with confidence that all data is preserved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984de21",
   "metadata": {},
   "source": [
    "# Final Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f7a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ENSEMBLE GREENWASHING SCORE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: Generate valid weight combinations\n",
    "# ==========================================\n",
    "\n",
    "print(\"Generating valid weight combinations...\")\n",
    "print(\"Constraints:\")\n",
    "print(\"- Individual weights: 0.05 ≤ w ≤ 0.50\")\n",
    "print(\"- Greenwashing_Score > all other components\")\n",
    "print(\"- Substantiation_Weakness > Language_Vagueness, Temporal_Orientation, Reporting_Consistency\")\n",
    "print(\"- All weights sum to 1.0\")\n",
    "print()\n",
    "\n",
    "# Create weight ranges (2 decimal precision as requested)\n",
    "weight_range = np.arange(0.05, 0.51, 0.01)\n",
    "weight_range = np.round(weight_range, 2)\n",
    "\n",
    "valid_combinations = []\n",
    "total_combinations = 0\n",
    "\n",
    "# Generate all possible combinations\n",
    "for w_gw in weight_range:  # Greenwashing_Score weight\n",
    "    for w_sub in weight_range:  # Substantiation_Weakness weight\n",
    "        for w_lang in weight_range:  # Language_Vagueness weight\n",
    "            for w_temp in weight_range:  # Temporal_Orientation weight\n",
    "                total_combinations += 1\n",
    "                \n",
    "                # Calculate remaining weight for Reporting_Consistency\n",
    "                w_rep = 1.0 - (w_gw + w_sub + w_lang + w_temp)\n",
    "                w_rep = round(w_rep, 2)\n",
    "                \n",
    "                # Check if all constraints are satisfied\n",
    "                if (0.05 <= w_rep <= 0.50 and  # Reporting_Consistency in valid range\n",
    "                    w_gw > w_sub and           # Greenwashing_Score > Substantiation_Weakness\n",
    "                    w_gw > w_lang and          # Greenwashing_Score > Language_Vagueness \n",
    "                    w_gw > w_temp and          # Greenwashing_Score > Temporal_Orientation\n",
    "                    w_gw > w_rep and           # Greenwashing_Score > Reporting_Consistency\n",
    "                    w_sub > w_lang and         # Substantiation_Weakness > Language_Vagueness\n",
    "                    w_sub > w_temp and         # Substantiation_Weakness > Temporal_Orientation  \n",
    "                    w_sub > w_rep and          # Substantiation_Weakness > Reporting_Consistency\n",
    "                    abs(w_gw + w_sub + w_lang + w_temp + w_rep - 1.0) < 0.001):  # Sum ≈ 1.0\n",
    "                    \n",
    "                    valid_combinations.append({\n",
    "                        'w_greenwashing': w_gw,\n",
    "                        'w_substantiation': w_sub,\n",
    "                        'w_language': w_lang,\n",
    "                        'w_temporal': w_temp,\n",
    "                        'w_reporting': w_rep\n",
    "                    })\n",
    "\n",
    "print(f\"Total combinations tested: {total_combinations:,}\")\n",
    "print(f\"Valid combinations found: {len(valid_combinations):,}\")\n",
    "print(f\"Percentage valid: {len(valid_combinations)/total_combinations*100:.2f}%\")\n",
    "\n",
    "if len(valid_combinations) == 0:\n",
    "    print(\"ERROR: No valid weight combinations found. Check constraints.\")\n",
    "else:\n",
    "    # ==========================================\n",
    "    # STEP 2: Calculate scores for all combinations\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\nCalculating scores for {len(valid_combinations):,} weight combinations...\")\n",
    "    \n",
    "    # Initialize storage for all results\n",
    "    all_results = []\n",
    "    \n",
    "    # Progress bar for weight combinations\n",
    "    for i, weights in enumerate(tqdm(valid_combinations, desc=\"Processing combinations\")):\n",
    "        \n",
    "        # Calculate final score for this weight combination\n",
    "        final_scores = (\n",
    "            weights['w_greenwashing'] * greenwashing_df['Greenwashing_Score'] +\n",
    "            weights['w_substantiation'] * greenwashing_df['Substantiation_Weakness'] +\n",
    "            weights['w_language'] * greenwashing_df['Language_Vagueness'] +\n",
    "            weights['w_temporal'] * greenwashing_df['Temporal_Orientation'] +\n",
    "            weights['w_reporting'] * greenwashing_df['Reporting_Consistency']\n",
    "        ).round(2)\n",
    "        \n",
    "        # Store results for this combination\n",
    "        combination_result = {\n",
    "            'combination_id': i,\n",
    "            'weights': weights,\n",
    "            'scores': final_scores.tolist(),\n",
    "            'organizations': greenwashing_df['Organization'].tolist(),\n",
    "            'years': greenwashing_df['year'].tolist(),\n",
    "            'summary_stats': {\n",
    "                'mean': final_scores.mean(),\n",
    "                'median': final_scores.median(),\n",
    "                'std': final_scores.std(),\n",
    "                'min': final_scores.min(),\n",
    "                'max': final_scores.max(),\n",
    "                'q25': final_scores.quantile(0.25),\n",
    "                'q75': final_scores.quantile(0.75)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        all_results.append(combination_result)\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 3: Create ensemble summary\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"\\nCreating ensemble summary...\")\n",
    "    \n",
    "    # Extract all scores into matrix format\n",
    "    n_combinations = len(all_results)\n",
    "    n_observations = len(greenwashing_df)\n",
    "    \n",
    "    # Matrix: rows = combinations, columns = observations\n",
    "    score_matrix = np.array([result['scores'] for result in all_results])\n",
    "    \n",
    "    # Calculate statistics across all combinations for each observation\n",
    "    ensemble_stats = pd.DataFrame({\n",
    "        'Organization': greenwashing_df['Organization'],\n",
    "        'year': greenwashing_df['year'],\n",
    "        'mean_score': np.mean(score_matrix, axis=0),\n",
    "        'median_score': np.median(score_matrix, axis=0),\n",
    "        'std_score': np.std(score_matrix, axis=0),\n",
    "        'min_score': np.min(score_matrix, axis=0),\n",
    "        'max_score': np.max(score_matrix, axis=0),\n",
    "        'q25_score': np.percentile(score_matrix, 25, axis=0),\n",
    "        'q75_score': np.percentile(score_matrix, 75, axis=0),\n",
    "        'iqr_score': np.percentile(score_matrix, 75, axis=0) - np.percentile(score_matrix, 25, axis=0),\n",
    "        'range_score': np.max(score_matrix, axis=0) - np.min(score_matrix, axis=0)\n",
    "    }).round(2)\n",
    "    \n",
    "    # Weight distribution analysis\n",
    "    weight_analysis = pd.DataFrame([result['weights'] for result in all_results])\n",
    "    weight_stats = {\n",
    "        'weight_ranges': {\n",
    "            'greenwashing': [weight_analysis['w_greenwashing'].min(), weight_analysis['w_greenwashing'].max()],\n",
    "            'substantiation': [weight_analysis['w_substantiation'].min(), weight_analysis['w_substantiation'].max()], \n",
    "            'language': [weight_analysis['w_language'].min(), weight_analysis['w_language'].max()],\n",
    "            'temporal': [weight_analysis['w_temporal'].min(), weight_analysis['w_temporal'].max()],\n",
    "            'reporting': [weight_analysis['w_reporting'].min(), weight_analysis['w_reporting'].max()]\n",
    "        },\n",
    "        'weight_means': weight_analysis.mean().to_dict(),\n",
    "        'weight_stds': weight_analysis.std().to_dict()\n",
    "    }\n",
    "    \n",
    "    # Overall ensemble statistics\n",
    "    overall_stats = {\n",
    "        'total_combinations': n_combinations,\n",
    "        'total_observations': n_observations,\n",
    "        'score_stability': {\n",
    "            'mean_std_across_combinations': ensemble_stats['std_score'].mean(),\n",
    "            'max_std_across_combinations': ensemble_stats['std_score'].max(),\n",
    "            'mean_range_across_combinations': ensemble_stats['range_score'].mean(),\n",
    "            'companies_with_high_uncertainty': len(ensemble_stats[ensemble_stats['std_score'] > ensemble_stats['std_score'].quantile(0.9)])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 4: Calculate company averages across both years\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"Calculating company averages across years...\")\n",
    "    \n",
    "    # Company averages for median scores\n",
    "    company_averages_median = ensemble_stats.groupby('Organization').agg({\n",
    "        'median_score': ['mean', 'count']\n",
    "    }).round(2)\n",
    "    company_averages_median.columns = ['Avg_Median_Score', 'Years_Count']\n",
    "    company_averages_median = company_averages_median.reset_index()\n",
    "    company_averages_median = company_averages_median[company_averages_median['Years_Count'] == 2]  # Only companies with both years\n",
    "    \n",
    "    # Company averages for mean scores\n",
    "    company_averages_mean = ensemble_stats.groupby('Organization').agg({\n",
    "        'mean_score': ['mean', 'count']\n",
    "    }).round(2)\n",
    "    company_averages_mean.columns = ['Avg_Mean_Score', 'Years_Count']\n",
    "    company_averages_mean = company_averages_mean.reset_index()\n",
    "    company_averages_mean = company_averages_mean[company_averages_mean['Years_Count'] == 2]  # Only companies with both years\n",
    "    \n",
    "    # Combine company averages\n",
    "    company_averages = pd.merge(company_averages_median[['Organization', 'Avg_Median_Score']], \n",
    "                               company_averages_mean[['Organization', 'Avg_Mean_Score']], \n",
    "                               on='Organization') \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d399bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# ==========================================\n",
    "# STEP 5: Save results\n",
    "# ==========================================\n",
    "if len(valid_combinations) == 0:\n",
    "    print(\"ERROR: No valid weight combinations found. Check constraints.\")\n",
    "else:    \n",
    "    print(f\"\\nSaving results...\")\n",
    "    \n",
    "    # # Define file path\n",
    "    # output_path = \"data/Greenwashing Results/ensemble_results_summary_ensforperf.xlsx\"\n",
    "    \n",
    "    # # Save key results as Excel for easy viewing\n",
    "    # with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "    #     ensemble_stats.to_excel(writer, sheet_name='Ensemble_Scores', index=False)\n",
    "    #     company_averages.to_excel(writer, sheet_name='Company_Averages', index=False)\n",
    "        \n",
    "    #     # Summary statistics sheet\n",
    "    #     summary_df = pd.DataFrame([\n",
    "    #         ['Total Combinations', overall_stats['total_combinations']],\n",
    "    #         ['Total Observations', overall_stats['total_observations']],\n",
    "    #         ['Mean Std Across Combinations', overall_stats['score_stability']['mean_std_across_combinations']],\n",
    "    #         ['Max Std Across Combinations', overall_stats['score_stability']['max_std_across_combinations']],\n",
    "    #         ['Mean Range Across Combinations', overall_stats['score_stability']['mean_range_across_combinations']],\n",
    "    #         ['High Uncertainty Companies', overall_stats['score_stability']['companies_with_high_uncertainty']]\n",
    "    #     ], columns=['Metric', 'Value'])\n",
    "    #     summary_df.to_excel(writer, sheet_name='Summary_Stats', index=False)\n",
    "    \n",
    "    # print(\"Applying Excel formatting...\")\n",
    "    \n",
    "    # # Load the workbook for formatting\n",
    "    # wb = load_workbook(output_path)\n",
    "    \n",
    "    # # Define grey fill for alternating rows\n",
    "    # grey_fill = PatternFill(start_color=\"D9D9D9\", end_color=\"D9D9D9\", fill_type=\"solid\")\n",
    "    \n",
    "    # # Format each sheet\n",
    "    # for sheet_name in wb.sheetnames:\n",
    "    #     ws = wb[sheet_name]\n",
    "        \n",
    "    #     # Auto-adjust column widths based on the longest string in each column\n",
    "    #     for col in ws.columns:\n",
    "    #         max_length = 0\n",
    "    #         col_letter = get_column_letter(col[0].column)\n",
    "    #         for cell in col:\n",
    "    #             if cell.value:\n",
    "    #                 max_length = max(max_length, len(str(cell.value)))\n",
    "    #         ws.column_dimensions[col_letter].width = max_length + 3  # Add padding\n",
    "        \n",
    "    #     # Apply alternating row colors\n",
    "    #     if sheet_name in ['Ensemble_Scores', 'Company_Averages']:\n",
    "    #         # These sheets have company names in column A - alternate by company\n",
    "    #         prev_company = None\n",
    "    #         use_grey = False\n",
    "    #         for row in range(2, ws.max_row + 1):\n",
    "    #             current_company = ws[f\"A{row}\"].value  # Column A has the company names\n",
    "    #             if current_company != prev_company:\n",
    "    #                 use_grey = not use_grey\n",
    "    #                 prev_company = current_company\n",
    "                \n",
    "    #             if use_grey:\n",
    "    #                 for col in range(1, ws.max_column + 1):\n",
    "    #                     ws.cell(row=row, column=col).fill = grey_fill\n",
    "    #     else:\n",
    "    #         # Summary_Stats sheet - simple alternating rows\n",
    "    #         for row in range(2, ws.max_row + 1):\n",
    "    #             if row % 2 == 0:  # Even rows get grey background\n",
    "    #                 for col in range(1, ws.max_column + 1):\n",
    "    #                     ws.cell(row=row, column=col).fill = grey_fill\n",
    "    \n",
    "    # # Save the final formatted workbook\n",
    "    # wb.save(output_path)\n",
    "    \n",
    "    # print(f\"Results saved and formatted:\")\n",
    "    # print(f\"- Summary tables: data/Greenwashing Results/ensemble_results_summary_ensforperf.xlsx\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 6: Display results by year and averages\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ENSEMBLE GREENWASHING SCORE RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nOVERALL STATISTICS:\")\n",
    "    print(f\"Total weight combinations tested: {overall_stats['total_combinations']:,}\")\n",
    "    print(f\"Mean uncertainty (std) across all companies: {overall_stats['score_stability']['mean_std_across_combinations']:.2f}\")\n",
    "    print(f\"Maximum uncertainty (std) for any company: {overall_stats['score_stability']['max_std_across_combinations']:.2f}\")\n",
    "    print(f\"Companies with high score uncertainty (>90th percentile): {overall_stats['score_stability']['companies_with_high_uncertainty']}\")\n",
    "    \n",
    "    # Results by year - 2021\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(\"HIGHEST RISK COMPANIES - 2021\")\n",
    "    print(\"=\"*40)\n",
    "    print(\"(Ranked by median ensemble score)\")\n",
    "    \n",
    "    ensemble_2021 = ensemble_stats[ensemble_stats['year'] == 2021]\n",
    "    top_risk_2021 = ensemble_2021.nlargest(10, 'median_score')[['Organization', 'median_score', 'mean_score', 'std_score', 'min_score', 'max_score']]\n",
    "    print(top_risk_2021.to_string(index=False))\n",
    "    \n",
    "    # Results by year - 2022\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(\"HIGHEST RISK COMPANIES - 2022\")\n",
    "    print(\"=\"*40)\n",
    "    print(\"(Ranked by median ensemble score)\")\n",
    "    \n",
    "    ensemble_2022 = ensemble_stats[ensemble_stats['year'] == 2022]\n",
    "    top_risk_2022 = ensemble_2022.nlargest(10, 'median_score')[['Organization', 'median_score', 'mean_score', 'std_score', 'min_score', 'max_score']]\n",
    "    print(top_risk_2022.to_string(index=False))\n",
    "    \n",
    "    # Company averages across both years\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(\"HIGHEST RISK COMPANIES - AVERAGE (2021-2022)\")\n",
    "    print(\"=\"*40)\n",
    "    print(\"(Companies with data for both years)\")\n",
    "    \n",
    "    top_avg_companies = company_averages.nlargest(14, 'Avg_Median_Score')\n",
    "    print(top_avg_companies.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nCompanies with data for both years: {len(company_averages)}\")\n",
    "    \n",
    "    # Most uncertain companies by year\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(\"MOST UNCERTAIN COMPANIES BY YEAR\")\n",
    "    print(\"=\"*40)\n",
    "    print(\"(Highest std across weight combinations)\")\n",
    "    \n",
    "    print(f\"\\n2021 - Most Uncertain:\")\n",
    "    most_uncertain_2021 = ensemble_2021.nlargest(5, 'std_score')[['Organization', 'median_score', 'mean_score', 'std_score', 'min_score', 'max_score']]\n",
    "    print(most_uncertain_2021.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n2022 - Most Uncertain:\")\n",
    "    most_uncertain_2022 = ensemble_2022.nlargest(5, 'std_score')[['Organization', 'median_score', 'mean_score', 'std_score', 'min_score', 'max_score']]\n",
    "    print(most_uncertain_2022.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"ANALYSIS COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Use 'ensemble_stats' DataFrame for individual company/year scores\")\n",
    "    print(\"Use 'company_averages' DataFrame for cross-year company averages\")\n",
    "    print(\"All results saved to files for further analysis\")\n",
    "\n",
    "print(\"\\nEnsemble analysis complete!\")\n",
    "print(\"\\nKey variables created:\")\n",
    "print(\"- ensemble_stats: Individual company scores by year\")\n",
    "print(\"- company_averages: Company averages across both years\") \n",
    "print(\"- weight_analysis: All valid weight combinations (in memory)\")\n",
    "print(\"- all_results: Complete results for all combinations\")\n",
    "print(\"\\nFormatted results saved to: data/Greenwashing Results/ensemble_results_summary_ensforperf.xlsx\")\n",
    "print(\"(3 sheets: Ensemble_Scores, Company_Averages, Summary_Stats)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4478364",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity Analysis Implementation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "import os\n",
    "\n",
    "# Create directory for results\n",
    "os.makedirs('data/Greenwashing Results/sensitivity_analysis', exist_ok=True)\n",
    "\n",
    "print(\"COMMUNICATION COMPONENT SENSITIVITY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: Define Sensitivity Scenarios\n",
    "# ==========================================\n",
    "\n",
    "def create_sensitivity_scenarios():\n",
    "    \"\"\"Create all 17 sensitivity scenarios with hierarchy-preserving weight adjustments\"\"\"\n",
    "    \n",
    "    scenarios = {\n",
    "        'baseline': {\n",
    "            'name': 'Baseline',\n",
    "            'description': 'Literature-based weights',\n",
    "            'weights': {\n",
    "                'combined_sentiment': [0.6, 0.2, 0.2],  # avg, renewable, climate\n",
    "                'combined_green': [0.7, 0.3],  # freq, diversity\n",
    "                'green_communication': [0.4, 0.6],  # green, sentiment\n",
    "                'substantiation': [0.30, 0.35, 0.35],  # quantified, evidence, aspirational\n",
    "                'language_Vagueness': [0.7, 0.3],  # vague, hedge\n",
    "                'temporal': [0.6, 0.4],  # future, timeline\n",
    "                'reporting': [0.7, 0.3],  # similarity, sentences\n",
    "                'amplifier': 1.5\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Combined Sentiment variations (0.6-0.2-0.2)\n",
    "    scenarios['sentiment_plus'] = {\n",
    "        'name': 'Combined Sentiment +10%',\n",
    "        'description': 'Primary sentiment weight increased',\n",
    "        'weights': {**scenarios['baseline']['weights'],\n",
    "                   'combined_sentiment': [0.66, 0.17, 0.17]}\n",
    "    }\n",
    "    scenarios['sentiment_minus'] = {\n",
    "        'name': 'Combined Sentiment -10%',\n",
    "        'description': 'Primary sentiment weight decreased',\n",
    "        'weights': {**scenarios['baseline']['weights'],\n",
    "                   'combined_sentiment': [0.54, 0.23, 0.23]}\n",
    "    }\n",
    "    \n",
    "    # Green Term variations (0.7-0.3)\n",
    "    scenarios['green_term_plus'] = {\n",
    "        'name': 'Green Term +10%',\n",
    "        'description': 'Term frequency weight increased',\n",
    "        'weights': {**scenarios['baseline']['weights'],\n",
    "                   'combined_green': [0.77, 0.23]}\n",
    "    }\n",
    "    scenarios['green_term_minus'] = {\n",
    "        'name': 'Green Term -10%',\n",
    "        'description': 'Term frequency weight decreased',\n",
    "        'weights': {**scenarios['baseline']['weights'],\n",
    "                   'combined_green': [0.63, 0.37]}\n",
    "    }\n",
    "    \n",
    "    # Green Communication variations (0.4-0.6)\n",
    "    scenarios['green_comm_plus'] = {\n",
    "        'name': 'Green Communication +10%',\n",
    "        'description': 'Sentiment priority increased',\n",
    "        'weights': {**scenarios['baseline']['weights'],\n",
    "                   'green_communication': [0.36, 0.64]}\n",
    "    }\n",
    "    scenarios['green_comm_minus'] = {\n",
    "        'name': 'Green Communication -10%',\n",
    "        'description': 'Sentiment priority decreased',\n",
    "        'weights': {**scenarios['baseline']['weights'],\n",
    "                   'green_communication': [0.44, 0.56]}\n",
    "    }\n",
    "    \n",
    "    # Substantiation variations (limited to ±5% to preserve hierarchy)\n",
    "    scenarios['substantiation_plus'] = {\n",
    "        'name': 'Substantiation +5%',\n",
    "        'description': 'Evidence/aspirational weights increased',\n",
    "        'weights': {**scenarios['baseline']['weights'],\n",
    "                   'substantiation': [0.275, 0.3625, 0.3625]}\n",
    "    }\n",
    "    scenarios['substantiation_minus'] = {\n",
    "        'name': 'Substantiation -5%',\n",
    "        'description': 'Evidence/aspirational weights decreased',\n",
    "        'weights': {**scenarios['baseline']['weights'],\n",
    "                   'substantiation': [0.325, 0.3375, 0.3375]}\n",
    "    }\n",
    "    \n",
    "    # Language Vagueness variations (0.7-0.3)\n",
    "    scenarios['language_plus'] = {\n",
    "        'name': 'Language Vagueness +10%',\n",
    "        'description': 'Vague language weight increased',\n",
    "        'weights': {**scenarios['baseline']['weights'],\n",
    "                   'language_Vagueness': [0.77, 0.23]}\n",
    "    }\n",
    "    scenarios['language_minus'] = {\n",
    "        'name': 'Language Vagueness -10%',\n",
    "        'description': 'Vague language weight decreased',\n",
    "        'weights': {**scenarios['baseline']['weights'],\n",
    "                   'language_Vagueness': [0.63, 0.37]}\n",
    "    }\n",
    "    \n",
    "    # Temporal variations (0.6-0.4)\n",
    "    scenarios['temporal_plus'] = {\n",
    "        'name': 'Temporal +10%',\n",
    "        'description': 'Future orientation weight increased',\n",
    "        'weights': {**scenarios['baseline']['weights'],\n",
    "                   'temporal': [0.66, 0.34]}\n",
    "    }\n",
    "    scenarios['temporal_minus'] = {\n",
    "        'name': 'Temporal -10%',\n",
    "        'description': 'Future orientation weight decreased',\n",
    "        'weights': {**scenarios['baseline']['weights'],\n",
    "                   'temporal': [0.54, 0.46]}\n",
    "    }\n",
    "    \n",
    "    # Reporting variations (0.7-0.3)\n",
    "    scenarios['reporting_plus'] = {\n",
    "        'name': 'Reporting +10%',\n",
    "        'description': 'Cross-year similarity weight increased',\n",
    "        'weights': {**scenarios['baseline']['weights'],\n",
    "                   'reporting': [0.77, 0.23]}\n",
    "    }\n",
    "    scenarios['reporting_minus'] = {\n",
    "        'name': 'Reporting -10%',\n",
    "        'description': 'Cross-year similarity weight decreased',\n",
    "        'weights': {**scenarios['baseline']['weights'],\n",
    "                   'reporting': [0.63, 0.37]}\n",
    "    }\n",
    "    \n",
    "    # Amplifier variations\n",
    "    scenarios['amplifier_plus'] = {\n",
    "        'name': 'Amplifier +10%',\n",
    "        'description': 'Performance-communication gap amplifier increased',\n",
    "        'weights': {**scenarios['baseline']['weights'],\n",
    "                   'amplifier': 1.65}\n",
    "    }\n",
    "    scenarios['amplifier_minus'] = {\n",
    "        'name': 'Amplifier -10%',\n",
    "        'description': 'Performance-communication gap amplifier decreased',\n",
    "        'weights': {**scenarios['baseline']['weights'],\n",
    "                   'amplifier': 1.35}\n",
    "    }\n",
    "    \n",
    "    return scenarios\n",
    "\n",
    "scenarios = create_sensitivity_scenarios()\n",
    "print(f\"Created {len(scenarios)} sensitivity scenarios\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: Recalculate Components for Each Scenario\n",
    "# ==========================================\n",
    "\n",
    "def recalculate_components_scenario(df_orig, weights):\n",
    "    \"\"\"Recalculate all communication components with modified weights\"\"\"\n",
    "    \n",
    "    df = df_orig.copy()\n",
    "    \n",
    "    # Recalculate combined sentiment score\n",
    "    w_sent = weights['combined_sentiment']\n",
    "    df['combined_sentiment_score'] = (\n",
    "        w_sent[0] * df['avg_sentiment_score'] +     \n",
    "        w_sent[1] * df['renewable_energy_avg_sentiment'] +\n",
    "        w_sent[2] * df['climate_emissions_avg_sentiment']\n",
    "    ).round(2)\n",
    "    \n",
    "    # Recalculate combined green score\n",
    "    w_green = weights['combined_green']\n",
    "    df['combined_green_score'] = (\n",
    "        w_green[0] * df['gt_freq_pct'] +     \n",
    "        w_green[1] * df['unique_gt_relative']\n",
    "    ).round(2)\n",
    "    \n",
    "    # Recalculate Green Communication Score\n",
    "    w_comm = weights['green_communication']\n",
    "    df['Green_Com_Score'] = (\n",
    "        w_comm[0] * df['combined_green_score'] +\n",
    "        w_comm[1] * df['combined_sentiment_score']\n",
    "    ).round(2)\n",
    "    \n",
    "    # Normalize Green_Com_Score per year to 0–100 scale\n",
    "    normalize_by_year(df, 'Green_Com_Score')\n",
    "    \n",
    "    # Recalculate Substantiation Weakness\n",
    "    w_sub = weights['substantiation']\n",
    "    df['Substantiation_Weakness'] = (\n",
    "        w_sub[0] * (100 - df['quantification_intensity_score']) +\n",
    "        w_sub[1] * (100 - df['evidence_intensity_score']) +\n",
    "        w_sub[2] * df['aspirational_intensity_score']\n",
    "    ).round(2)\n",
    "    normalize_by_year(df, 'Substantiation_Weakness')\n",
    "    \n",
    "    # Recalculate Language Vagueness\n",
    "    w_lang = weights['language_Vagueness']\n",
    "    df['Language_Vagueness'] = (\n",
    "        w_lang[0] * df['vague_intensity_score'] +\n",
    "        w_lang[1] * (100 - df['hedge_intensity_score'])\n",
    "    ).round(2)\n",
    "    normalize_by_year(df, 'Language_Vagueness')\n",
    "    \n",
    "    # Recalculate Temporal Orientation\n",
    "    w_temp = weights['temporal']\n",
    "    df['Temporal_Orientation'] = (\n",
    "        w_temp[0] * df['future_vs_past_present_ratio'] +\n",
    "        w_temp[1] * (100 - df['commitment_timeline_pct'])\n",
    "    ).round(2)\n",
    "    normalize_by_year(df, 'Temporal_Orientation')\n",
    "    \n",
    "    # Recalculate Reporting Consistency\n",
    "    w_rep = weights['reporting']\n",
    "    df['Reporting_Consistency'] = (\n",
    "        w_rep[0] * df['similarity_combined'] +\n",
    "        w_rep[1] * df['SpaCy_HighSim_Ratio']\n",
    "    ).round(2)\n",
    "    normalize_by_year(df, 'Reporting_Consistency')\n",
    "    \n",
    "    # Recalculate Greenwashing Score with modified amplifier\n",
    "    amplifier = weights['amplifier']\n",
    "    \n",
    "    # Calculate absolute gap\n",
    "    df['Greenwashing_Risk_Abs'] = (\n",
    "        df['Green_Com_Score'] - df['Performance_Score']\n",
    "    ).round(2)\n",
    "    \n",
    "    # Calculate yearly medians for classic greenwashing pattern\n",
    "    yearly_medians = df.groupby('year').agg({\n",
    "        'Performance_Score': 'median',\n",
    "        'Green_Com_Score': 'median'\n",
    "    })\n",
    "    \n",
    "    # Apply amplifier for classic greenwashing\n",
    "    df['Amplified_Score'] = df.apply(\n",
    "        lambda row: row['Greenwashing_Risk_Abs'] * amplifier if \n",
    "        ((row['Green_Com_Score'] > yearly_medians.loc[row['year'], 'Green_Com_Score']) and \n",
    "         (row['Performance_Score'] < yearly_medians.loc[row['year'], 'Performance_Score'])) \n",
    "        else row['Greenwashing_Risk_Abs'],\n",
    "        axis=1\n",
    "    ).round(2)\n",
    "    \n",
    "    # Normalize to 0–100 scale within each year\n",
    "    normalize_by_year(df, 'Amplified_Score')\n",
    "    df['Greenwashing_Score'] = df['Amplified_Score'].round(2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: Run Ensemble Analysis for Each Scenario\n",
    "# ==========================================\n",
    "\n",
    "def run_ensemble_for_scenario(df_scenario, scenario_name):\n",
    "    \"\"\"Run the ensemble methodology for a specific scenario using SAME method as original\"\"\"\n",
    "    \n",
    "    # Use the same ensemble weight combinations from the original analysis\n",
    "    scenario_results = []\n",
    "    \n",
    "    # Use ALL valid combinations (same as original analysis)\n",
    "    print(f\"  Running ensemble with {len(valid_combinations):,} combinations...\")\n",
    "    \n",
    "    # Calculate ensemble scores using ALL existing valid_combinations (same as original)\n",
    "    for i, weights in enumerate(valid_combinations):\n",
    "        \n",
    "        # Calculate final score for this weight combination (EXACT same formula)\n",
    "        final_scores = (\n",
    "            weights['w_greenwashing'] * df_scenario['Greenwashing_Score'] +\n",
    "            weights['w_substantiation'] * df_scenario['Substantiation_Weakness'] +\n",
    "            weights['w_language'] * df_scenario['Language_Vagueness'] +\n",
    "            weights['w_temporal'] * df_scenario['Temporal_Orientation'] +\n",
    "            weights['w_reporting'] * df_scenario['Reporting_Consistency']\n",
    "        ).round(2)\n",
    "        \n",
    "        scenario_results.append(final_scores.tolist())\n",
    "    \n",
    "    # Calculate ensemble statistics (EXACT same as original)\n",
    "    score_matrix = np.array(scenario_results)\n",
    "    \n",
    "    scenario_ensemble = pd.DataFrame({\n",
    "        'Organization': df_scenario['Organization'],\n",
    "        'year': df_scenario['year'],\n",
    "        'scenario': scenario_name,\n",
    "        'mean_score': np.mean(score_matrix, axis=0),\n",
    "        'median_score': np.median(score_matrix, axis=0),\n",
    "        'std_score': np.std(score_matrix, axis=0),\n",
    "        'min_score': np.min(score_matrix, axis=0),\n",
    "        'max_score': np.max(score_matrix, axis=0),\n",
    "        'q25_score': np.percentile(score_matrix, 25, axis=0),\n",
    "        'q75_score': np.percentile(score_matrix, 75, axis=0),\n",
    "        'iqr_score': np.percentile(score_matrix, 75, axis=0) - np.percentile(score_matrix, 25, axis=0),\n",
    "        'range_score': np.max(score_matrix, axis=0) - np.min(score_matrix, axis=0)\n",
    "    }).round(2)\n",
    "    \n",
    "    return scenario_ensemble\n",
    "\n",
    "print(f\"\\nRunning ensemble analysis for each scenario...\")\n",
    "print(f\"Note: Using all {len(valid_combinations):,} weight combinations per scenario\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Store all scenario results\n",
    "all_scenario_results = {}\n",
    "scenario_counter = 0\n",
    "\n",
    "for scenario_key, scenario_info in scenarios.items():\n",
    "    scenario_counter += 1\n",
    "    print(f\"\\nProcessing scenario {scenario_counter}/{len(scenarios)}: {scenario_info['name']}\")\n",
    "    \n",
    "    # Recalculate components with modified weights\n",
    "    df_scenario = recalculate_components_scenario(greenwashing_df, scenario_info['weights'])\n",
    "    \n",
    "    # Run ensemble analysis (this will take time due to all combinations)  \n",
    "    scenario_ensemble = run_ensemble_for_scenario(df_scenario, scenario_info['name'])\n",
    "    \n",
    "    all_scenario_results[scenario_key] = scenario_ensemble\n",
    "    print(f\"  Completed: {scenario_info['name']}\")\n",
    "\n",
    "print(\"\\nEnsemble analysis complete for all scenarios!\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 4: Calculate Sensitivity Metrics\n",
    "# ==========================================\n",
    "\n",
    "# Combine all scenario results\n",
    "combined_results = pd.concat(all_scenario_results.values(), ignore_index=True)\n",
    "\n",
    "# Get baseline results for comparison\n",
    "baseline_results = all_scenario_results['baseline'].copy()\n",
    "baseline_rankings = baseline_results.groupby('year').apply(\n",
    "    lambda x: x.sort_values('median_score', ascending=False).reset_index(drop=True)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"\\nCalculating sensitivity metrics...\")\n",
    "\n",
    "# Calculate ranking stability (R-bar-S) for each scenario\n",
    "ranking_stability = {}\n",
    "scenario_impact = []\n",
    "\n",
    "for scenario_key, scenario_info in scenarios.items():\n",
    "    if scenario_key == 'baseline':\n",
    "        continue\n",
    "        \n",
    "    scenario_results = all_scenario_results[scenario_key]\n",
    "    \n",
    "    # Calculate R-bar-S (average ranking shift)\n",
    "    total_rank_shifts = 0\n",
    "    total_companies = 0\n",
    "    \n",
    "    for year in [2021, 2022]:\n",
    "        baseline_year = baseline_results[baseline_results['year'] == year].copy()\n",
    "        scenario_year = scenario_results[scenario_results['year'] == year].copy()\n",
    "        \n",
    "        # Rank companies by median score\n",
    "        baseline_year['baseline_rank'] = baseline_year['median_score'].rank(method='dense', ascending=False)\n",
    "        scenario_year['scenario_rank'] = scenario_year['median_score'].rank(method='dense', ascending=False)\n",
    "        \n",
    "        # Merge and calculate rank differences\n",
    "        merged = baseline_year[['Organization', 'baseline_rank']].merge(\n",
    "            scenario_year[['Organization', 'scenario_rank']], on='Organization'\n",
    "        )\n",
    "        \n",
    "        rank_shifts = abs(merged['baseline_rank'] - merged['scenario_rank'])\n",
    "        total_rank_shifts += rank_shifts.sum()\n",
    "        total_companies += len(merged)\n",
    "    \n",
    "    r_bar_s = total_rank_shifts / total_companies if total_companies > 0 else 0\n",
    "    ranking_stability[scenario_key] = r_bar_s\n",
    "    \n",
    "    # Calculate score impact metrics\n",
    "    baseline_scores = baseline_results['median_score'].values\n",
    "    scenario_scores = scenario_results['median_score'].values\n",
    "    \n",
    "    mad = np.mean(np.abs(scenario_scores - baseline_scores))\n",
    "    cv = (np.std(scenario_scores - baseline_scores) / np.mean(baseline_scores)) * 100\n",
    "    max_shift = np.max(np.abs(scenario_scores - baseline_scores))\n",
    "    companies_large_shift = np.sum(np.abs(scenario_scores - baseline_scores) > 10)\n",
    "    \n",
    "    scenario_impact.append({\n",
    "        'Scenario': scenario_info['name'],\n",
    "        'R_bar_S': r_bar_s,\n",
    "        'Avg_MAD': mad,\n",
    "        'Avg_CV': cv,\n",
    "        'Max_Score_Shift': max_shift,\n",
    "        'Companies_Large_Shift': companies_large_shift,\n",
    "        'Impact_Level': 'High' if r_bar_s > 4.0 else 'Moderate' if r_bar_s > 2.0 else 'Low'\n",
    "    })\n",
    "\n",
    "scenario_impact_df = pd.DataFrame(scenario_impact).sort_values('R_bar_S', ascending=False)\n",
    "\n",
    "# ==========================================\n",
    "# STEP 5: Company-Level Sensitivity Analysis  \n",
    "# ==========================================\n",
    "\n",
    "print(\"Analyzing company-level sensitivity...\")\n",
    "\n",
    "# Calculate sensitivity metrics for each company\n",
    "company_sensitivity = []\n",
    "\n",
    "for org in baseline_results['Organization'].unique():\n",
    "    org_baseline = baseline_results[baseline_results['Organization'] == org]['median_score'].values\n",
    "    \n",
    "    org_scores = []\n",
    "    for scenario_key in scenarios.keys():\n",
    "        if scenario_key == 'baseline':\n",
    "            continue\n",
    "        scenario_scores = all_scenario_results[scenario_key]\n",
    "        org_scenario = scenario_scores[scenario_scores['Organization'] == org]['median_score'].values  \n",
    "        org_scores.extend(org_scenario)\n",
    "    \n",
    "    if len(org_scores) > 0 and len(org_baseline) > 0:\n",
    "        # Calculate sensitivity metrics\n",
    "        baseline_mean = np.mean(org_baseline)\n",
    "        all_scores = np.array(org_scores + org_baseline.tolist())\n",
    "        \n",
    "        cv = (np.std(all_scores) / np.mean(all_scores)) * 100 if np.mean(all_scores) != 0 else 0\n",
    "        score_range = np.max(all_scores) - np.min(all_scores)\n",
    "        mad = np.mean(np.abs(org_scores - np.mean(org_baseline)))\n",
    "        \n",
    "        # Calculate ranking sensitivity\n",
    "        rank_shifts = []\n",
    "        for scenario_key in scenarios.keys():\n",
    "            if scenario_key == 'baseline':\n",
    "                continue\n",
    "            scenario_results = all_scenario_results[scenario_key]\n",
    "            for year in [2021, 2022]:\n",
    "                if org in baseline_results[baseline_results['year'] == year]['Organization'].values:\n",
    "                    baseline_rank = baseline_results[\n",
    "                        (baseline_results['year'] == year)\n",
    "                    ]['median_score'].rank(method='dense', ascending=False)[\n",
    "                        baseline_results[\n",
    "                            (baseline_results['year'] == year) & \n",
    "                            (baseline_results['Organization'] == org)\n",
    "                        ].index[0]\n",
    "                    ]\n",
    "                    \n",
    "                    scenario_rank = scenario_results[\n",
    "                        (scenario_results['year'] == year)\n",
    "                    ]['median_score'].rank(method='dense', ascending=False)[\n",
    "                        scenario_results[\n",
    "                            (scenario_results['year'] == year) & \n",
    "                            (scenario_results['Organization'] == org)\n",
    "                        ].index[0]\n",
    "                    ]\n",
    "                    \n",
    "                    rank_shifts.append(abs(baseline_rank - scenario_rank))\n",
    "        \n",
    "        avg_rank_shift = np.mean(rank_shifts) if rank_shifts else 0\n",
    "        max_rank_shift = np.max(rank_shifts) if rank_shifts else 0\n",
    "        \n",
    "        company_sensitivity.append({\n",
    "            'Organization': org,\n",
    "            'Baseline_Score_Mean': baseline_mean,\n",
    "            'CV_Percent': cv,\n",
    "            'Score_Range': score_range,\n",
    "            'MAD': mad,\n",
    "            'Avg_Rank_Shift': avg_rank_shift,\n",
    "            'Max_Rank_Shift': max_rank_shift,\n",
    "            'Sensitivity_Level': 'High' if cv > 15 else 'Moderate' if cv > 5 else 'Low'\n",
    "        })\n",
    "\n",
    "company_sensitivity_df = pd.DataFrame(company_sensitivity).sort_values('CV_Percent', ascending=False)\n",
    "\n",
    "print(\"Sensitivity analysis complete!\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 6: Save Results\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\nSaving sensitivity analysis results...\")\n",
    "\n",
    "# Save comprehensive results\n",
    "output_file = 'data/Greenwashing Results/sensitivity_analysis/communication_sensitivity_results.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    # Scenario impact summary\n",
    "    scenario_impact_df.to_excel(writer, sheet_name='Scenario_Impact', index=False)\n",
    "    \n",
    "    # Company sensitivity analysis\n",
    "    company_sensitivity_df.to_excel(writer, sheet_name='Company_Sensitivity', index=False)\n",
    "    \n",
    "    # All scenario results (sample)\n",
    "    combined_results.to_excel(writer, sheet_name='All_Scenarios', index=False)\n",
    "    \n",
    "    # Baseline results for reference\n",
    "    baseline_results.to_excel(writer, sheet_name='Baseline_Results', index=False)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 7: Display Key Results\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SENSITIVITY ANALYSIS RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nSCENARIO IMPACT RANKING (by R-bar-S):\")\n",
    "print(\"=\"*40)\n",
    "display_cols = ['Scenario', 'R_bar_S', 'Avg_MAD', 'Max_Score_Shift', 'Impact_Level']\n",
    "print(scenario_impact_df[display_cols].head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\nFRAMEWORK ROBUSTNESS ASSESSMENT:\")\n",
    "print(\"=\"*40)\n",
    "avg_r_bar_s = scenario_impact_df['R_bar_S'].mean()\n",
    "robust_scenarios = len(scenario_impact_df[scenario_impact_df['R_bar_S'] < 2.0])\n",
    "print(f\"Average R-bar-S across all scenarios: {avg_r_bar_s:.2f}\")\n",
    "print(f\"Scenarios with high robustness (R-bar-S < 2.0): {robust_scenarios}/{len(scenario_impact_df)}\")\n",
    "print(f\"Overall framework robustness: {'High' if avg_r_bar_s < 2.0 else 'Moderate' if avg_r_bar_s < 4.0 else 'Low'}\")\n",
    "\n",
    "print(f\"\\nMOST SENSITIVE COMPANIES:\")\n",
    "print(\"=\"*40)\n",
    "sens_cols = ['Organization', 'CV_Percent', 'Score_Range', 'Max_Rank_Shift', 'Sensitivity_Level']\n",
    "print(company_sensitivity_df[sens_cols].head(8).to_string(index=False))\n",
    "\n",
    "print(f\"\\nCOMPANY SENSITIVITY DISTRIBUTION:\")\n",
    "print(\"=\"*40)\n",
    "sensitivity_dist = company_sensitivity_df['Sensitivity_Level'].value_counts()\n",
    "for level, count in sensitivity_dist.items():\n",
    "    pct = (count / len(company_sensitivity_df)) * 100\n",
    "    print(f\"{level} sensitivity: {count} companies ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key findings saved to Excel with multiple sheets\")\n",
    "print(\"Use scenario impact ranking to identify most influential methodological choices\")\n",
    "print(\"Use company sensitivity analysis to assess reliability of individual assessments\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
