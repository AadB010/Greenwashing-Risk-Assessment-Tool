{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b79baae",
   "metadata": {},
   "source": [
    "# NLP Text Loading and Cleaning Pipeline\n",
    "\n",
    "## Overview\n",
    "This module handles the complete text extraction and cleaning pipeline for sustainability reports, transforming PDF documents into cleaned text files suitable for NLP analysis. The cleaning process was developed after manually reviewing extracted text files to identify document-level issues requiring correction.\n",
    "\n",
    "## Text Extraction Process\n",
    "**PDF to Text**: Uses spaCyLayout with spaCy to extract text from PDF sustainability reports while preserving reading order\n",
    "- **Challenge addressed**: Multi-column layouts in sustainability reports cause standard extraction tools to scramble sentences by reading left-to-right across columns\n",
    "- **Processing time**: ~30 minutes per report due to layout-aware parsing complexity\n",
    "- **Caching mechanism**: Saves extracted text as .txt files to avoid re-processing\n",
    "\n",
    "## Multi-Stage Cleaning Pipeline\n",
    "\n",
    "### Stage 1: Initial Extraction (`/Clean/` folder)\n",
    "Raw text extraction from PDFs using spaCyLayout, cached to prevent re-processing\n",
    "\n",
    "### Stage 2: Manual Corrections (`/Cleaner/` folder)  \n",
    "**Company-specific character encoding fixes** identified through manual review of extracted files:\n",
    "- **EDP**: Extensive character mapping (40+ replacements) for garbled characters like \"Ä¸\" → \"g\", \"Æ™\" → \"r\", number encoding fixes\n",
    "- **Terna Energy**: Specific character issues like \"/idotaccent\" → \"i\"\n",
    "- **Other companies**: Targeted fixes for PDF extraction artifacts and encoding errors\n",
    "\n",
    "### Stage 3: Automated Final Cleaning (`/Cleanest/` folder)\n",
    "Systematic text standardization across all documents:\n",
    "- **Space normalization**: Multiple spaces reduced to single spaces\n",
    "- **Chemical term standardization**: \"CO 2\" → \"CO2\" (case-insensitive) to prevent token splitting in spaCy\n",
    "- **Bracket spacing**: Remove spaces in \"( text )\" → \"(text)\" for consistent tokenization\n",
    "\n",
    "## Processing Configuration\n",
    "- **Test mode**: 2 companies for development/testing\n",
    "- **Actual mode**: 14 sample companies for full analysis\n",
    "- **Years processed**: 2021 and 2022 sustainability reports\n",
    "- **Text handling**: Increased spaCy max_length to 1.5M characters for large reports\n",
    "\n",
    "## Manual Review Integration\n",
    "The cleaning steps were specifically designed after manually checking extracted .txt files to identify:\n",
    "- Encoding issues from PDF extraction artifacts\n",
    "- Layout-specific problems requiring targeted corrections\n",
    "- Document-level cleaning requirements for reliable NLP processing\n",
    "\n",
    "## Final Output\n",
    "Clean, standardized text files in `/Cleanest/` folders ready for communication analysis modules, with all major extraction artifacts and encoding issues resolved through the systematic multi-stage approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eac54c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy_layout import spaCyLayout\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load models\n",
    "nlp_blank = spacy.blank(\"en\")\n",
    "layout = spaCyLayout(nlp_blank)\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Increase max_length to safely handle long texts\n",
    "nlp.max_length = 1_500_000\n",
    "\n",
    "print(\"Models loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083084b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle between \"test\" and \"actual\"\n",
    "MODE = \"actual\"  \n",
    "\n",
    "# Define configuration based on mode\n",
    "if MODE == \"test\":\n",
    "    report_names = [ \n",
    "        \"Axpo_Holding_AG\", \"NEOEN_SA\"\n",
    "    ]\n",
    "    folders = {\n",
    "        \"2021\": Path(\"data/NLP/Testing/Reports/2021\"),\n",
    "        \"2022\": Path(\"data/NLP/Testing/Reports/2022\")\n",
    "    }\n",
    "\n",
    "    # Output paths for clean text files\n",
    "    clean_text_folders = {\n",
    "        \"2021\": Path(\"data/NLP/Testing/Reports/Clean/2021\"),\n",
    "        \"2022\": Path(\"data/NLP/Testing/Reports/Clean/2022\")\n",
    "    }\n",
    "\n",
    "elif MODE == \"actual\":\n",
    "    report_names = [ \n",
    "        \"Akenerji_Elektrik_Uretim_AS\",\n",
    "        \"Arendals_Fossekompani_ASA\",\n",
    "        \"Atlantica_Sustainable_Infrastructure_PLC\",\n",
    "        \"CEZ\",\n",
    "        \"EDF\",\n",
    "        \"EDP_Energias_de_Portugal_SA\",\n",
    "        \"Endesa\",\n",
    "        \"ERG_SpA\",\n",
    "        \"Orsted\",\n",
    "        \"Polska_Grupa_Energetyczna_PGE_SA\",\n",
    "        \"Romande_Energie_Holding_SA\",\n",
    "        \"Scatec_ASA\",\n",
    "        \"Solaria_Energia_y_Medio_Ambiente_SA\",\n",
    "        \"Terna_Energy_SA\"\n",
    "    ]\n",
    "    folders = {\n",
    "        \"2021\": Path(\"data/NLP/Reports/2021\"),\n",
    "        \"2022\": Path(\"data/NLP/Reports/2022\")\n",
    "    }\n",
    "\n",
    "    # Output paths for clean text files\n",
    "    clean_text_folders = {\n",
    "        \"2021\": Path(\"data/NLP/Reports/Clean/2021\"),\n",
    "        \"2022\": Path(\"data/NLP/Reports/Clean/2022\")\n",
    "    }\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid MODE. Use 'test' or 'actual'.\")\n",
    "\n",
    "# Check availability\n",
    "for name in report_names:\n",
    "    file_name = f\"{name.replace('_', ' ')}.pdf\"\n",
    "    in_2021 = (folders[\"2021\"] / file_name).exists()\n",
    "    in_2022 = (folders[\"2022\"] / file_name).exists()\n",
    "    print(f\"{file_name}: 2021: {'✔️' if in_2021 else '❌'} | 2022: {'✔️' if in_2022 else '❌'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d837d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Processing {len(report_names)} companies for years: {list(folders.keys())}\")\n",
    "\n",
    "documents = {}\n",
    "\n",
    "for version, folder_path in folders.items():\n",
    "    for name in report_names:\n",
    "        file_name = name.replace(\"_\", \" \") + \".pdf\"\n",
    "        pdf_path = folder_path / file_name\n",
    "        clean_text_path = clean_text_folders[version] / f\"{name}.txt\"\n",
    "\n",
    "        try:\n",
    "            # Only parse layout if text file doesn't exist\n",
    "            if clean_text_path.exists():\n",
    "                with open(clean_text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    clean_text = f.read()\n",
    "                print(f\"Loaded cached text: {clean_text_path.name}\")\n",
    "            else:\n",
    "                layout_doc = layout(str(pdf_path))\n",
    "                clean_text = layout_doc.text\n",
    "                with open(clean_text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(clean_text)\n",
    "                print(f\"Saved clean text: {clean_text_path.name}\")\n",
    "\n",
    "            # Run linguistic analysis\n",
    "            nlp_doc = nlp(clean_text)\n",
    "            doc_key = f\"{name}_{version}\"\n",
    "            documents[doc_key] = nlp_doc\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe44c200",
   "metadata": {},
   "source": [
    "## Clean specific reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86287897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Input- and outputpath\n",
    "input_path = Path(\"data/NLP/Reports/Clean/2021/EDP_Energias_de_Portugal_SA.txt\")\n",
    "output_path = Path(\"data/NLP/Reports/Cleaner/2021/EDP_Energias_de_Portugal_SA_corrected.txt\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Table with replacements\n",
    "replacements = {\n",
    "    \"ĸ\": \"g\",\n",
    "    \"ƙ\": \"r\",\n",
    "    \"ķ\": \"f\",\n",
    "    \"ǎ\": \"w\",\n",
    "    \"Ǎ\": \"v\",\n",
    "    \"ǔ\": \"y\",\n",
    "    \"Ǧ\": \"fi\",\n",
    "    \"Ǔ\": \"x\",\n",
    "    \"s̫\": \"s\",\n",
    "    \"E̺\": \"E\",\n",
    "    \"I\": \"I\",\n",
    "    \"i̺\": \"i)\",\n",
    "    \"O̺\": \"O)\",\n",
    "    \"o̺\": \"o)\",\n",
    "    \"U̺\": \"U)\",\n",
    "    \"s̺\": \"s)\",\n",
    "    \"Ƙ\": \"q\",\n",
    "    \"ǧ\": \"fl\",\n",
    "    \"a̬\": \"a;\",\n",
    "    \"n̬\": \"n;\",\n",
    "    \"e̬\": \"e;\",\n",
    "    \"t̺\": \"t)\",\n",
    "    \"˩\": \"0\",\n",
    "    \"˪\": \"1\",\n",
    "    \"˫\": \"2\",\n",
    "    \"˩\": \"0\",\n",
    "    \"˪\": \"1\",\n",
    "    \"ˬ\": \"3\",\n",
    "    \"˭\": \"4\",\n",
    "    \"ˮ\": \"5\",\n",
    "    \"˯\": \"6\",\n",
    "    \"˰\": \"7\",\n",
    "    \"˱\": \"8\",\n",
    "    \"˲\": \"9\"\n",
    "}\n",
    "\n",
    "# Read original text\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Replace incorrect characters with correct ones\n",
    "for wrong, correct in replacements.items():\n",
    "    text = text.replace(wrong, correct)\n",
    "\n",
    "# Write the corrected text to the output file\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d4385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Input- and outputpath\n",
    "input_path = Path(\"data/NLP/Reports/Clean/2021/Terna_Energy_SA.txt\")\n",
    "output_path = Path(\"data/NLP/Reports/Cleaner/2021/Terna_Energy_SA_corrected.txt\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Table with replacements\n",
    "replacements = {\n",
    "    \"/idotaccent\": \"i\",\n",
    "}\n",
    "\n",
    "# Read original text\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Replace incorrect characters with correct ones\n",
    "for wrong, correct in replacements.items():\n",
    "    text = text.replace(wrong, correct)\n",
    "\n",
    "# Write the corrected text to the output file\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc0045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: Akenerji_Elektrik_Uretim_AS.txt\n",
      "Processed: Arendals_Fossekompani_ASA.txt\n",
      "Processed: Atlantica_Sustainable_Infrastructure_PLC.txt\n",
      "Processed: CEZ.txt\n",
      "Processed: EDF.txt\n",
      "Processed: EDP_Energias_de_Portugal_SA.txt\n",
      "Processed: Endesa.txt\n",
      "Processed: ERG_SpA.txt\n",
      "Processed: Orsted.txt\n",
      "Processed: Polska_Grupa_Energetyczna_PGE_SA.txt\n",
      "Processed: Romande_Energie_Holding_SA.txt\n",
      "Processed: Scatec_ASA.txt\n",
      "Processed: Solaria_Energia_y_Medio_Ambiente_SA.txt\n",
      "Processed: Terna_Energy_SA.txt\n",
      "Processed: Akenerji_Elektrik_Uretim_AS.txt\n",
      "Processed: Arendals_Fossekompani_ASA.txt\n",
      "Processed: Atlantica_Sustainable_Infrastructure_PLC.txt\n",
      "Processed: CEZ.txt\n",
      "Processed: EDF.txt\n",
      "Processed: EDP_Energias_de_Portugal_SA.txt\n",
      "Processed: Endesa.txt\n",
      "Processed: ERG_SpA.txt\n",
      "Processed: Orsted.txt\n",
      "Processed: Polska_Grupa_Energetyczna_PGE_SA.txt\n",
      "Processed: Romande_Energie_Holding_SA.txt\n",
      "Processed: Scatec_ASA.txt\n",
      "Processed: Solaria_Energia_y_Medio_Ambiente_SA.txt\n",
      "Processed: Terna_Energy_SA.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "input_folders = {\n",
    "    \"2021\": Path(\"data/NLP/Reports/Cleaner/2021\"),\n",
    "    \"2022\": Path(\"data/NLP/Reports/Cleaner/2022\")\n",
    "}\n",
    "output_folders = {\n",
    "    \"2021\": Path(\"data/NLP/Reports/Cleanest/2021\"),\n",
    "    \"2022\": Path(\"data/NLP/Reports/Cleanest/2022\")\n",
    "}\n",
    "\n",
    "for folder in output_folders.values():\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for year, input_folder in input_folders.items():\n",
    "    if input_folder.exists():\n",
    "        for file_path in input_folder.glob(\"*.txt\"):\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "\n",
    "            # Replace multiple spaces with a single space\n",
    "            cleaned_text = re.sub(r' +', ' ', text)\n",
    "            # Replace 'co 2' with 'co2' (case insensitive)\n",
    "            cleaned_text = re.sub(r'\\bco 2\\b', 'co2', cleaned_text, flags=re.IGNORECASE)\n",
    "            # Remove space after opening bracket and before closing bracket\n",
    "            cleaned_text = cleaned_text.replace('( ', '(').replace(' )', ')')\n",
    "\n",
    "\n",
    "            output_path = output_folders[year] / file_path.name\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(cleaned_text)\n",
    "\n",
    "            print(f\"Processed: {file_path.name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
