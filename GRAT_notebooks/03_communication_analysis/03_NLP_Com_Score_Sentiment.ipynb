{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91374082",
   "metadata": {},
   "source": [
    "# Environmental Sentiment Analysis using Climate-BERT\n",
    "\n",
    "## Overview\n",
    "This module analyzes environmental sentiment using Climate-BERT, a specialized model trained on climate texts. It measures how positively or negatively companies frame environmental topics, supporting both Green Communication Intensity and Substantiation Weakness dimensions through sentiment-based analysis.\n",
    "\n",
    "## Climate-BERT Integration\n",
    "- **Specialized model**: Uses \"climatebert/distilroberta-base-climate-sentiment\" for climate-specific sentiment classification\n",
    "- **Three-class output**: OPPORTUNITY (positive climate sentiment), RISK (negative climate sentiment), NEUTRAL (neutral content)  \n",
    "- **Sentiment scoring**: Final score = Opportunity Score - Risk Score, ranging from -1 to +1\n",
    "- **Sentence-level analysis**: Processes individual sentences containing green terms for granular sentiment assessment\n",
    "\n",
    "## Multi-Level Analysis\n",
    "1. **Document-level sentiment**: Average sentiment across all sustainability sentences per report\n",
    "2. **Topic-weighted sentiment**: Separate analysis for renewable energy and climate emissions categories\n",
    "3. **Weighted contribution**: When sentences contain multiple topics, sentiment contributions weighted by term frequency within sentence\n",
    "\n",
    "## Topic-Specific Focus\n",
    "- **Renewable energy terms**: \"solar power\", \"wind turbines\", \"clean energy generation\"\n",
    "- **Climate emissions terms**: \"carbon neutral\", \"emission reduction\", \"decarbonization\"  \n",
    "- **Justification**: These topics carry distinctly positive sentiment that shapes public environmental perception\n",
    "\n",
    "## Variables Produced for Communication Scoring\n",
    "According to the analysis framework:\n",
    "- **Average Environmental Sentiment** → Green Communication Intensity dimension\n",
    "- **Renewable Energy Sentiment** → Green Communication Intensity dimension  \n",
    "- **Climate and Emissions Sentiment** → Green Communication Intensity dimension\n",
    "- **Sentiment analysis contributes to** → Substantiation Weakness dimension (through positive framing assessment)\n",
    "\n",
    "## Processing Pipeline\n",
    "Extracts sentences with green terms → Climate-BERT sentiment classification → Topic-based weighting → Aggregated sentiment scores for communication gap analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395af05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy_layout import spaCyLayout\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Load spaCy model and configure for large documents\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.max_length = 1_500_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2128fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Toggle between \"test\" and \"actual\"\n",
    "MODE = \"actual\"  \n",
    "\n",
    "# Define configuration based on mode\n",
    "if MODE == \"test\":\n",
    "    report_names = [ \n",
    "        \"Axpo_Holding_AG\", \"NEOEN_SA\"\n",
    "    ]\n",
    "    folders = {\n",
    "        \"2021\": Path(\"data/NLP/Testing/Reports/Clean/2021\"),\n",
    "        \"2022\": Path(\"data/NLP/Testing/Reports/Clean/2022\")\n",
    "    }\n",
    "\n",
    "elif MODE == \"actual\":\n",
    "    report_names = [ \n",
    "        \"Akenerji_Elektrik_Uretim_AS\",\n",
    "        \"Arendals_Fossekompani_ASA\",\n",
    "        \"Atlantica_Sustainable_Infrastructure_PLC\",\n",
    "        \"CEZ\",\n",
    "        \"EDF\",\n",
    "        \"EDP_Energias_de_Portugal_SA\",\n",
    "        \"Endesa\",\n",
    "        \"ERG_SpA\",\n",
    "        \"Orsted\",\n",
    "        \"Polska_Grupa_Energetyczna_PGE_SA\",\n",
    "        \"Romande_Energie_Holding_SA\",\n",
    "        \"Scatec_ASA\",\n",
    "        \"Solaria_Energia_y_Medio_Ambiente_SA\",\n",
    "        \"Terna_Energy_SA\"\n",
    "    ]\n",
    "\n",
    "    folders = {\n",
    "        \"2021\": Path(\"data/NLP/Reports/Cleanest/2021\"),\n",
    "        \"2022\": Path(\"data/NLP/Reports/Cleanest/2022\")\n",
    "    }\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid MODE. Use 'test' or 'actual'.\")\n",
    "\n",
    "# Check availability\n",
    "for name in report_names:\n",
    "    file_name = f\"{name}.txt\"\n",
    "    in_2021 = (folders[\"2021\"] / file_name).exists()\n",
    "    in_2022 = (folders[\"2022\"] / file_name).exists()\n",
    "    print(f\"{file_name}: 2021: {'YES' if in_2021 else 'NO'} | 2022: {'YES' if in_2022 else 'NO'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a05eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store processed docs\n",
    "documents = {}\n",
    "\n",
    "# Load and process all documents\n",
    "for version, folder_path in folders.items():\n",
    "    for name in report_names:\n",
    "        txt_path = folder_path / f\"{name}.txt\"\n",
    "        try:\n",
    "            with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            doc_key = f\"{name}_{version}\"\n",
    "            documents[doc_key] = nlp(text)\n",
    "            print(f\"Processed {doc_key}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {txt_path.name}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c41a0",
   "metadata": {},
   "source": [
    "## Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fa9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GREEN TOPIC CATEGORIZATION WORD LISTS\n",
    "\n",
    "# =============================================================================\n",
    "# 1. RENEWABLE ENERGY SOURCES\n",
    "# =============================================================================\n",
    "\n",
    "# Single-word renewable energy terms (lemmas)\n",
    "renewable_energy_nouns = [\n",
    "    \"biogas\", \"biofuel\", \"biomass\", \"geothermal\", \"hydroelectric\", \"hydropower\", \n",
    "    \"photovoltaic\", \"pv\", \"renewables\", \"solar\", \"turbine\", \"wind\"\n",
    "]\n",
    "\n",
    "renewable_energy_adverbs = [\n",
    "    \"renewably\"\n",
    "]\n",
    "\n",
    "# Multi-word renewable energy terms (lemmas) - base_word: [modifier_words]\n",
    "renewable_energy_multiword = {\n",
    "    \"energy\": [\"alternative\", \"bio\", \"biomass\", \"clean\", \"geothermal\", \"green\", \"hydro\", \"renewable\", \"solar\", \"tidal\", \"wind\"],\n",
    "    \"fuel\": [\"alternative\", \"bio\", \"biomass\", \"clean\", \"renewable\", \"synthetic\"],\n",
    "    \"fuels\": [\"alternative\", \"clean\", \"renewable\", \"synthetic\"],\n",
    "    \"gas\": [\"bio\", \"biomass\", \"renewable\"],\n",
    "    \"mass\": [\"bio\"],\n",
    "    \"fired\": [\"biomass\"],\n",
    "    \"fueled\": [\"biomass\"],\n",
    "    \"powered\": [\"biomass\"],\n",
    "    \"hydrogen\": [\"blue\", \"clean\", \"green\", \"renewable\"],\n",
    "    \"power\": [\"hydro\", \"renewable\", \"solar\", \"wind\"],\n",
    "    \"farm\": [\"offshore\", \"solar\", \"wind\"],\n",
    "    \"farms\": [\"offshore\", \"solar\", \"wind\"],\n",
    "    \"station\": [\"offshore\", \"solar\", \"wind\"],\n",
    "    \"stations\": [\"offshore\", \"solar\", \"wind\"],\n",
    "    \"turbine\": [\"offshore\", \"onshore\", \"wind\"],\n",
    "    \"turbines\": [\"offshore\", \"onshore\", \"wind\"],\n",
    "    \"wind\": [\"offshore\"],\n",
    "    \"panel\": [\"photovoltaic\", \"pv\", \"solar\"],\n",
    "    \"panels\": [\"photovoltaic\", \"pv\", \"solar\"],\n",
    "    \"generation\": [\"renewable\"],\n",
    "    \"source\": [\"renewable\"],\n",
    "    \"sources\": [\"renewable\"],\n",
    "    \"plant\": [\"solar\", \"wind\"],\n",
    "    \"plants\": [\"solar\", \"wind\"]\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. CLIMATE & EMISSIONS MANAGEMENT\n",
    "# =============================================================================\n",
    "\n",
    "# Single-word climate & emissions terms (lemmas)\n",
    "climate_emissions_nouns = [\n",
    "    \"climate\", \"co2\", \"co2e\", \"decarbonisation\", \"decarbonization\", \"emission\", \"emissions\", \n",
    "    \"footprint\", \"ghg\", \"greenhouse\", \"methane\", \"mitigation\", \"pollution\", \"scope1\", \n",
    "    \"scope2\", \"scope3\", \"sequestration\"\n",
    "]\n",
    "\n",
    "climate_emissions_verbs = [\n",
    "    \"decarbonise\", \"decarbonised\", \"decarbonising\", \"decarbonize\", \"decarbonized\", \"decarbonizing\"\n",
    "]\n",
    "\n",
    "# Multi-word climate & emissions terms (lemmas) - base_word: [modifier_words]\n",
    "climate_emissions_multiword = {\n",
    "    \"emission\": [\"annual\", \"baseline\", \"carbon\", \"co2\", \"co2e\", \"direct\", \"ghg\", \"indirect\", \"scope\", \"total\"],\n",
    "    \"emissions\": [\"annual\", \"baseline\", \"carbon\", \"co2\", \"co2e\", \"direct\", \"ghg\", \"indirect\", \"scope\", \"total\"],\n",
    "    \"abatement\": [\"carbon\", \"co2\", \"co2e\", \"emission\", \"ghg\", \"pollution\"],\n",
    "    \"capture\": [\"carbon\", \"co2\", \"ghg\", \"methane\"],\n",
    "    \"captured\": [\"carbon\"],\n",
    "    \"capturing\": [\"carbon\"],\n",
    "    \"economy\": [\"carbon\"],\n",
    "    \"footprint\": [\"carbon\", \"co2\", \"ecological\", \"environmental\", \"ghg\", \"zero\"],\n",
    "    \"free\": [\"carbon\", \"co2\", \"emission\", \"emissions\", \"pollution\"],\n",
    "    \"goal\": [\"carbon\", \"climate\", \"emission\"],\n",
    "    \"goals\": [\"carbon\", \"climate\", \"emission\"],\n",
    "    \"impact\": [\"carbon\", \"climate\", \"ecological\", \"environmental\"],\n",
    "    \"intensity\": [\"carbon\", \"co2\", \"emission\", \"fuel\", \"ghg\"],\n",
    "    \"low\": [\"carbon\", \"emission\", \"emissions\"],\n",
    "    \"lower\": [\"carbon\"],\n",
    "    \"management\": [\"carbon\"],\n",
    "    \"negative\": [\"carbon\", \"co2\"],\n",
    "    \"neutral\": [\"carbon\", \"climate\", \"co2\", \"emission\"],\n",
    "    \"neutrality\": [\"carbon\", \"climate\", \"co2\", \"emission\"],\n",
    "    \"non\": [\"carbon\", \"emitting\"],\n",
    "    \"sequestered\": [\"carbon\"],\n",
    "    \"storage\": [\"carbon\", \"co2\"],\n",
    "    \"zero\": [\"carbon\", \"co2\", \"emission\", \"emissions\", \"footprint\", \"net\", \"pollution\", \"waste\"],\n",
    "    \"change\": [\"climate\"],\n",
    "    \"emitting\": [\"non\", \"zero\"],\n",
    "    \"risk\": [\"climate\"],\n",
    "    \"science\": [\"climate\"],\n",
    "    \"warming\": [\"global\"],\n",
    "    \"gas\": [\"greenhouse\"],\n",
    "    \"1\": [\"scope\"],\n",
    "    \"2\": [\"scope\"],\n",
    "    \"3\": [\"scope\"]\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ENVIRONMENTAL CONSERVATION & BIODIVERSITY\n",
    "# =============================================================================\n",
    "\n",
    "# Single-word environmental conservation terms (lemmas)\n",
    "environmental_conservation_nouns = [\n",
    "    \"adaptation\", \"afforestation\", \"biodiversity\", \"conservation\", \"deforestation\", \n",
    "    \"ecology\", \"ecosystem\", \"ecosystemic\", \"environment\", \"forest\", \"habitat\", \n",
    "    \"nature\", \"preservation\", \"reforestation\", \"regeneration\", \"restoration\", \n",
    "    \"soil\", \"species\", \"wildlife\"\n",
    "]\n",
    "\n",
    "environmental_conservation_adjectives = [\n",
    "    \"ecological\", \"environmental\", \"ecosystemic\"\n",
    "]\n",
    "\n",
    "environmental_conservation_adverbs = [\n",
    "    \"ecologically\", \"environmentally\"\n",
    "]\n",
    "\n",
    "environmental_conservation_verbs = [\n",
    "    \"afforest\", \"afforesting\", \"conserve\", \"conserving\", \"preserve\", \"preserving\", \n",
    "    \"reforest\", \"reforesting\", \"regenerate\", \"regenerated\", \"regenerating\", \n",
    "    \"restore\", \"restored\", \"restoring\"\n",
    "]\n",
    "\n",
    "# Multi-word environmental conservation terms (lemmas) - base_word: [modifier_words]\n",
    "environmental_conservation_multiword = {\n",
    "    \"based\": [\"nature\", \"plant\"],\n",
    "    \"environmental\": [\"protected\"],\n",
    "    \"natural\": [\"protected\"],\n",
    "    \"use\": [\"land\"],\n",
    "    \"ecosystem\": [\"marine\"],\n",
    "    \"capital\": [\"natural\"],\n",
    "    \"habitat\": [\"natural\"],\n",
    "    \"area\": [\"protected\"],\n",
    "    \"areas\": [\"protected\"]\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 4. ENERGY SYSTEMS & EFFICIENCY\n",
    "# =============================================================================\n",
    "\n",
    "# Single-word energy systems & efficiency terms (lemmas)\n",
    "energy_systems_nouns = [\n",
    "    \"baseload\", \"battery\", \"ccs\", \"ccus\", \"cogeneration\", \"cooling\", \"demand\", \n",
    "    \"distribution\", \"electrification\", \"energy\", \"ess\", \"fuel\", \"grid\", \"heat\", \n",
    "    \"heating\", \"infrastructure\", \"insulation\", \"load\", \"optimization\", \"peak\", \n",
    "    \"supply\", \"thermal\", \"transmission\", \"transportation\"\n",
    "]\n",
    "\n",
    "energy_systems_adjectives = [\n",
    "    \"efficient\", \"optimal\"\n",
    "]\n",
    "\n",
    "energy_systems_verbs = [\n",
    "    \"cogenerate\", \"cogenerating\", \"electrify\", \"electrifying\", \"optimize\", \n",
    "    \"optimized\", \"optimising\", \"optimizing\"\n",
    "]\n",
    "\n",
    "# Multi-word energy systems & efficiency terms (lemmas) - base_word: [modifier_words]\n",
    "energy_systems_multiword = {\n",
    "    \"electric\": [\"all\", \"geothermal\", \"hydro\", \"tidal\"],\n",
    "    \"technology\": [\"carbon\", \"clean\", \"efficiency\", \"green\", \"renewable\", \"smart\"],\n",
    "    \"technologies\": [\"carbon\", \"clean\", \"efficiency\", \"green\", \"renewable\"],\n",
    "    \"enabled\": [\"ccs\"],\n",
    "    \"equipped\": [\"ccs\"],\n",
    "    \"ready\": [\"ccs\"],\n",
    "    \"generation\": [\"clean\"],\n",
    "    \"power\": [\"clean\", \"green\", \"renewable\"],\n",
    "    \"production\": [\"clean\"],\n",
    "    \"source\": [\"clean\", \"green\"],\n",
    "    \"sources\": [\"clean\", \"green\"],\n",
    "    \"station\": [\"clean\", \"green\", \"hydro\", \"renewable\"],\n",
    "    \"stations\": [\"clean\", \"green\", \"hydro\", \"renewable\"],\n",
    "    \"consumption\": [\"coal\", \"electricity\", \"energy\", \"fuel\", \"gas\", \"oil\", \"power\"],\n",
    "    \"efficient\": [\"eco\", \"energy\", \"fuel\", \"high\", \"resource\"],\n",
    "    \"efficiency\": [\"eco\", \"energy\", \"fuel\", \"high\", \"resource\", \"thermal\"],\n",
    "    \"standard\": [\"efficiency\", \"performance\"],\n",
    "    \"standards\": [\"efficiency\", \"performance\"],\n",
    "    \"usage\": [\"electricity\", \"energy\", \"fuel\", \"power\", \"resource\"],\n",
    "    \"use\": [\"electricity\", \"energy\", \"fuel\", \"power\", \"resource\"],\n",
    "    \"alternative\": [\"energy\"],\n",
    "    \"clean\": [\"energy\"],\n",
    "    \"environmental\": [\"efficient\", \"energy\"],\n",
    "    \"renewable\": [\"energy\"],\n",
    "    \"saved\": [\"energy\"],\n",
    "    \"transition\": [\"energy\"],\n",
    "    \"plant\": [\"geothermal\", \"hydro\", \"renewable\"],\n",
    "    \"plants\": [\"geothermal\", \"hydro\", \"renewable\"],\n",
    "    \"storage\": [\"battery\", \"energy\"],\n",
    "    \"management\": [\"demand\"],\n",
    "    \"infrastructure\": [\"energy\"],\n",
    "    \"pump\": [\"heat\"],\n",
    "    \"grid\": [\"smart\"]\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 5. CIRCULAR ECONOMY & WASTE MANAGEMENT\n",
    "# =============================================================================\n",
    "\n",
    "# Single-word circular economy & waste management terms (lemmas)\n",
    "circular_economy_nouns = [\n",
    "    \"composting\", \"incineration\", \"landfill\", \"lifecycle\", \"repair\", \"waste\"\n",
    "]\n",
    "\n",
    "circular_economy_adjectives = [\n",
    "    \"circular\", \"durable\", \"recoverable\", \"recyclable\", \"recyclable\", \"recycled\", \n",
    "    \"refurbished\", \"regenerable\", \"reusable\"\n",
    "]\n",
    "\n",
    "circular_economy_verbs = [\n",
    "    \"recover\", \"recovered\", \"recycle\", \"recycling\", \"refurbish\", \"reuse\"\n",
    "]\n",
    "\n",
    "# Multi-word circular economy & waste management terms (lemmas) - base_word: [modifier_words]\n",
    "circular_economy_multiword = {\n",
    "    \"economy\": [\"circular\"],\n",
    "    \"free\": [\"waste\"],\n",
    "    \"management\": [\"waste\"],\n",
    "    \"zero\": [\"waste\"],\n",
    "    \"assessment\": [\"lifecycle\"],\n",
    "    \"efficiency\": [\"material\"],\n",
    "    \"material\": [\"raw\", \"virgin\"],\n",
    "    \"materials\": [\"raw\", \"virgin\"]\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 6. SUSTAINABILITY & GOVERNANCE\n",
    "# =============================================================================\n",
    "\n",
    "# Single-word sustainability & governance terms (lemmas)\n",
    "sustainability_governance_nouns = [\n",
    "    \"esg\", \"ethics\", \"governance\", \"innovation\", \"responsibility\", \"social\", \n",
    "    \"sustainability\", \"transparency\"\n",
    "]\n",
    "\n",
    "sustainability_governance_adjectives = [\n",
    "    \"clean\", \"enriching\", \"green\", \"innovative\", \"responsible\", \"sustainable\"\n",
    "]\n",
    "\n",
    "sustainability_governance_adverbs = [\n",
    "    \"sustainably\"\n",
    "]\n",
    "\n",
    "sustainability_governance_verbs = [\n",
    "    \"innovate\", \"innovating\"\n",
    "]\n",
    "\n",
    "# Multi-word sustainability & governance terms (lemmas) - base_word: [modifier_words]\n",
    "sustainability_governance_multiword = {\n",
    "    \"development\": [\"clean\", \"green\", \"renewable\", \"sustainable\"],\n",
    "    \"economy\": [\"green\", \"sustainable\"],\n",
    "    \"growth\": [\"green\", \"sustainable\"],\n",
    "    \"financing\": [\"sustainable\"],\n",
    "    \"production\": [\"responsible\", \"sustainable\"],\n",
    "    \"environmental\": [\"responsible\"],\n",
    "    \"assessment\": [\"impact\"],\n",
    "    \"impact\": [\"social\"],\n",
    "    \"engagement\": [\"stakeholder\"],\n",
    "    \"reporting\": [\"sustainability\"],\n",
    "    \"standard\": [\"sustainability\"],\n",
    "    \"standards\": [\"sustainability\"]\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 7. GREEN FINANCE & INVESTMENT\n",
    "# =============================================================================\n",
    "\n",
    "# Single-word green finance & investment terms (lemmas) - None identified\n",
    "green_finance_nouns = []\n",
    "\n",
    "green_finance_adjectives = []\n",
    "\n",
    "green_finance_verbs = []\n",
    "\n",
    "# Multi-word green finance & investment terms (lemmas) - base_word: [modifier_words]\n",
    "green_finance_multiword = {\n",
    "    \"bond\": [\"climate\", \"green\", \"sustainability\"],\n",
    "    \"bonds\": [\"climate\", \"green\", \"sustainability\"],\n",
    "    \"finance\": [\"blended\", \"climate\", \"green\", \"sustainable\", \"transition\"],\n",
    "    \"financing\": [\"climate\", \"green\", \"sustainable\"],\n",
    "    \"fund\": [\"climate\", \"green\", \"sustainability\"],\n",
    "    \"funds\": [\"climate\", \"green\", \"sustainability\"],\n",
    "    \"investment\": [\"climate\", \"esg\", \"green\", \"sustainable\"],\n",
    "    \"investments\": [\"climate\", \"green\", \"sustainable\"],\n",
    "    \"investing\": [\"esg\", \"impact\", \"sustainable\"],\n",
    "    \"loan\": [\"sustainable\"],\n",
    "    \"loans\": [\"sustainable\"]\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# CONSOLIDATED TOPIC DICTIONARY\n",
    "# =============================================================================\n",
    "\n",
    "# All topic categories in one dictionary for easy access\n",
    "GREEN_TOPICS = {\n",
    "    \"renewable_energy\": {\n",
    "        \"nouns\": renewable_energy_nouns,\n",
    "        \"adjectives\": [],\n",
    "        \"verbs\": [],\n",
    "        \"adverbs\": renewable_energy_adverbs,\n",
    "        \"multiword\": renewable_energy_multiword\n",
    "    },\n",
    "    \"climate_emissions\": {\n",
    "        \"nouns\": climate_emissions_nouns,\n",
    "        \"adjectives\": [],\n",
    "        \"verbs\": climate_emissions_verbs,\n",
    "        \"adverbs\": [],\n",
    "        \"multiword\": climate_emissions_multiword\n",
    "    },\n",
    "    \"environmental_conservation\": {\n",
    "        \"nouns\": environmental_conservation_nouns,\n",
    "        \"adjectives\": environmental_conservation_adjectives,\n",
    "        \"verbs\": environmental_conservation_verbs,\n",
    "        \"adverbs\": environmental_conservation_adverbs,\n",
    "        \"multiword\": environmental_conservation_multiword\n",
    "    },\n",
    "    \"energy_systems\": {\n",
    "        \"nouns\": energy_systems_nouns,\n",
    "        \"adjectives\": energy_systems_adjectives,\n",
    "        \"verbs\": energy_systems_verbs,\n",
    "        \"adverbs\": [],\n",
    "        \"multiword\": energy_systems_multiword\n",
    "    },\n",
    "    \"circular_economy\": {\n",
    "        \"nouns\": circular_economy_nouns,\n",
    "        \"adjectives\": circular_economy_adjectives,\n",
    "        \"verbs\": circular_economy_verbs,\n",
    "        \"adverbs\": [],\n",
    "        \"multiword\": circular_economy_multiword\n",
    "    },\n",
    "    \"sustainability_governance\": {\n",
    "        \"nouns\": sustainability_governance_nouns,\n",
    "        \"adjectives\": sustainability_governance_adjectives,\n",
    "        \"verbs\": sustainability_governance_verbs,\n",
    "        \"adverbs\": sustainability_governance_adverbs,\n",
    "        \"multiword\": sustainability_governance_multiword\n",
    "    },\n",
    "    \"green_finance\": {\n",
    "        \"nouns\": green_finance_nouns,\n",
    "        \"adjectives\": green_finance_adjectives,\n",
    "        \"verbs\": green_finance_verbs,\n",
    "        \"adverbs\": [],\n",
    "        \"multiword\": green_finance_multiword\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceca40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for topic analysis\n",
    "\n",
    "def count_valid_words(doc):\n",
    "    \"\"\"\n",
    "    Count total valid words in document (excluding stop words, punctuation, whitespace).\n",
    "    \"\"\"\n",
    "    valid_count = 0\n",
    "    for token in doc:\n",
    "        if (not token.is_stop and \n",
    "            not token.is_punct and \n",
    "            not token.is_space and \n",
    "            token.text.strip()):\n",
    "            valid_count += 1\n",
    "    return valid_count\n",
    "\n",
    "def is_position_excluded(token_idx, excluded_positions):\n",
    "    \"\"\"\n",
    "    Check if token position is already used in another term.\n",
    "    \"\"\"\n",
    "    return token_idx in excluded_positions\n",
    "\n",
    "def mark_positions_as_used(start_idx, end_idx, excluded_positions):\n",
    "    \"\"\"\n",
    "    Mark token positions as used to prevent double counting.\n",
    "    \"\"\"\n",
    "    for i in range(start_idx, end_idx + 1):\n",
    "        excluded_positions.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14215d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_multiword_topic_terms(doc, excluded_positions):\n",
    "    \"\"\"\n",
    "    Find multiword terms across all topics with position tracking.\n",
    "    Returns: (found_terms, updated_excluded_positions)\n",
    "    \"\"\"\n",
    "    found_terms = []\n",
    "    tokens = [token.lemma_.lower() for token in doc]\n",
    "    \n",
    "    # Process all topics\n",
    "    for topic_name, topic_data in GREEN_TOPICS.items():\n",
    "        multiword_dict = topic_data[\"multiword\"]\n",
    "        \n",
    "        for base_word, modifiers in multiword_dict.items():\n",
    "            for modifier in modifiers:\n",
    "                # Create search patterns\n",
    "                pattern = f\"{modifier} {base_word}\"\n",
    "                \n",
    "                # Search for pattern in token sequence\n",
    "                for i in range(len(tokens) - 1):\n",
    "                    if (i not in excluded_positions and \n",
    "                        (i + 1) not in excluded_positions):\n",
    "                        \n",
    "                        if (tokens[i] == modifier and \n",
    "                            tokens[i + 1] == base_word):\n",
    "                            \n",
    "                            # Create term info\n",
    "                            term_info = {\n",
    "                                'term': pattern,\n",
    "                                'topic': topic_name,\n",
    "                                'start_idx': i,\n",
    "                                'end_idx': i + 1,\n",
    "                                'sentence': doc[i].sent\n",
    "                            }\n",
    "                            found_terms.append(term_info)\n",
    "                            \n",
    "                            # Mark positions as used\n",
    "                            mark_positions_as_used(i, i + 1, excluded_positions)\n",
    "    \n",
    "    return found_terms, excluded_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea971cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_single_topic_terms(doc, excluded_positions):\n",
    "    \"\"\"\n",
    "    Find single word terms across all topics, excluding already used positions.\n",
    "    Returns: found_terms\n",
    "    \"\"\"\n",
    "    found_terms = []\n",
    "    \n",
    "    # Process all topics and word types\n",
    "    for topic_name, topic_data in GREEN_TOPICS.items():\n",
    "        word_types = ['nouns', 'adjectives', 'verbs', 'adverbs']\n",
    "        \n",
    "        for word_type in word_types:\n",
    "            word_list = topic_data[word_type]\n",
    "            \n",
    "            for i, token in enumerate(doc):\n",
    "                if is_position_excluded(i, excluded_positions):\n",
    "                    continue\n",
    "                \n",
    "                lemma_lower = token.lemma_.lower()\n",
    "                \n",
    "                if lemma_lower in word_list:\n",
    "                    term_info = {\n",
    "                        'term': token.text,\n",
    "                        'topic': topic_name,\n",
    "                        'start_idx': i,\n",
    "                        'end_idx': i,\n",
    "                        'sentence': token.sent,\n",
    "                        'pos': word_type\n",
    "                    }\n",
    "                    found_terms.append(term_info)\n",
    "                    \n",
    "                    # Mark position as used\n",
    "                    excluded_positions.add(i)\n",
    "    \n",
    "    return found_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14db89ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_topic_densities(all_found_terms, total_valid_words):\n",
    "    \"\"\"\n",
    "    Calculate density and counts for each topic.\n",
    "    Returns: topic_results dictionary\n",
    "    \"\"\"\n",
    "    topic_results = {}\n",
    "    \n",
    "    # Initialize all topics\n",
    "    for topic_name in GREEN_TOPICS.keys():\n",
    "        topic_results[topic_name] = {\n",
    "            'count': 0,\n",
    "            'density': 0.0,\n",
    "            'terms_found': []\n",
    "        }\n",
    "    \n",
    "    # Count terms per topic\n",
    "    for term_info in all_found_terms:\n",
    "        topic = term_info['topic']\n",
    "        topic_results[topic]['count'] += 1\n",
    "        topic_results[topic]['terms_found'].append(term_info['term'])\n",
    "    \n",
    "    # Calculate densities as percentage\n",
    "    for topic_name, results in topic_results.items():\n",
    "        if total_valid_words > 0:\n",
    "            results['density'] = (results['count'] / total_valid_words) * 100\n",
    "        else:\n",
    "            results['density'] = 0.0\n",
    "    \n",
    "    return topic_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ffba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_document_topics(doc, document_name):\n",
    "    \"\"\"\n",
    "    Main function to analyze topics in a document.\n",
    "    Returns: complete analysis results\n",
    "    \"\"\"\n",
    "    excluded_positions = set()\n",
    "    \n",
    "    # Step 1: Find multiword terms first (priority)\n",
    "    multiword_terms, excluded_positions = find_multiword_topic_terms(doc, excluded_positions)\n",
    "    \n",
    "    # Step 2: Find single word terms (excluding used positions)\n",
    "    single_terms = find_single_topic_terms(doc, excluded_positions)\n",
    "    \n",
    "    # Step 3: Combine all found terms\n",
    "    all_found_terms = multiword_terms + single_terms\n",
    "    \n",
    "    # Step 4: Count total valid words in document\n",
    "    total_valid_words = count_valid_words(doc)\n",
    "    \n",
    "    # Step 5: Calculate topic densities and counts\n",
    "    topic_results = calculate_topic_densities(all_found_terms, total_valid_words)\n",
    "    \n",
    "    # Step 6: Create final result structure\n",
    "    result = {\n",
    "        'document_name': document_name,\n",
    "        'total_valid_words': total_valid_words,\n",
    "        'total_terms_found': len(all_found_terms),\n",
    "        'topics': topic_results\n",
    "    }\n",
    "    \n",
    "    return result, all_found_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_company_years(result_2021, result_2022, terms_2021, terms_2022, company_name):\n",
    "    \"\"\"\n",
    "    Compare topic analysis results between two years for the same company.\n",
    "    Focuses on density changes and individual term changes.\n",
    "    Returns: comparison analysis with term-level details\n",
    "    \"\"\"\n",
    "    comparison = {\n",
    "        'company': company_name,\n",
    "        'year_2021': {\n",
    "            'total_valid_words': result_2021['total_valid_words'],\n",
    "            'total_terms_found': result_2021['total_terms_found']\n",
    "        },\n",
    "        'year_2022': {\n",
    "            'total_valid_words': result_2022['total_valid_words'],\n",
    "            'total_terms_found': result_2022['total_terms_found']\n",
    "        },\n",
    "        'topic_comparison': {},\n",
    "        'term_changes': {\n",
    "            'increasing_terms': [],  # Terms that increased significantly\n",
    "            'decreasing_terms': []   # Terms that decreased significantly\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create term frequency dictionaries for both years\n",
    "    terms_freq_2021 = {}\n",
    "    terms_freq_2022 = {}\n",
    "    \n",
    "    for term_info in terms_2021:\n",
    "        term = term_info['term']\n",
    "        if term not in terms_freq_2021:\n",
    "            terms_freq_2021[term] = 0\n",
    "        terms_freq_2021[term] += 1\n",
    "    \n",
    "    for term_info in terms_2022:\n",
    "        term = term_info['term']\n",
    "        if term not in terms_freq_2022:\n",
    "            terms_freq_2022[term] = 0\n",
    "        terms_freq_2022[term] += 1\n",
    "    \n",
    "    # Compare each topic\n",
    "    for topic_name in GREEN_TOPICS.keys():\n",
    "        topic_2021 = result_2021['topics'][topic_name]\n",
    "        topic_2022 = result_2022['topics'][topic_name]\n",
    "        \n",
    "        # Calculate changes (density is already in percentage)\n",
    "        count_change = topic_2022['count'] - topic_2021['count']\n",
    "        density_change = topic_2022['density'] - topic_2021['density']\n",
    "        \n",
    "        # Calculate percentage change in density\n",
    "        if topic_2021['density'] > 0:\n",
    "            density_percentage = (density_change / topic_2021['density']) * 100\n",
    "        else:\n",
    "            density_percentage = 100 if topic_2022['density'] > 0 else 0\n",
    "        \n",
    "        comparison['topic_comparison'][topic_name] = {\n",
    "            '2021_count': topic_2021['count'],\n",
    "            '2022_count': topic_2022['count'],\n",
    "            '2021_density': round(topic_2021['density'], 4),\n",
    "            '2022_density': round(topic_2022['density'], 4),\n",
    "            'count_change': count_change,\n",
    "            'density_change': round(density_change, 4),\n",
    "            'density_percentage_change': round(density_percentage, 1)\n",
    "        }\n",
    "    \n",
    "    # Analyze individual term changes\n",
    "    all_terms = set(terms_freq_2021.keys()) | set(terms_freq_2022.keys())\n",
    "    \n",
    "    for term in all_terms:\n",
    "        freq_2021 = terms_freq_2021.get(term, 0)\n",
    "        freq_2022 = terms_freq_2022.get(term, 0)\n",
    "        \n",
    "        # Look for significant changes (rare in one year, common in another)\n",
    "        if freq_2021 <= 1 and freq_2022 >= 3:  # Rare in 2021, more common in 2022\n",
    "            comparison['term_changes']['increasing_terms'].append({\n",
    "                'term': term,\n",
    "                '2021_freq': freq_2021,\n",
    "                '2022_freq': freq_2022,\n",
    "                'change': freq_2022 - freq_2021\n",
    "            })\n",
    "        elif freq_2021 >= 3 and freq_2022 <= 1:  # Common in 2021, rare in 2022\n",
    "            comparison['term_changes']['decreasing_terms'].append({\n",
    "                'term': term,\n",
    "                '2021_freq': freq_2021,\n",
    "                '2022_freq': freq_2022,\n",
    "                'change': freq_2022 - freq_2021\n",
    "            })\n",
    "    \n",
    "    # Sort term changes by magnitude\n",
    "    comparison['term_changes']['increasing_terms'].sort(key=lambda x: x['change'], reverse=True)\n",
    "    comparison['term_changes']['decreasing_terms'].sort(key=lambda x: abs(x['change']), reverse=True)\n",
    "    \n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7446828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all documents and store results\n",
    "document_results = {}\n",
    "document_terms = {}\n",
    "\n",
    "print(\"Processing documents for topic analysis...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for doc_key, doc in documents.items():\n",
    "    print(f\"Analyzing: {doc_key}\")\n",
    "    \n",
    "    # Analyze document topics\n",
    "    result, terms = analyze_document_topics(doc, doc_key)\n",
    "    \n",
    "    # Store results\n",
    "    document_results[doc_key] = result\n",
    "    document_terms[doc_key] = terms\n",
    "    \n",
    "    # Print summary\n",
    "    total_terms = result['total_terms_found']\n",
    "    total_words = result['total_valid_words']\n",
    "    overall_density = round(total_terms / total_words, 6) if total_words > 0 else 0\n",
    "    \n",
    "    print(f\"  Total valid words: {total_words}\")\n",
    "    print(f\"  Total terms found: {total_terms}\")\n",
    "    print(f\"  Overall topic density: {overall_density*100:.2f}%\")\n",
    "    print()\n",
    "\n",
    "print(\"Topic analysis completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f65724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed topic analysis results\n",
    "def display_topic_results(document_results):\n",
    "    \"\"\"\n",
    "    Display topic analysis results in a formatted way.\n",
    "    \"\"\"\n",
    "    for doc_name, result in document_results.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TOPIC ANALYSIS: {doc_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total valid words: {result['total_valid_words']}\")\n",
    "        print(f\"Total terms found: {result['total_terms_found']}\")\n",
    "        print()\n",
    "        \n",
    "        # Sort topics by count (descending)\n",
    "        sorted_topics = sorted(\n",
    "            result['topics'].items(), \n",
    "            key=lambda x: x[1]['count'], \n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        print(\"TOPIC BREAKDOWN:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"{'Topic':<25} {'Count':<8} {'Density %':<10} {'Sample Terms'}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for topic_name, topic_data in sorted_topics:\n",
    "            # Format topic name for display\n",
    "            display_name = topic_name.replace('_', ' ').title()\n",
    "            \n",
    "            # Get sample terms (first 3)\n",
    "            sample_terms = topic_data['terms_found'][:3]\n",
    "            sample_str = \", \".join(sample_terms) if sample_terms else \"None\"\n",
    "            if len(sample_str) > 30:\n",
    "                sample_str = sample_str[:27] + \"...\"\n",
    "            \n",
    "            print(f\"{display_name:<25} {topic_data['count']:<8} \"\n",
    "                  f\"{topic_data['density']:<10.4f} {sample_str}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "# Display results\n",
    "display_topic_results(document_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf64d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year-over-year comparison analysis with term-level details\n",
    "print(\"YEAR-OVER-YEAR TOPIC COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Extract company names (assuming format: CompanyName_YEAR)\n",
    "companies = {}\n",
    "company_terms = {}\n",
    "\n",
    "for doc_key in document_results.keys():\n",
    "    if '_2021' in doc_key or '_2022' in doc_key:\n",
    "        company = doc_key.replace('_2021', '').replace('_2022', '')\n",
    "        if company not in companies:\n",
    "            companies[company] = {}\n",
    "            company_terms[company] = {}\n",
    "        \n",
    "        year = '2021' if '_2021' in doc_key else '2022'\n",
    "        companies[company][year] = document_results[doc_key]\n",
    "        company_terms[company][year] = document_terms[doc_key]\n",
    "\n",
    "# Compare each company\n",
    "for company_name, years_data in companies.items():\n",
    "    if '2021' in years_data and '2022' in years_data:\n",
    "        print(f\"\\nCOMPANY: {company_name}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Perform comparison with term data\n",
    "        comparison = compare_company_years(\n",
    "            years_data['2021'], \n",
    "            years_data['2022'],\n",
    "            company_terms[company_name]['2021'],\n",
    "            company_terms[company_name]['2022'],\n",
    "            company_name\n",
    "        )\n",
    "        \n",
    "        # Display overall changes\n",
    "        word_change = comparison['year_2022']['total_valid_words'] - comparison['year_2021']['total_valid_words']\n",
    "        term_change = comparison['year_2022']['total_terms_found'] - comparison['year_2021']['total_terms_found']\n",
    "        \n",
    "        print(f\"Document size change: {word_change:+d} words\")\n",
    "        print(f\"Total terms change: {term_change:+d} terms\")\n",
    "        print()\n",
    "        \n",
    "        # Display topic density changes (focus on density percentage change)\n",
    "        print(f\"{'Topic':<25} {'2021%':<8} {'2022%':<8} {'Density Δ':<10} {'% Change':<10}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Sort by absolute density percentage change (descending)\n",
    "        sorted_topics = sorted(\n",
    "            comparison['topic_comparison'].items(),\n",
    "            key=lambda x: abs(x[1]['density_percentage_change']),\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        for topic_name, topic_comp in sorted_topics:\n",
    "            display_name = topic_name.replace('_', ' ').title()\n",
    "            \n",
    "            print(f\"{display_name:<25} \"\n",
    "                  f\"{topic_comp['2021_density']:<8.3f} \"\n",
    "                  f\"{topic_comp['2022_density']:<8.3f} \"\n",
    "                  f\"{topic_comp['density_change']:+8.3f} \"\n",
    "                  f\"{topic_comp['density_percentage_change']:+8.1f}%\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # Display increasing terms (rare in 2021, common in 2022)\n",
    "        increasing_terms = comparison['term_changes']['increasing_terms']\n",
    "        if increasing_terms:\n",
    "            print(\"TERMS GAINING PROMINENCE (rare in 2021, more common in 2022):\")\n",
    "            print(\"-\" * 60)\n",
    "            print(f\"{'Term':<30} {'2021':<6} {'2022':<6} {'Change'}\")\n",
    "            print(\"-\" * 60)\n",
    "            for term_info in increasing_terms[:10]:  # Show top 10\n",
    "                print(f\"{term_info['term']:<30} \"\n",
    "                      f\"{term_info['2021_freq']:<6} \"\n",
    "                      f\"{term_info['2022_freq']:<6} \"\n",
    "                      f\"{term_info['change']:+d}\")\n",
    "            print()\n",
    "        \n",
    "        # Display decreasing terms (common in 2021, rare in 2022)\n",
    "        decreasing_terms = comparison['term_changes']['decreasing_terms']\n",
    "        if decreasing_terms:\n",
    "            print(\"TERMS LOSING PROMINENCE (common in 2021, rare in 2022):\")\n",
    "            print(\"-\" * 60)\n",
    "            print(f\"{'Term':<30} {'2021':<6} {'2022':<6} {'Change'}\")\n",
    "            print(\"-\" * 60)\n",
    "            for term_info in decreasing_terms[:10]:  # Show top 10\n",
    "                print(f\"{term_info['term']:<30} \"\n",
    "                      f\"{term_info['2021_freq']:<6} \"\n",
    "                      f\"{term_info['2022_freq']:<6} \"\n",
    "                      f\"{term_info['change']:+d}\")\n",
    "            print()\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nCOMPANY: {company_name}\")\n",
    "        print(\"Incomplete data - missing 2021 or 2022 report\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad61e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topic_analysis_dataframe(document_results):\n",
    "    \"\"\"\n",
    "    Create a DataFrame for topic analysis.\n",
    "    Rows: Organizations (company-year combinations)\n",
    "    Columns: Topic metrics (counts and density percentages for each topic)\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for doc_name, result_data in document_results.items():\n",
    "        \n",
    "        # Extract organization and year from document name\n",
    "        parts = doc_name.split('_')\n",
    "        year = parts[-1]\n",
    "        org_name = '_'.join(parts[:-1])\n",
    "        \n",
    "        # Get basic metrics\n",
    "        total_valid_words = result_data['total_valid_words']\n",
    "        total_terms_found = result_data['total_terms_found']\n",
    "        overall_density = round((total_terms_found / total_valid_words * 100) if total_valid_words > 0 else 0, 4)\n",
    "        \n",
    "        # TOPIC-SPECIFIC METRICS\n",
    "        topic_stats = {}\n",
    "        topic_densities = []\n",
    "        topic_counts = []\n",
    "        \n",
    "        for topic_name, topic_data in result_data['topics'].items():\n",
    "            count = topic_data['count']\n",
    "            density = topic_data['density']\n",
    "            \n",
    "            # Store individual topic metrics\n",
    "            topic_stats[f'{topic_name}_count'] = count\n",
    "            topic_stats[f'{topic_name}_density'] = round(density, 4)\n",
    "            \n",
    "            topic_densities.append(density)\n",
    "            topic_counts.append(count)\n",
    "        \n",
    "        # COMBINED TOPIC INSIGHTS\n",
    "        # Most prominent topic\n",
    "        max_density_topic = max(result_data['topics'].items(), key=lambda x: x[1]['density'])\n",
    "        most_prominent_topic = max_density_topic[0]\n",
    "        highest_density = round(max_density_topic[1]['density'], 4)\n",
    "        \n",
    "        # Topic diversity (number of topics with terms)\n",
    "        active_topics = sum(1 for topic_data in result_data['topics'].values() if topic_data['count'] > 0)\n",
    "        \n",
    "        # Topic concentration (percentage of terms in most prominent topic)\n",
    "        if total_terms_found > 0:\n",
    "            concentration = round((max_density_topic[1]['count'] / total_terms_found * 100), 2)\n",
    "        else:\n",
    "            concentration = 0\n",
    "        \n",
    "        # Topic balance (standard deviation of densities)\n",
    "        if topic_densities:\n",
    "            import statistics\n",
    "            topic_balance = round(statistics.stdev(topic_densities) if len(topic_densities) > 1 else 0, 4)\n",
    "        else:\n",
    "            topic_balance = 0\n",
    "        \n",
    "        # Specific topic combinations\n",
    "        renewable_plus_climate = (result_data['topics']['renewable_energy']['count'] + \n",
    "                                 result_data['topics']['climate_emissions']['count'])\n",
    "        \n",
    "        governance_plus_finance = (result_data['topics']['sustainability_governance']['count'] + \n",
    "                                  result_data['topics']['green_finance']['count'])\n",
    "        \n",
    "        # Create the row dictionary\n",
    "        row = {\n",
    "            # Basic identifiers\n",
    "            'organization': org_name,\n",
    "            'year': int(year),\n",
    "            \n",
    "            # Overall metrics\n",
    "            'total_valid_words': total_valid_words,\n",
    "            'total_terms_found': total_terms_found,\n",
    "            'overall_density': overall_density,\n",
    "            \n",
    "            # Individual topic metrics\n",
    "            **topic_stats,\n",
    "            \n",
    "            # Combined insights\n",
    "            'most_prominent_topic': most_prominent_topic,\n",
    "            'highest_topic_density': highest_density,\n",
    "            'active_topics': active_topics,\n",
    "            'topic_concentration': concentration,\n",
    "            'topic_balance': topic_balance,\n",
    "            'renewable_plus_climate': renewable_plus_climate,\n",
    "            'governance_plus_finance': governance_plus_finance\n",
    "        }\n",
    "        \n",
    "        data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create the topic analysis DataFrame\n",
    "import pandas as pd\n",
    "topic_analysis_df = create_topic_analysis_dataframe(document_results)\n",
    "\n",
    "# Sort by organization and year for better readability\n",
    "topic_analysis_df = topic_analysis_df.sort_values(['organization', 'year']).reset_index(drop=True)\n",
    "\n",
    "print(\"TOPIC ANALYSIS DATAFRAME CREATED\")\n",
    "print(\"=\"*80)\n",
    "print(topic_analysis_df.to_string(index=False))\n",
    "\n",
    "# Export to Excel\n",
    "import os\n",
    "excel_path = \"data/NLP/Results/Communication_Score_df_Topics.xlsx\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(excel_path), exist_ok=True)\n",
    "\n",
    "topic_analysis_df.to_excel(excel_path, index=False, sheet_name='Topic_Analysis')\n",
    "print(f\"\\nExported to Excel: {excel_path}\")\n",
    "print(f\"DataFrame shape: {topic_analysis_df.shape[0]} organizations × {topic_analysis_df.shape[1]} topic metrics\")\n",
    "\n",
    "# Column descriptions for reference\n",
    "print(f\"\\nCOLUMN DESCRIPTIONS:\")\n",
    "column_descriptions = {\n",
    "    # Basic identifiers\n",
    "    'organization': 'Organization name',\n",
    "    'year': 'Report year',\n",
    "    \n",
    "    # Overall metrics\n",
    "    'total_valid_words': 'Total meaningful words in document',\n",
    "    'total_terms_found': 'Total sustainability terms found',\n",
    "    'overall_density': 'Overall sustainability density percentage',\n",
    "    \n",
    "    # Topic counts\n",
    "    'renewable_energy_count': 'Renewable energy terms count',\n",
    "    'climate_emissions_count': 'Climate & emissions terms count',\n",
    "    'environmental_conservation_count': 'Environmental conservation terms count',\n",
    "    'energy_systems_count': 'Energy systems & efficiency terms count',\n",
    "    'circular_economy_count': 'Circular economy & waste terms count',\n",
    "    'sustainability_governance_count': 'Sustainability & governance terms count',\n",
    "    'green_finance_count': 'Green finance & investment terms count',\n",
    "    \n",
    "    # Topic densities\n",
    "    'renewable_energy_density': 'Renewable energy density percentage',\n",
    "    'climate_emissions_density': 'Climate & emissions density percentage',\n",
    "    'environmental_conservation_density': 'Environmental conservation density percentage',\n",
    "    'energy_systems_density': 'Energy systems & efficiency density percentage',\n",
    "    'circular_economy_density': 'Circular economy & waste density percentage',\n",
    "    'sustainability_governance_density': 'Sustainability & governance density percentage',\n",
    "    'green_finance_density': 'Green finance & investment density percentage',\n",
    "    \n",
    "    # Combined insights\n",
    "    'most_prominent_topic': 'Topic with highest density',\n",
    "    'highest_topic_density': 'Density of most prominent topic',\n",
    "    'active_topics': 'Number of topics with at least one term',\n",
    "    'topic_concentration': 'Percentage of terms in most prominent topic',\n",
    "    'topic_balance': 'Standard deviation of topic densities',\n",
    "    'renewable_plus_climate': 'Combined renewable energy + climate terms',\n",
    "    'governance_plus_finance': 'Combined governance + finance terms'\n",
    "}\n",
    "\n",
    "for col, desc in column_descriptions.items():\n",
    "    if col in topic_analysis_df.columns:\n",
    "        print(f\"  {col:<35}: {desc}\")\n",
    "\n",
    "print(f\"\\nData saved as: {excel_path}\")\n",
    "print(f\"Variable available as: topic_analysis_df\")\n",
    "print(\"Contains topic analysis metrics (counts, densities, insights)\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TOPIC ANALYSIS SUMMARY STATISTICS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total organizations analyzed: {topic_analysis_df['organization'].nunique()}\")\n",
    "print(f\"Total documents: {len(topic_analysis_df)}\")\n",
    "print(f\"Year range: {topic_analysis_df['year'].min()} - {topic_analysis_df['year'].max()}\")\n",
    "\n",
    "print(f\"\\nOverall Metrics (averages):\")\n",
    "print(f\"  Total valid words: {topic_analysis_df['total_valid_words'].mean():.0f}\")\n",
    "print(f\"  Total terms found: {topic_analysis_df['total_terms_found'].mean():.1f}\")\n",
    "print(f\"  Overall density: {topic_analysis_df['overall_density'].mean():.2f}%\")\n",
    "\n",
    "print(f\"\\nTopic Density Distribution (averages):\")\n",
    "topic_cols = [col for col in topic_analysis_df.columns if col.endswith('_density')]\n",
    "for col in topic_cols:\n",
    "    topic_name = col.replace('_density', '').replace('_', ' ').title()\n",
    "    print(f\"  {topic_name:<35}: {topic_analysis_df[col].mean():.3f}%\")\n",
    "\n",
    "print(f\"\\nTopic Insights (averages):\")\n",
    "print(f\"  Active topics per document: {topic_analysis_df['active_topics'].mean():.1f}\")\n",
    "print(f\"  Topic concentration: {topic_analysis_df['topic_concentration'].mean():.1f}%\")\n",
    "print(f\"  Topic balance (lower = more balanced): {topic_analysis_df['topic_balance'].mean():.3f}\")\n",
    "print(f\"  Renewable + Climate terms: {topic_analysis_df['renewable_plus_climate'].mean():.1f}\")\n",
    "print(f\"  Governance + Finance terms: {topic_analysis_df['governance_plus_finance'].mean():.1f}\")\n",
    "\n",
    "print(f\"\\nMost Prominent Topics:\")\n",
    "topic_prominence = topic_analysis_df['most_prominent_topic'].value_counts()\n",
    "for topic, count in topic_prominence.items():\n",
    "    topic_display = topic.replace('_', ' ').title()\n",
    "    print(f\"  {topic_display}: {count} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ef28c",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d44b1a",
   "metadata": {},
   "source": [
    "### Overall Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a2c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CLIMATE-BERT SENTIMENT ANALYSIS EXECUTION\n",
    "# This analyzes climate-specific sentiment using Climate-BERT which classifies text as:\n",
    "# - OPPORTUNITY: Positive climate/sustainability sentiment\n",
    "# - RISK: Negative climate/sustainability sentiment  \n",
    "# - NEUTRAL: Neutral climate/sustainability sentiment\n",
    "# Add this to your existing notebook after your topic analysis\n",
    "# ===================================================================\n",
    "\n",
    "# First, install required packages if not already installed\n",
    "# Run this in a separate cell first:\n",
    "# !pip install transformers torch\n",
    "\n",
    "print(\"Starting Climate-BERT Climate Sentiment Analysis\")\n",
    "print(\"Climate-BERT classifies sustainability text as OPPORTUNITY/RISK/NEUTRAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Initialize Climate-BERT Model\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def initialize_climate_bert():\n",
    "    \"\"\"Initialize Climate-BERT model for sentiment analysis.\"\"\"\n",
    "    try:\n",
    "        print(\"Loading Climate-BERT model...\")\n",
    "        model_name = \"climatebert/distilroberta-base-climate-sentiment\"\n",
    "        \n",
    "        climate_sentiment = pipeline(\n",
    "            \"sentiment-analysis\", \n",
    "            model=model_name,\n",
    "            return_all_scores=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        print(\"Climate-BERT model loaded successfully!\")\n",
    "        return climate_sentiment\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Climate-BERT: {e}\")\n",
    "        print(\"Falling back to RoBERTa-base sentiment model...\")\n",
    "        \n",
    "        # Fallback model\n",
    "        fallback_sentiment = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "            return_all_scores=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        print(\"Fallback model loaded successfully!\")\n",
    "        return fallback_sentiment\n",
    "\n",
    "# Initialize the model\n",
    "climate_sentiment_model = initialize_climate_bert()\n",
    "\n",
    "# Step 2: Extract Sustainability Sentences\n",
    "def extract_sustainability_sentences_from_terms(doc, found_terms):\n",
    "    \"\"\"Extract sentences containing sustainability terms.\"\"\"\n",
    "    sustainability_sentences = []\n",
    "    processed_sentences = set()\n",
    "    \n",
    "    for term_info in found_terms:\n",
    "        # Use the sentence directly from term_info (it's already a spaCy Span)\n",
    "        sentence = term_info['sentence']\n",
    "        sentence_text = sentence.text.strip()\n",
    "        \n",
    "        # Avoid duplicates and very short sentences\n",
    "        if sentence_text not in processed_sentences and len(sentence_text) > 20:\n",
    "            sustainability_sentences.append({\n",
    "                'text': sentence_text,\n",
    "                'term_found': term_info['term'],\n",
    "                'topic': term_info['topic'],\n",
    "                'start_char': sentence.start_char,\n",
    "                'end_char': sentence.end_char\n",
    "            })\n",
    "            processed_sentences.add(sentence_text)\n",
    "    \n",
    "    return sustainability_sentences\n",
    "\n",
    "# Step 3: Analyze Sentiment\n",
    "def analyze_sentences_sentiment(sustainability_sentences, model):\n",
    "    \"\"\"Analyze sentiment of sustainability sentences.\"\"\"\n",
    "    if not sustainability_sentences:\n",
    "        return {\n",
    "            'avg_sentiment_score': 0.0,\n",
    "            'positive_ratio': 0.0,\n",
    "            'negative_ratio': 0.0,\n",
    "            'neutral_ratio': 0.0,\n",
    "            'sentiment_confidence': 0.0,\n",
    "            'sentiment_volatility': 0.0,\n",
    "            'total_sentences': 0,\n",
    "            'detailed_results': []\n",
    "        }\n",
    "    \n",
    "    sentiment_results = []\n",
    "    \n",
    "    for sent_info in sustainability_sentences:\n",
    "        text = sent_info['text']\n",
    "        \n",
    "        # Truncate very long sentences\n",
    "        if len(text) > 500:\n",
    "            text = text[:500] + \"...\"\n",
    "        \n",
    "        try:\n",
    "            # Get sentiment prediction\n",
    "            result = model(text)\n",
    "            \n",
    "            # Process Climate-BERT results (uses 'opportunity', 'risk', 'neutral')\n",
    "            if isinstance(result[0], list):\n",
    "                scores_dict = {item['label']: item['score'] for item in result[0]}\n",
    "                \n",
    "                # Climate-BERT specific labels\n",
    "                opportunity_score = scores_dict.get('opportunity', 0)  # Positive climate sentiment\n",
    "                risk_score = scores_dict.get('risk', 0)               # Negative climate sentiment  \n",
    "                neutral_score = scores_dict.get('neutral', 0)         # Neutral climate sentiment\n",
    "                \n",
    "                # Calculate sentiment score (-1 to +1)\n",
    "                # opportunity = positive, risk = negative\n",
    "                sentiment_score = opportunity_score - risk_score\n",
    "                confidence = max(opportunity_score, risk_score, neutral_score)\n",
    "                \n",
    "                # Determine primary label\n",
    "                if opportunity_score > max(risk_score, neutral_score):\n",
    "                    label = 'OPPORTUNITY'\n",
    "                elif risk_score > max(opportunity_score, neutral_score):\n",
    "                    label = 'RISK'\n",
    "                else:\n",
    "                    label = 'NEUTRAL'\n",
    "            else:\n",
    "                # Single result format (shouldn't happen with return_all_scores=True)\n",
    "                main_result = result[0]\n",
    "                label = main_result['label']\n",
    "                confidence = main_result['score']\n",
    "                \n",
    "                # Convert Climate-BERT labels to sentiment score\n",
    "                if 'opportunity' in label.lower():\n",
    "                    sentiment_score = confidence\n",
    "                    label = 'OPPORTUNITY'\n",
    "                elif 'risk' in label.lower():\n",
    "                    sentiment_score = -confidence\n",
    "                    label = 'RISK'\n",
    "                else:\n",
    "                    sentiment_score = 0.0\n",
    "                    label = 'NEUTRAL'\n",
    "            \n",
    "            sentiment_results.append({\n",
    "                'text': sent_info['text'],\n",
    "                'term_found': sent_info['term_found'],\n",
    "                'topic': sent_info['topic'],\n",
    "                'sentiment_score': sentiment_score,\n",
    "                'sentiment_label': label,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sentence: {e}\")\n",
    "            sentiment_results.append({\n",
    "                'text': sent_info['text'],\n",
    "                'term_found': sent_info['term_found'],\n",
    "                'topic': sent_info['topic'],\n",
    "                'sentiment_score': 0.0,\n",
    "                'sentiment_label': 'NEUTRAL',\n",
    "                'confidence': 0.5\n",
    "            })\n",
    "    \n",
    "    # Calculate aggregate metrics\n",
    "    scores = [r['sentiment_score'] for r in sentiment_results]\n",
    "    labels = [r['sentiment_label'] for r in sentiment_results]\n",
    "    confidences = [r['confidence'] for r in sentiment_results]\n",
    "    \n",
    "    # Count Climate-BERT labels correctly\n",
    "    opportunity_count = sum(1 for label in labels if label == 'OPPORTUNITY')\n",
    "    risk_count = sum(1 for label in labels if label == 'RISK')\n",
    "    neutral_count = sum(1 for label in labels if label == 'NEUTRAL')\n",
    "    \n",
    "    total_sentences = len(sentiment_results)\n",
    "    \n",
    "    return {\n",
    "        'avg_sentiment_score': np.mean(scores) if scores else 0.0,\n",
    "        'positive_ratio': opportunity_count / total_sentences if total_sentences > 0 else 0.0,  # opportunity ratio\n",
    "        'negative_ratio': risk_count / total_sentences if total_sentences > 0 else 0.0,        # risk ratio\n",
    "        'neutral_ratio': neutral_count / total_sentences if total_sentences > 0 else 0.0,\n",
    "        'sentiment_confidence': np.mean(confidences) if confidences else 0.0,\n",
    "        'sentiment_volatility': np.std(scores) if len(scores) > 1 else 0.0,\n",
    "        'total_sentences': total_sentences,\n",
    "        'detailed_results': sentiment_results\n",
    "}\n",
    "\n",
    "# Step 4: Run Analysis on All Documents\n",
    "print(\"\\nProcessing documents for sentiment analysis...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "sentiment_analysis_results = {}\n",
    "\n",
    "for doc_name, doc in documents.items():\n",
    "    print(f\"Processing: {doc_name}\")\n",
    "    \n",
    "    # Get the found terms from your existing analysis\n",
    "    if doc_name in document_terms:\n",
    "        found_terms = document_terms[doc_name]\n",
    "        \n",
    "        # Extract sustainability sentences\n",
    "        sustainability_sentences = extract_sustainability_sentences_from_terms(doc, found_terms)\n",
    "        \n",
    "        # Analyze sentiment\n",
    "        sentiment_analysis = analyze_sentences_sentiment(sustainability_sentences, climate_sentiment_model)\n",
    "        \n",
    "        # Store results\n",
    "        sentiment_analysis_results[doc_name] = {\n",
    "            'sustainability_sentences_count': len(sustainability_sentences),\n",
    "            'climate_bert_sentiment': sentiment_analysis\n",
    "        }\n",
    "        \n",
    "        # Print summary (using opportunity ratio instead of positive)\n",
    "        avg_sentiment = sentiment_analysis['avg_sentiment_score']\n",
    "        sentence_count = len(sustainability_sentences)\n",
    "        opp_ratio = sentiment_analysis['positive_ratio']  # This is actually opportunity ratio\n",
    "        \n",
    "        print(f\"Avg Sentiment: {avg_sentiment:+.3f} | Sentences: {sentence_count} | Opportunity: {opp_ratio:.1%}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"No terms found for {doc_name}\")\n",
    "\n",
    "# Step 5: Create Results DataFrame\n",
    "print(f\"\\nCreating results summary...\")\n",
    "\n",
    "sentiment_data = []\n",
    "\n",
    "for doc_name, sentiment_data_item in sentiment_analysis_results.items():\n",
    "    # Extract organization and year\n",
    "    parts = doc_name.split('_')\n",
    "    year = parts[-1]\n",
    "    org_name = '_'.join(parts[:-1])\n",
    "    \n",
    "    sentiment_metrics = sentiment_data_item['climate_bert_sentiment']\n",
    "    \n",
    "    row = {\n",
    "        'organization': org_name,\n",
    "        'year': int(year),\n",
    "        'sustainability_sentences': sentiment_data_item['sustainability_sentences_count'],\n",
    "        'avg_sentiment_score': round(sentiment_metrics['avg_sentiment_score'], 4),\n",
    "        'opportunity_ratio': round(sentiment_metrics['positive_ratio'], 3),  # Actually opportunity ratio\n",
    "        'risk_ratio': round(sentiment_metrics['negative_ratio'], 3),         # Actually risk ratio\n",
    "        'neutral_ratio': round(sentiment_metrics['neutral_ratio'], 3),\n",
    "        'sentiment_confidence': round(sentiment_metrics['sentiment_confidence'], 3),\n",
    "        'sentiment_volatility': round(sentiment_metrics['sentiment_volatility'], 3),\n",
    "        'total_sentences_analyzed': sentiment_metrics['total_sentences']\n",
    "    }\n",
    "    \n",
    "    sentiment_data.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "climate_bert_sentiment_df = pd.DataFrame(sentiment_data)\n",
    "climate_bert_sentiment_df = climate_bert_sentiment_df.sort_values(['organization', 'year']).reset_index(drop=True)\n",
    "\n",
    "# Step 6: Save Results\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"data/NLP/Results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save to Excel\n",
    "excel_path = f\"{output_dir}/Overall_Sentiment_Analysis.xlsx\"\n",
    "climate_bert_sentiment_df.to_excel(excel_path, index=False, sheet_name='Document_Sentiment')\n",
    "\n",
    "print(f\"\\nClimate-BERT sentiment analysis complete!\")\n",
    "print(f\"Results saved to: {excel_path}\")\n",
    "print(f\"Analyzed {len(sentiment_analysis_results)} documents\")\n",
    "\n",
    "# Step 7: Display Summary Statistics\n",
    "print(f\"\\nSUMMARY STATISTICS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Average sentiment across all documents: {climate_bert_sentiment_df['avg_sentiment_score'].mean():+.3f}\")\n",
    "print(f\"Standard deviation: {climate_bert_sentiment_df['avg_sentiment_score'].std():.3f}\")\n",
    "\n",
    "# Top 5 most opportunity-focused (most positive sentiment)\n",
    "most_positive = climate_bert_sentiment_df.nlargest(5, 'avg_sentiment_score')\n",
    "print(f\"\\nTOP 5 MOST OPPORTUNITY-FOCUSED:\")\n",
    "for _, row in most_positive.iterrows():\n",
    "    print(f\"  {row['organization']} ({row['year']}): {row['avg_sentiment_score']:+.3f}\")\n",
    "\n",
    "# Top 5 most risk-focused (most negative sentiment)  \n",
    "most_negative = climate_bert_sentiment_df.nsmallest(5, 'avg_sentiment_score')\n",
    "print(f\"\\nTOP 5 MOST RISK-FOCUSED:\")\n",
    "for _, row in most_negative.iterrows():\n",
    "    print(f\"  {row['organization']} ({row['year']}): {row['avg_sentiment_score']:+.3f}\")\n",
    "\n",
    "# Display DataFrame\n",
    "print(f\"\\nCOMPLETE RESULTS:\")\n",
    "print(climate_bert_sentiment_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nDone! Your Climate-BERT sentiment analysis data is ready for integration with your communication score.\")\n",
    "print(f\"Note: Climate-BERT measures 'opportunity' (positive) vs 'risk' (negative) sentiment specific to climate/sustainability topics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951c6403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Define file path and output path\n",
    "output_path = \"data/NLP/Results/Overall_Sentiment_Analysis.xlsx\"\n",
    "\n",
    "# Save the DataFrame to Excel\n",
    "climate_bert_sentiment_df.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "# Load the workbook and sheet\n",
    "wb = load_workbook(output_path)\n",
    "ws = wb.active  # There's only one sheet since we saved just one DataFrame\n",
    "\n",
    "# Auto-adjust column widths based on the longest string in each column\n",
    "for col in ws.columns:\n",
    "    max_length = 0\n",
    "    col_letter = get_column_letter(col[0].column)\n",
    "    for cell in col:\n",
    "        if cell.value:\n",
    "            max_length = max(max_length, len(str(cell.value)))\n",
    "    ws.column_dimensions[col_letter].width = max_length + 3  # Add padding\n",
    "\n",
    "# Define grey fill for alternating rows\n",
    "grey_fill = PatternFill(start_color=\"D9D9D9\", end_color=\"D9D9D9\", fill_type=\"solid\")\n",
    "\n",
    "# Alternate row colors by company\n",
    "prev_company = None\n",
    "use_grey = False\n",
    "for row in range(2, ws.max_row + 1):\n",
    "    current_company = ws[f\"A{row}\"].value  # Column A has the company names\n",
    "    if current_company != prev_company:\n",
    "        use_grey = not use_grey\n",
    "        prev_company = current_company\n",
    "\n",
    "    if use_grey:\n",
    "        for col in range(1, ws.max_column + 1):\n",
    "            ws.cell(row=row, column=col).fill = grey_fill\n",
    "\n",
    "# Save the final cleaned and formatted workbook\n",
    "wb.save(output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839e72c3",
   "metadata": {},
   "source": [
    "### Sentiment per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5298ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# TOPIC-LEVEL WEIGHTED SENTIMENT ANALYSIS USING CLIMATE-BERT\n",
    "# This calculates average weighted sentiment per sustainability topic\n",
    "# Can be run independently of main sentiment analysis\n",
    "# ===================================================================\n",
    "\n",
    "print(\"Starting Topic-Level Weighted Sentiment Analysis\")\n",
    "print(\"Using Climate-BERT with weighted contribution method\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Step 1: Import required libraries and define topics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define the same GREEN_TOPICS structure used in your topic analysis\n",
    "GREEN_TOPICS = {\n",
    "    'renewable_energy': 'Renewable Energy',\n",
    "    'climate_emissions': 'Climate & Emissions', \n",
    "    'environmental_conservation': 'Environmental Conservation',\n",
    "    'energy_systems': 'Energy Systems & Efficiency',\n",
    "    'circular_economy': 'Circular Economy & Waste',\n",
    "    'sustainability_governance': 'Sustainability & Governance',\n",
    "    'green_finance': 'Green Finance & Investment'\n",
    "}\n",
    "\n",
    "# Step 2: Initialize Climate-BERT model\n",
    "def initialize_climate_bert_for_topics():\n",
    "    \"\"\"Initialize Climate-BERT model for topic sentiment analysis.\"\"\"\n",
    "    try:\n",
    "        print(\"Loading Climate-BERT model for topic analysis...\")\n",
    "        model_name = \"climatebert/distilroberta-base-climate-sentiment\"\n",
    "        \n",
    "        climate_sentiment = pipeline(\n",
    "            \"sentiment-analysis\", \n",
    "            model=model_name,\n",
    "            return_all_scores=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        print(\"Climate-BERT model loaded successfully!\")\n",
    "        return climate_sentiment\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Climate-BERT: {e}\")\n",
    "        print(\"Falling back to RoBERTa-base sentiment model...\")\n",
    "        \n",
    "        fallback_sentiment = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "            return_all_scores=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        print(\"Fallback model loaded successfully!\")\n",
    "        return fallback_sentiment\n",
    "\n",
    "# Initialize model\n",
    "topic_climate_model = initialize_climate_bert_for_topics()\n",
    "\n",
    "# Step 3: Extract sentences with topic mapping\n",
    "def extract_sentences_with_topic_mapping(doc, found_terms):\n",
    "    \"\"\"\n",
    "    Extract sentences and map which topics are mentioned in each sentence.\n",
    "    Returns list of sentences with topic term counts.\n",
    "    \"\"\"\n",
    "    sentence_topic_mapping = {}\n",
    "    \n",
    "    # Group terms by sentence\n",
    "    for term_info in found_terms:\n",
    "        sentence = term_info['sentence']\n",
    "        sentence_text = sentence.text.strip()\n",
    "        \n",
    "        # Skip very short sentences\n",
    "        if len(sentence_text) <= 20:\n",
    "            continue\n",
    "            \n",
    "        # Initialize sentence if not seen before\n",
    "        if sentence_text not in sentence_topic_mapping:\n",
    "            sentence_topic_mapping[sentence_text] = {\n",
    "                'sentence_obj': sentence,\n",
    "                'topic_counts': {topic: 0 for topic in GREEN_TOPICS.keys()},\n",
    "                'terms_found': []\n",
    "            }\n",
    "        \n",
    "        # Add term to this sentence's topic count\n",
    "        topic = term_info['topic']\n",
    "        sentence_topic_mapping[sentence_text]['topic_counts'][topic] += 1\n",
    "        sentence_topic_mapping[sentence_text]['terms_found'].append(term_info['term'])\n",
    "    \n",
    "    # Convert to list format with additional info\n",
    "    sentences_with_topics = []\n",
    "    for sentence_text, info in sentence_topic_mapping.items():\n",
    "        total_terms_in_sentence = sum(info['topic_counts'].values())\n",
    "        \n",
    "        # Only include sentences that have sustainability terms\n",
    "        if total_terms_in_sentence > 0:\n",
    "            sentences_with_topics.append({\n",
    "                'text': sentence_text,\n",
    "                'sentence_obj': info['sentence_obj'],\n",
    "                'topic_counts': info['topic_counts'],\n",
    "                'total_terms': total_terms_in_sentence,\n",
    "                'terms_found': info['terms_found']\n",
    "            })\n",
    "    \n",
    "    return sentences_with_topics\n",
    "\n",
    "# Step 4: Calculate weighted sentiment for each topic\n",
    "def calculate_weighted_topic_sentiment(sentence_info, climate_model):\n",
    "    \"\"\"\n",
    "    Calculate weighted sentiment contribution for each topic in a sentence.\n",
    "    \"\"\"\n",
    "    text = sentence_info['text']\n",
    "    topic_counts = sentence_info['topic_counts']\n",
    "    total_terms = sentence_info['total_terms']\n",
    "    \n",
    "    # Truncate if needed\n",
    "    if len(text) > 500:\n",
    "        text = text[:500] + \"...\"\n",
    "    \n",
    "    try:\n",
    "        # Get Climate-BERT sentiment for the sentence\n",
    "        result = climate_model(text)\n",
    "        scores_dict = {item['label']: item['score'] for item in result[0]}\n",
    "        \n",
    "        opportunity_score = scores_dict.get('opportunity', 0)\n",
    "        risk_score = scores_dict.get('risk', 0)\n",
    "        neutral_score = scores_dict.get('neutral', 0)\n",
    "        \n",
    "        # Calculate overall sentiment score for this sentence\n",
    "        sentence_sentiment = opportunity_score - risk_score\n",
    "        confidence = max(opportunity_score, risk_score, neutral_score)\n",
    "        \n",
    "        # Determine primary label\n",
    "        if opportunity_score > max(risk_score, neutral_score):\n",
    "            label = 'OPPORTUNITY'\n",
    "        elif risk_score > max(opportunity_score, neutral_score):\n",
    "            label = 'RISK'\n",
    "        else:\n",
    "            label = 'NEUTRAL'\n",
    "        \n",
    "        # Calculate weighted contribution to each topic\n",
    "        topic_sentiment_contributions = {}\n",
    "        for topic, count in topic_counts.items():\n",
    "            if count > 0:  # Only calculate for topics mentioned in this sentence\n",
    "                weight = count / total_terms  # Proportional weight\n",
    "                weighted_sentiment = sentence_sentiment * weight\n",
    "                topic_sentiment_contributions[topic] = {\n",
    "                    'weighted_sentiment': weighted_sentiment,\n",
    "                    'weight': weight,\n",
    "                    'term_count': count,\n",
    "                    'sentence_sentiment': sentence_sentiment,\n",
    "                    'sentence_label': label,\n",
    "                    'sentence_confidence': confidence\n",
    "                }\n",
    "            else:\n",
    "                topic_sentiment_contributions[topic] = {\n",
    "                    'weighted_sentiment': 0.0,\n",
    "                    'weight': 0.0,\n",
    "                    'term_count': 0,\n",
    "                    'sentence_sentiment': 0.0,\n",
    "                    'sentence_label': 'NONE',\n",
    "                    'sentence_confidence': 0.0\n",
    "                }\n",
    "        \n",
    "        return topic_sentiment_contributions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sentence: {e}\")\n",
    "        # Return zero contributions for all topics\n",
    "        return {topic: {\n",
    "            'weighted_sentiment': 0.0,\n",
    "            'weight': 0.0,\n",
    "            'term_count': 0,\n",
    "            'sentence_sentiment': 0.0,\n",
    "            'sentence_label': 'ERROR',\n",
    "            'sentence_confidence': 0.0\n",
    "        } for topic in GREEN_TOPICS.keys()}\n",
    "\n",
    "# Step 5: Analyze all documents for topic-level sentiment\n",
    "def analyze_topic_weighted_sentiment_all_docs(documents_dict, document_terms_dict, climate_model):\n",
    "    \"\"\"\n",
    "    Analyze weighted topic sentiment for all documents.\n",
    "    \"\"\"\n",
    "    print(\"\\nProcessing documents for topic-weighted sentiment...\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for doc_name, doc in documents_dict.items():\n",
    "        print(f\"Processing: {doc_name}\")\n",
    "        \n",
    "        if doc_name in document_terms_dict:\n",
    "            found_terms = document_terms_dict[doc_name]\n",
    "            \n",
    "            # Extract sentences with topic mapping\n",
    "            sentences_with_topics = extract_sentences_with_topic_mapping(doc, found_terms)\n",
    "            \n",
    "            print(f\"Found {len(sentences_with_topics)} sentences with sustainability terms\")\n",
    "            \n",
    "            # Initialize topic accumulators\n",
    "            topic_aggregations = {topic: {\n",
    "                'total_weighted_sentiment': 0.0,\n",
    "                'total_weight': 0.0,\n",
    "                'sentence_count': 0,\n",
    "                'term_count_total': 0,\n",
    "                'opportunity_sentences': 0,\n",
    "                'risk_sentences': 0,\n",
    "                'neutral_sentences': 0\n",
    "            } for topic in GREEN_TOPICS.keys()}\n",
    "            \n",
    "            # Process each sentence\n",
    "            for sentence_info in sentences_with_topics:\n",
    "                # Get weighted contributions for this sentence\n",
    "                topic_contributions = calculate_weighted_topic_sentiment(sentence_info, climate_model)\n",
    "                \n",
    "                # Aggregate contributions for each topic\n",
    "                for topic, contribution in topic_contributions.items():\n",
    "                    if contribution['weight'] > 0:  # Only aggregate topics mentioned in this sentence\n",
    "                        topic_aggregations[topic]['total_weighted_sentiment'] += contribution['weighted_sentiment']\n",
    "                        topic_aggregations[topic]['total_weight'] += contribution['weight']\n",
    "                        topic_aggregations[topic]['sentence_count'] += 1\n",
    "                        topic_aggregations[topic]['term_count_total'] += contribution['term_count']\n",
    "                        \n",
    "                        # Count sentence types\n",
    "                        if contribution['sentence_label'] == 'OPPORTUNITY':\n",
    "                            topic_aggregations[topic]['opportunity_sentences'] += 1\n",
    "                        elif contribution['sentence_label'] == 'RISK':\n",
    "                            topic_aggregations[topic]['risk_sentences'] += 1\n",
    "                        else:\n",
    "                            topic_aggregations[topic]['neutral_sentences'] += 1\n",
    "            \n",
    "            # Calculate average weighted sentiment per topic\n",
    "            topic_results = {}\n",
    "            for topic, aggregation in topic_aggregations.items():\n",
    "                if aggregation['sentence_count'] > 0:\n",
    "                    avg_weighted_sentiment = aggregation['total_weighted_sentiment'] / aggregation['sentence_count']\n",
    "                    avg_weight_per_sentence = aggregation['total_weight'] / aggregation['sentence_count']\n",
    "                    \n",
    "                    # Calculate ratios\n",
    "                    total_sentences = aggregation['sentence_count']\n",
    "                    opp_ratio = aggregation['opportunity_sentences'] / total_sentences\n",
    "                    risk_ratio = aggregation['risk_sentences'] / total_sentences  \n",
    "                    neutral_ratio = aggregation['neutral_sentences'] / total_sentences\n",
    "                else:\n",
    "                    avg_weighted_sentiment = 0.0\n",
    "                    avg_weight_per_sentence = 0.0\n",
    "                    opp_ratio = 0.0\n",
    "                    risk_ratio = 0.0\n",
    "                    neutral_ratio = 0.0\n",
    "                \n",
    "                topic_results[topic] = {\n",
    "                    'avg_weighted_sentiment': avg_weighted_sentiment,\n",
    "                    'avg_weight_per_sentence': avg_weight_per_sentence,\n",
    "                    'sentence_count': aggregation['sentence_count'],\n",
    "                    'total_terms': aggregation['term_count_total'],\n",
    "                    'opportunity_ratio': opp_ratio,\n",
    "                    'risk_ratio': risk_ratio,\n",
    "                    'neutral_ratio': neutral_ratio\n",
    "                }\n",
    "            \n",
    "            all_results[doc_name] = {\n",
    "                'total_sentences_analyzed': len(sentences_with_topics),\n",
    "                'topic_results': topic_results\n",
    "            }\n",
    "            \n",
    "            # Print summary for this document\n",
    "            topics_with_data = sum(1 for topic, results in topic_results.items() if results['sentence_count'] > 0)\n",
    "            print(f\"Topics with sentiment data: {topics_with_data}/{len(GREEN_TOPICS)}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"No terms found for {doc_name}\")\n",
    "            all_results[doc_name] = {\n",
    "                'total_sentences_analyzed': 0,\n",
    "                'topic_results': {topic: {\n",
    "                    'avg_weighted_sentiment': 0.0,\n",
    "                    'avg_weight_per_sentence': 0.0,\n",
    "                    'sentence_count': 0,\n",
    "                    'total_terms': 0,\n",
    "                    'opportunity_ratio': 0.0,\n",
    "                    'risk_ratio': 0.0,\n",
    "                    'neutral_ratio': 0.0\n",
    "                } for topic in GREEN_TOPICS.keys()}\n",
    "            }\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Step 6: Create DataFrame with topic sentiment results\n",
    "def create_topic_sentiment_dataframe(all_results):\n",
    "    \"\"\"\n",
    "    Create DataFrame with documents as rows and topic sentiment metrics as columns.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for doc_name, doc_results in all_results.items():\n",
    "        # Extract organization and year\n",
    "        parts = doc_name.split('_')\n",
    "        year = parts[-1]\n",
    "        org_name = '_'.join(parts[:-1])\n",
    "        \n",
    "        # Base row information\n",
    "        row = {\n",
    "            'organization': org_name,\n",
    "            'year': int(year),\n",
    "            'total_sentences_analyzed': doc_results['total_sentences_analyzed']\n",
    "        }\n",
    "        \n",
    "        # Add metrics for each topic\n",
    "        for topic, topic_results in doc_results['topic_results'].items():\n",
    "            topic_display_name = topic.replace('_', ' ').title()\n",
    "            \n",
    "            # Add columns for this topic\n",
    "            row[f'{topic}_avg_sentiment'] = round(topic_results['avg_weighted_sentiment'], 4)\n",
    "            row[f'{topic}_sentence_count'] = topic_results['sentence_count']\n",
    "            row[f'{topic}_total_terms'] = topic_results['total_terms']\n",
    "            row[f'{topic}_opportunity_ratio'] = round(topic_results['opportunity_ratio'], 3)\n",
    "            row[f'{topic}_risk_ratio'] = round(topic_results['risk_ratio'], 3)\n",
    "            row[f'{topic}_neutral_ratio'] = round(topic_results['neutral_ratio'], 3)\n",
    "        \n",
    "        data.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.sort_values(['organization', 'year']).reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Step 7: Run the complete analysis\n",
    "print(\"\\nRunning topic-weighted sentiment analysis...\")\n",
    "\n",
    "# Run analysis (assumes 'documents' and 'document_terms' variables exist)\n",
    "topic_sentiment_results = analyze_topic_weighted_sentiment_all_docs(\n",
    "    documents, \n",
    "    document_terms, \n",
    "    topic_climate_model\n",
    ")\n",
    "\n",
    "# Create DataFrame\n",
    "topic_sentiment_df = create_topic_sentiment_dataframe(topic_sentiment_results)\n",
    "\n",
    "# Step 8: Save results\n",
    "import os\n",
    "output_dir = \"data/NLP/Results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "excel_path = f\"{output_dir}/Topic_Weighted_Sentiment_Analysis.xlsx\"\n",
    "topic_sentiment_df.to_excel(excel_path, index=False, sheet_name='Topic_Sentiment')\n",
    "\n",
    "print(f\"\\nTopic-weighted sentiment analysis complete!\")\n",
    "print(f\"Results saved to: {excel_path}\")\n",
    "print(f\"Analyzed {len(topic_sentiment_results)} documents across {len(GREEN_TOPICS)} topics\")\n",
    "\n",
    "# Step 9: Display summary statistics\n",
    "print(f\"\\nTOPIC SENTIMENT SUMMARY:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate and display average sentiment per topic across all documents\n",
    "topic_columns = [col for col in topic_sentiment_df.columns if col.endswith('_avg_sentiment')]\n",
    "\n",
    "print(\"Average weighted sentiment by topic (across all documents):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for col in topic_columns:\n",
    "    topic_name = col.replace('_avg_sentiment', '').replace('_', ' ').title()\n",
    "    avg_sentiment = topic_sentiment_df[col].mean()\n",
    "    std_sentiment = topic_sentiment_df[col].std()\n",
    "    \n",
    "    # Count documents with data for this topic\n",
    "    docs_with_data = sum(1 for val in topic_sentiment_df[col] if val != 0.0)\n",
    "    \n",
    "    print(f\"{topic_name:<35}: {avg_sentiment:+.3f} (±{std_sentiment:.3f}) | {docs_with_data} docs\")\n",
    "\n",
    "# Find most positive and negative topics overall\n",
    "print(f\"\\nMOST OPPORTUNITY-FOCUSED TOPICS:\")\n",
    "topic_averages = [(col, topic_sentiment_df[col].mean()) for col in topic_columns]\n",
    "topic_averages.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for col, avg in topic_averages[:3]:\n",
    "    topic_name = col.replace('_avg_sentiment', '').replace('_', ' ').title()\n",
    "    print(f\"  {topic_name}: {avg:+.3f}\")\n",
    "\n",
    "print(f\"\\nMOST RISK-FOCUSED TOPICS:\")\n",
    "for col, avg in topic_averages[-3:]:\n",
    "    topic_name = col.replace('_avg_sentiment', '').replace('_', ' ').title()\n",
    "    print(f\"  {topic_name}: {avg:+.3f}\")\n",
    "\n",
    "print(f\"\\nDataFrame shape: {topic_sentiment_df.shape}\")\n",
    "print(f\"Columns: {len(topic_sentiment_df.columns)} total\")\n",
    "\n",
    "# Display first few rows for verification\n",
    "print(f\"\\nSAMPLE RESULTS (first 5 rows):\")\n",
    "print(topic_sentiment_df.head().to_string(index=False))\n",
    "\n",
    "print(f\"\\nTopic-weighted sentiment analysis complete!\")\n",
    "print(f\"Each topic's sentiment represents weighted average across sentences mentioning that topic.\")\n",
    "print(f\" Weighting based on proportion of topic terms in each sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b24b9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Define file path and output path\n",
    "output_path = \"data/NLP/Results/Topic_Weighted_Sentiment_Analysis.xlsx\"\n",
    "\n",
    "# Save the DataFrame to Excel\n",
    "topic_sentiment_df.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "# Load the workbook and sheet\n",
    "wb = load_workbook(output_path)\n",
    "ws = wb.active  # There's only one sheet since we saved just one DataFrame\n",
    "\n",
    "# Auto-adjust column widths based on the longest string in each column\n",
    "for col in ws.columns:\n",
    "    max_length = 0\n",
    "    col_letter = get_column_letter(col[0].column)\n",
    "    for cell in col:\n",
    "        if cell.value:\n",
    "            max_length = max(max_length, len(str(cell.value)))\n",
    "    ws.column_dimensions[col_letter].width = max_length + 3  # Add padding\n",
    "\n",
    "# Define grey fill for alternating rows\n",
    "grey_fill = PatternFill(start_color=\"D9D9D9\", end_color=\"D9D9D9\", fill_type=\"solid\")\n",
    "\n",
    "# Alternate row colors by company\n",
    "prev_company = None\n",
    "use_grey = False\n",
    "for row in range(2, ws.max_row + 1):\n",
    "    current_company = ws[f\"A{row}\"].value  # Column A has the company names\n",
    "    if current_company != prev_company:\n",
    "        use_grey = not use_grey\n",
    "        prev_company = current_company\n",
    "\n",
    "    if use_grey:\n",
    "        for col in range(1, ws.max_column + 1):\n",
    "            ws.cell(row=row, column=col).fill = grey_fill\n",
    "\n",
    "# Save the final cleaned and formatted workbook\n",
    "wb.save(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0620db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples of Climate-BERT sentiment analysis\n",
    "if sentiment_analysis_results:\n",
    "    print(f\"\\nCLIMATE-BERT SENTIMENT ANALYSIS EXAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Climate-BERT classifies sustainability sentences as OPPORTUNITY/RISK/NEUTRAL\")\n",
    "    print(\"Sentiment Score: Opportunity - Risk (ranges from -1 to +1)\")\n",
    "    print()\n",
    "    \n",
    "    # Get the document with most analyzed sentences\n",
    "    most_analyzed_doc = max(sentiment_analysis_results.items(), \n",
    "                           key=lambda x: x[1]['sustainability_sentences_count'])\n",
    "    doc_name = most_analyzed_doc[0]\n",
    "    doc_results = most_analyzed_doc[1]['climate_bert_sentiment']\n",
    "    \n",
    "    print(f\"EXAMPLES FROM: {doc_name}\")\n",
    "    print(f\"Total sustainability sentences analyzed: {doc_results['total_sentences']}\")\n",
    "    print(f\"Average sentiment score: {doc_results['avg_sentiment_score']:+.3f}\")\n",
    "    print(f\"Opportunity ratio: {doc_results['positive_ratio']:.1%}\")\n",
    "    print(f\"Risk ratio: {doc_results['negative_ratio']:.1%}\")\n",
    "    print(f\"Neutral ratio: {doc_results['neutral_ratio']:.1%}\")\n",
    "    print()\n",
    "    \n",
    "    # Show detailed examples if available\n",
    "    if 'detailed_results' in doc_results and doc_results['detailed_results']:\n",
    "        detailed_results = doc_results['detailed_results']\n",
    "        \n",
    "        # Get examples of each sentiment type\n",
    "        opportunity_examples = [r for r in detailed_results if r['sentiment_label'] == 'OPPORTUNITY']\n",
    "        risk_examples = [r for r in detailed_results if r['sentiment_label'] == 'RISK']\n",
    "        neutral_examples = [r for r in detailed_results if r['sentiment_label'] == 'NEUTRAL']\n",
    "        \n",
    "        # Show OPPORTUNITY examples\n",
    "        if opportunity_examples:\n",
    "            print(f\"OPPORTUNITY EXAMPLES ({len(opportunity_examples)} total):\")\n",
    "            for i, example in enumerate(opportunity_examples[:3], 1):\n",
    "                confidence = example['confidence']\n",
    "                sentiment_score = example['sentiment_score']\n",
    "                term_found = example['term_found']\n",
    "                topic = example['topic']\n",
    "                text = example['text']\n",
    "                # Truncate long sentences for display\n",
    "                display_text = text[:150] + \"...\" if len(text) > 150 else text\n",
    "                print(f\"{i}. Term: '{term_found}' | Topic: {topic}\")\n",
    "                print(f\"   Sentiment Score: {sentiment_score:+.3f} | Confidence: {confidence:.3f}\")\n",
    "                print(f\"   Text: {display_text}\")\n",
    "                print()\n",
    "        \n",
    "        # Show RISK examples  \n",
    "        if risk_examples:\n",
    "            print(f\"RISK EXAMPLES ({len(risk_examples)} total):\")\n",
    "            for i, example in enumerate(risk_examples[:3], 1):\n",
    "                confidence = example['confidence']\n",
    "                sentiment_score = example['sentiment_score']\n",
    "                term_found = example['term_found']\n",
    "                topic = example['topic']\n",
    "                text = example['text']\n",
    "                # Truncate long sentences for display\n",
    "                display_text = text[:150] + \"...\" if len(text) > 150 else text\n",
    "                print(f\"{i}. Term: '{term_found}' | Topic: {topic}\")\n",
    "                print(f\"   Sentiment Score: {sentiment_score:+.3f} | Confidence: {confidence:.3f}\")\n",
    "                print(f\"   Text: {display_text}\")\n",
    "                print()\n",
    "        \n",
    "        # Show NEUTRAL examples\n",
    "        if neutral_examples:\n",
    "            print(f\"NEUTRAL EXAMPLES ({len(neutral_examples)} total):\")\n",
    "            for i, example in enumerate(neutral_examples[:2], 1):\n",
    "                confidence = example['confidence']\n",
    "                sentiment_score = example['sentiment_score']\n",
    "                term_found = example['term_found']\n",
    "                topic = example['topic']\n",
    "                text = example['text']\n",
    "                # Truncate long sentences for display\n",
    "                display_text = text[:150] + \"...\" if len(text) > 150 else text\n",
    "                print(f\"{i}. Term: '{term_found}' | Topic: {topic}\")\n",
    "                print(f\"   Sentiment Score: {sentiment_score:+.3f} | Confidence: {confidence:.3f}\")\n",
    "                print(f\"   Text: {display_text}\")\n",
    "                print()\n",
    "    \n",
    "    else:\n",
    "        print(\"Detailed sentence examples not available in results\")\n",
    "        print(\"Note: To see individual sentence examples, ensure 'detailed_results' are stored in sentiment analysis\")\n",
    "\n",
    "print(f\"\\nClimate-BERT sentiment examples complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b69bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples of Topic-Weighted Climate-BERT sentiment analysis\n",
    "if 'topic_sentiment_results' in globals() and topic_sentiment_results:\n",
    "    print(f\"\\nTOPIC-WEIGHTED CLIMATE-BERT SENTIMENT EXAMPLES\")\n",
    "    print(\"=\" * 65)\n",
    "    print(\"Shows how climateBERT analyzes sentiment for specific sustainability topics\")\n",
    "    print(\"Weighted by topic term frequency within each sentence\")\n",
    "    print()\n",
    "    \n",
    "    # Get the document with most topic sentences analyzed\n",
    "    most_analyzed_doc = max(topic_sentiment_results.items(), \n",
    "                           key=lambda x: x[1]['total_sentences_analyzed'])\n",
    "    doc_name = most_analyzed_doc[0]\n",
    "    doc_results = most_analyzed_doc[1]\n",
    "    \n",
    "    print(f\"EXAMPLES FROM: {doc_name}\")\n",
    "    print(f\"Total sentences analyzed: {doc_results['total_sentences_analyzed']}\")\n",
    "    print()\n",
    "    \n",
    "    # Focus on the two main topics: renewable_energy and climate_emissions\n",
    "    topics_to_show = ['renewable_energy', 'climate_emissions']\n",
    "    topic_display_names = {\n",
    "        'renewable_energy': 'RENEWABLE ENERGY',\n",
    "        'climate_emissions': 'CLIMATE & EMISSIONS'\n",
    "    }\n",
    "    \n",
    "    for topic in topics_to_show:\n",
    "        if topic in doc_results['topic_results']:\n",
    "            topic_data = doc_results['topic_results'][topic]\n",
    "            \n",
    "            if topic_data['sentence_count'] > 0:\n",
    "                print(f\"{topic_display_names[topic]} TOPIC SENTIMENT:\")\n",
    "                print(f\"   Average Weighted Sentiment: {topic_data['avg_weighted_sentiment']:+.3f}\")\n",
    "                print(f\"   Sentences Analyzed: {topic_data['sentence_count']}\")\n",
    "                print(f\"   Terms Found: {topic_data['total_terms']}\")\n",
    "                print(f\"   Opportunity Ratio: {topic_data['opportunity_ratio']:.1%}\")\n",
    "                print(f\"   Risk Ratio: {topic_data['risk_ratio']:.1%}\")\n",
    "                print(f\"   Neutral Ratio: {topic_data['neutral_ratio']:.1%}\")\n",
    "                print()\n",
    "            else:\n",
    "                print(f\"{topic_display_names[topic]} TOPIC SENTIMENT:\")\n",
    "                print(f\"   No sentences found for this topic\")\n",
    "                print()\n",
    "    \n",
    "    print(\"NOTE: Topic-weighted sentiment calculation:\")\n",
    "    print(\"   • Each sentence gets analyzed by climateBERT for overall sentiment\")\n",
    "    print(\"   • Sentiment contribution is weighted by topic term frequency in sentence\")\n",
    "    print(\"   • Example: Sentence with 3 renewable terms + 1 emissions term:\")\n",
    "    print(\"     - Renewable energy gets 75% of sentence sentiment weight\")\n",
    "    print(\"     - Climate emissions gets 25% of sentence sentiment weight\")\n",
    "    print()\n",
    "    \n",
    "    # Show example of how weighting works if we have detailed results\n",
    "    print(\"TOPIC WEIGHTING EXAMPLE:\")\n",
    "    print(\"   Sentence: 'Our renewable energy projects reduce carbon emissions significantly'\")\n",
    "    print(\"   Terms found: renewable (1), energy (1), carbon (1), emissions (1)\")\n",
    "    print(\"   If climateBERT scores this as +0.8 OPPORTUNITY:\")\n",
    "    print(\"   • Renewable Energy gets: +0.8 × (2/4) = +0.4 weighted sentiment\") \n",
    "    print(\"   • Climate Emissions gets: +0.8 × (2/4) = +0.4 weighted sentiment\")\n",
    "    print()\n",
    "\n",
    "elif 'topic_sentiment_df' in globals() and len(topic_sentiment_df) > 0:\n",
    "    # If we only have the final DataFrame, show summary stats\n",
    "    print(f\"\\nTOPIC-WEIGHTED SENTIMENT SUMMARY\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Show average sentiment scores across all documents for key topics\n",
    "    renewable_avg = topic_sentiment_df['renewable_energy_avg_sentiment'].mean()\n",
    "    emissions_avg = topic_sentiment_df['climate_emissions_avg_sentiment'].mean()\n",
    "    \n",
    "    print(f\"AVERAGE TOPIC SENTIMENT ACROSS ALL DOCUMENTS:\")\n",
    "    print(f\"Renewable Energy: {renewable_avg:+.3f}\")\n",
    "    print(f\"Climate & Emissions: {emissions_avg:+.3f}\")\n",
    "    print()\n",
    "    print(\"Note: Run topic sentiment analysis with detailed results enabled\")\n",
    "    print(\"   to see individual sentence examples and weighting details\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\nTOPIC-WEIGHTED SENTIMENT ANALYSIS\")\n",
    "    print(\"=\" * 45)\n",
    "    print(\"Topic sentiment results not available\")\n",
    "    print(\"Run the topic-weighted sentiment analysis section first to see examples\")\n",
    "\n",
    "print(f\"\\nTopic-weighted sentiment examples complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
