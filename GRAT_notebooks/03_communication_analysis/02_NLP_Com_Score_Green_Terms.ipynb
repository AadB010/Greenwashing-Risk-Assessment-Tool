{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064f4df0",
   "metadata": {},
   "source": [
    "# Green Terms Communication Analysis\n",
    "\n",
    "## Overview\n",
    "This module identifies and quantifies environmental terminology usage in sustainability reports, supporting the Green Communication Intensity dimension. It detects green terms across multiple categories including basic nouns (\"sustainability\", \"renewable\"), context-dependent terms (\"reduced emissions\"), and multi-word phrases (\"low carbon economy\").\n",
    "\n",
    "## Term Detection Strategy  \n",
    "- **Systematic overlap resolution**: Longer terms processed first to prevent double-counting (\"renewable energy generation\" before \"renewable\")\n",
    "- **Negation filtering**: Excludes green terms in negative contexts (\"lack of renewable energy investments\")\n",
    "- **Context-dependent classification**: Neutral words only count when paired with positive modifiers (\"captured CO2\", \"improved efficiency\")\n",
    "- **POS-aware extraction**: Handles nouns, adjectives, verbs, adverbs, and multi-word combinations\n",
    "\n",
    "## Key Processing Steps\n",
    "1. **Term identification**: Matches lemmatized tokens against comprehensive green vocabulary lists\n",
    "2. **Overlap resolution**: Prevents double-counting through systematic precedence rules\n",
    "3. **Negation detection**: Uses dependency parsing to identify negated green terms\n",
    "4. **Frequency calculation**: (green words ÷ total content words) × 100, excluding stopwords and punctuation\n",
    "5. **Vocabulary diversity**: Tracks unique environmental terms relative to total unique words\n",
    "\n",
    "## Variables Produced for Communication Scoring\n",
    "According to the analysis framework:\n",
    "- **Green Term Frequency** → Green Communication Intensity dimension\n",
    "- **Vocabulary Diversity** → Green Communication Intensity dimension\n",
    "\n",
    "These variables measure environmental content coverage across climate, energy, and sustainability topics, providing quantitative assessment of how frequently companies discuss environmental themes relative to other content.\n",
    "\n",
    "## Validation Features\n",
    "Comprehensive term breakdown by POS type, negation analysis, and context verification ensure accurate green term identification across varied reporting styles and linguistic structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395af05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy_layout import spaCyLayout\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Load spaCy model and configure for large documents\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.max_length = 1_500_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2128fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Toggle between \"test\" and \"actual\"\n",
    "MODE = \"actual\"  \n",
    "\n",
    "# Define configuration based on mode\n",
    "if MODE == \"test\":\n",
    "    report_names = [ \n",
    "        \"Axpo_Holding_AG\", \"NEOEN_SA\"\n",
    "    ]\n",
    "    folders = {\n",
    "        \"2021\": Path(\"data/NLP/Testing/Reports/Clean/2021\"),\n",
    "        \"2022\": Path(\"data/NLP/Testing/Reports/Clean/2022\")\n",
    "    }\n",
    "\n",
    "elif MODE == \"actual\":\n",
    "    report_names = [ \n",
    "        \"Akenerji_Elektrik_Uretim_AS\",\n",
    "        \"Arendals_Fossekompani_ASA\",\n",
    "        \"Atlantica_Sustainable_Infrastructure_PLC\",\n",
    "        \"CEZ\",\n",
    "        \"EDF\",\n",
    "        \"EDP_Energias_de_Portugal_SA\",\n",
    "        \"Endesa\",\n",
    "        \"ERG_SpA\",\n",
    "        \"Orsted\",\n",
    "        \"Polska_Grupa_Energetyczna_PGE_SA\",\n",
    "        \"Romande_Energie_Holding_SA\",\n",
    "        \"Scatec_ASA\",\n",
    "        \"Solaria_Energia_y_Medio_Ambiente_SA\",\n",
    "        \"Terna_Energy_SA\"\n",
    "    ]\n",
    "\n",
    "    folders = {\n",
    "        \"2021\": Path(\"data/NLP/Reports/Cleanest/2021\"),\n",
    "        \"2022\": Path(\"data/NLP/Reports/Cleanest/2022\")\n",
    "    }\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid MODE. Use 'test' or 'actual'.\")\n",
    "\n",
    "# Check availability\n",
    "for name in report_names:\n",
    "    file_name = f\"{name}.txt\"\n",
    "    in_2021 = (folders[\"2021\"] / file_name).exists()\n",
    "    in_2022 = (folders[\"2022\"] / file_name).exists()\n",
    "    print(f\"{file_name}: 2021: {'YES' if in_2021 else 'NO'} | 2022: {'YES' if in_2022 else 'NO'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a05eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store processed docs\n",
    "documents = {}\n",
    "\n",
    "# Load and process all documents\n",
    "for version, folder_path in folders.items():\n",
    "    for name in report_names:\n",
    "        txt_path = folder_path / f\"{name}.txt\"\n",
    "        try:\n",
    "            with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            doc_key = f\"{name}_{version}\"\n",
    "            documents[doc_key] = nlp(text)\n",
    "            print(f\"Processed {doc_key}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {txt_path.name}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c41a0",
   "metadata": {},
   "source": [
    "## Green word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7833a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of green/sustainable nouns (lemmas) - EMISSIONS FOCUSED\n",
    "green_nouns = [\n",
    "    \"adaptation\", \"afforestation\", \"biodiversity\", \"biofuel\", \"biogas\", \"biomass\", \n",
    "    \"ccs\", \"ccus\", \"cogeneration\", \"decarbonisation\", \"decarbonization\", \"ecology\", \n",
    "    \"ecosystem\", \"electrification\", \"environment\", \"ess\", \"geothermal\", \"hydropower\", \n",
    "    \"improvement\", \"innovation\", \"mitigation\", \"optimization\", \"photovoltaic\", \n",
    "    \"preservation\", \"pv\", \"recycling\", \"reforestation\", \"regeneration\", \"renewable\", \n",
    "    \"renewables\", \"responsibility\", \"restoration\", \"solar\", \"sustainability\", \n",
    "    \"transition\", \"transparency\", \"wind\"\n",
    "]\n",
    "\n",
    "# Multi-word green nouns (lemmas) - EMISSIONS FOCUSED\n",
    "green_multiword_nouns = {\n",
    "    \"abatement\": [\"carbon\", \"co2\", \"co2e\", \"emission\", \"ghg\", \"pollution\"], \n",
    "    \"bond\": [\"climate\", \"green\", \"sustainability\"], \n",
    "    \"bonds\": [\"climate\", \"green\", \"sustainability\"], \n",
    "    \"capture\": [\"carbon\", \"co2\", \"ghg\", \"methane\"],\n",
    "    \"development\": [\"clean\", \"renewable\", \"sustainable\"],\n",
    "    \"economy\": [\"circular\", \"green\", \"hydrogen\", \"sustainable\"],\n",
    "    \"energy\": [\"alternative\", \"clean\", \"geothermal\", \"hydro\", \"renewable\", \"solar\", \"tidal\", \"wind\"],\n",
    "    \"farm\": [\"offshore\", \"solar\", \"wind\"],\n",
    "    \"farms\": [\"offshore\", \"solar\", \"wind\"],\n",
    "    \"finance\": [\"climate\", \"green\", \"sustainable\"],\n",
    "    \"financing\": [\"climate\", \"green\", \"sustainable\"],\n",
    "    \"fuel\": [\"alternative\", \"bio\", \"clean\", \"hydrogen\", \"synthetic\"],\n",
    "    \"fuels\": [\"alternative\", \"bio\", \"clean\", \"hydrogen\", \"synthetic\"], \n",
    "    \"fund\": [\"climate\", \"green\", \"sustainability\"], \n",
    "    \"funds\": [\"climate\", \"green\", \"sustainability\"], \n",
    "    \"generation\": [\"clean\", \"renewable\"],\n",
    "    \"goal\": [\"carbon\", \"climate\", \"emission\"], \n",
    "    \"goals\": [\"carbon\", \"climate\", \"emission\"],\n",
    "    \"growth\": [\"clean\", \"climate\", \"green\", \"renewable\", \"sustainable\"],\n",
    "    \"hydrogen\": [\"blue\", \"clean\", \"green\", \"renewable\"],\n",
    "    \"investment\": [\"clean\", \"climate\", \"green\", \"renewable\", \"sustainable\"],\n",
    "    \"investments\": [\"clean\", \"climate\", \"green\", \"renewable\", \"sustainable\"],\n",
    "    \"management\": [\"carbon\", \"energy\", \"environmental\", \"waste\"],\n",
    "    \"neutral\": [\"carbon\", \"climate\", \"co2\", \"emission\"],\n",
    "    \"neutrality\": [\"carbon\", \"climate\", \"co2\", \"emission\"],\n",
    "    \"panel\": [\"photovoltaic\", \"pv\", \"solar\"],\n",
    "    \"panels\": [\"photovoltaic\", \"pv\", \"solar\"],\n",
    "    \"plant\": [\"biomass\", \"geothermal\", \"hydro\", \"renewable\", \"solar\", \"wind\"],\n",
    "    \"plants\": [\"biomass\", \"geothermal\", \"hydro\", \"renewable\", \"solar\", \"wind\"],\n",
    "    \"power\": [\"clean\", \"geothermal\", \"hydro\", \"renewable\", \"solar\", \"tidal\", \"wind\"],\n",
    "    \"program\": [\"conservation\", \"efficiency\", \"renewable\", \"retrofit\"],\n",
    "    \"project\": [\"clean\", \"efficiency\", \"green\", \"renewable\"],\n",
    "    \"reduction\": [\"carbon\", \"co2\", \"emission\", \"ghg\", \"waste\"],\n",
    "    \"reductions\": [\"carbon\", \"co2\", \"emission\", \"ghg\", \"waste\"],\n",
    "    \"sequestration\": [\"carbon\", \"co2\", \"ghg\"], \n",
    "    \"solution\": [\"clean\", \"climate\", \"green\", \"renewable\"],\n",
    "    \"solutions\": [\"clean\", \"climate\", \"green\", \"renewable\"],\n",
    "    \"source\": [\"clean\", \"geothermal\", \"green\", \"hydro\", \"renewable\", \"solar\", \"wind\"], \n",
    "    \"sources\": [\"clean\", \"geothermal\", \"green\", \"hydro\", \"renewable\", \"solar\", \"wind\"],\n",
    "    \"standard\": [\"efficiency\", \"environmental\", \"green\", \"performance\"],\n",
    "    \"standards\": [\"efficiency\", \"environmental\", \"green\", \"performance\"],\n",
    "    \"station\": [\"clean\", \"geothermal\", \"green\", \"hydro\", \"renewable\", \"solar\", \"wind\"],\n",
    "    \"stations\": [\"clean\", \"geothermal\", \"green\", \"hydro\", \"renewable\", \"solar\", \"wind\"],\n",
    "    \"storage\": [\"carbon\", \"co2\"],\n",
    "    \"technology\": [\"carbon\", \"clean\", \"efficiency\", \"green\", \"renewable\"],\n",
    "    \"technologies\": [\"carbon\", \"clean\", \"efficiency\", \"green\", \"renewable\"],\n",
    "    \"transition\": [\"climate\", \"energy\", \"green\"],\n",
    "    \"turbine\": [\"offshore\", \"onshore\", \"wind\"],\n",
    "    \"turbines\": [\"offshore\", \"onshore\", \"wind\"],\n",
    "    \"zero\": [\"carbon\", \"climate\", \"emission\", \"footprint\", \"ghg\", \"net\", \"pollution\", \"waste\"]\n",
    "}\n",
    "\n",
    "# List of single-word green adjectives (lemmas) - EMISSIONS FOCUSED\n",
    "green_adjectives = [\n",
    "    \"circular\", \"clean\", \"decarbonise\", \"decarbonised\", \"decarbonising\", \"decarbonize\", \n",
    "    \"decarbonized\", \"decarbonizing\", \"durable\", \"ecological\", \"ecosystemic\", \"efficient\", \n",
    "    \"enriching\", \"environmental\", \"environmentally\", \"green\", \"hydroelectric\", \"innovative\",\n",
    "    \"optimal\", \"proenvironmental\", \"recover\", \"recoverable\", \"recovered\", \"recyclable\", \n",
    "    \"recycle\", \"recycled\", \"recycling\", \"reforested\", \"refurbish\", \"refurbished\", \n",
    "    \"regenerable\", \"regenerate\", \"regenerated\", \"renewable\", \"renewables\", \"responsible\", \n",
    "    \"restore\", \"restored\", \"reusable\", \"reuse\", \"sustainable\", \"sustainably\"\n",
    "]\n",
    "\n",
    "# Multi-word green adjectives (lemmas) - EMISSIONS FOCUSED\n",
    "green_multiword_adjectives = {\n",
    "    \"based\": [\"biomass\", \"nature\", \"plant\", \"renewable\"], \n",
    "    \"bio\": [\"energy\", \"fuel\", \"gas\", \"mass\"],\n",
    "    \"biomass\": [\"fired\", \"fueled\", \"powered\"],\n",
    "    \"carbon\": [\"captured\", \"capturing\", \"free\", \"low\", \"lower\", \"negative\", \"neutral\", \"non\", \"sequestered\", \"zero\"],\n",
    "    \"ccs\": [\"enabled\", \"equipped\", \"ready\"], \n",
    "    \"efficient\": [\"eco\", \"energy\", \"fuel\", \"high\", \"resource\"],\n",
    "    \"efficiency\": [\"eco\", \"energy\", \"fuel\", \"high\", \"resource\"],\n",
    "    \"electric\": [\"all\", \"geothermal\", \"hydro\", \"solar\", \"tidal\", \"wind\"],\n",
    "    \"emission\": [\"free\", \"low\", \"zero\"], \n",
    "    \"emissions\": [\"free\", \"low\", \"zero\"], \n",
    "    \"emitting\": [\"non\", \"zero\"],\n",
    "    \"energy\": [\"alternative\", \"clean\", \"efficient\", \"environmental\", \"renewable\", \"saved\"], \n",
    "    \"energies\": [\"alternative\", \"clean\", \"efficient\", \"environmental\", \"renewable\", \"saved\"],\n",
    "    \"environmental\": [\"certified\", \"energy\", \"management\", \"responsible\"],\n",
    "    \"free\": [\"carbon\", \"coal\", \"co2\", \"emission\", \"emissions\", \"fossil\", \"waste\"],\n",
    "    \"friendly\": [\"climate\", \"eco\", \"environment\", \"environmental\", \"planet\"], \n",
    "    \"friendlier\": [\"carbon\", \"climate\", \"eco\", \"environment\", \"environmental\", \"planet\"],\n",
    "    \"gas\": [\"bio\", \"renewable\"],\n",
    "    \"intensity\": [\"low\", \"reduced\"], \n",
    "    \"mitigating\": [\"key\"],\n",
    "    \"natural\": [\"protected\"],\n",
    "    \"negative\": [\"carbon\", \"co2\", \"emission\", \"emissions\"],\n",
    "    \"neutral\": [\"carbon\", \"climate\", \"co2\", \"emission\"],\n",
    "    \"oriented\": [\"climate\", \"ecosystem\", \"sustainability\"],\n",
    "    \"planting\": [\"forest\", \"tree\"],\n",
    "    \"pollutant\": [\"anti\", \"controlling\", \"low\", \"preventing\", \"reduced\", \"zero\"],\n",
    "    \"pollution\": [\"anti\", \"controlling\", \"low\", \"preventing\", \"reduced\", \"zero\"],\n",
    "    \"production\": [\"clean\", \"green\", \"renewable\", \"responsible\", \"sustainable\"],\n",
    "    \"proof\": [\"climate\", \"future\"],\n",
    "    \"protected\": [\"environmental\", \"natural\"],\n",
    "    \"reducing\": [\"carbon\", \"emission\", \"ghg\", \"pollution\"],\n",
    "    \"related\": [\"climate\", \"environment\", \"sustainability\"],\n",
    "    \"resilient\": [\"climate\", \"environment\"],\n",
    "    \"responsible\": [\"climate\", \"eco\", \"environmental\"],\n",
    "    \"saving\": [\"carbon\", \"energy\", \"fuel\", \"resource\"],\n",
    "    \"sustainable\": [\"certified\", \"climate\"],\n",
    "    \"zero\": [\"carbon\", \"emission\", \"net\"]\n",
    "}\n",
    "\n",
    "# List of green verbs (lemmas) - EMISSIONS FOCUSED\n",
    "green_verbs = [\n",
    "    \"afforeste\", \"afforesting\", \"conserve\", \"conserving\", \"decarbonize\", \"decarbonizing\", \n",
    "    \"decarbonise\", \"decarbonising\", \"electrify\", \"electrifying\", \"innovate\", \"innovating\",\n",
    "    \"minimize\", \"minimizing\", \"minimise\", \"minimising\", \"mitigate\", \"mitigating\", \n",
    "    \"optimize\", \"optimizing\", \"optimise\", \"optimising\", \"preserve\", \"preserving\", \n",
    "    \"recover\", \"recovering\", \"recycle\", \"recycling\", \"remediate\", \"remediating\", \n",
    "    \"reforest\", \"reforesting\", \"regenerate\", \"regenerating\", \"restore\", \"restoring\", \n",
    "    \"reuse\", \"reusing\", \"transition\", \"transitioning\", \"upgrade\", \"upgrading\"\n",
    "]\n",
    "\n",
    "# List of green adverbs (lemmas) - EMISSIONS FOCUSED\n",
    "green_adverbs = [\n",
    "    \"cleanly\", \"consciously\", \"ecologically\", \"efficiently\", \"environmentally\", \n",
    "    \"optimally\", \"renewably\", \"responsibly\", \"sustainably\"\n",
    "]\n",
    "\n",
    "# Multi-word green adverbs (lemmas) - EMISSIONS FOCUSED\n",
    "green_multiword_adverbs = {\n",
    "    \"aware\": [\"carbon\", \"climate\", \"eco\", \"environmentally\"],\n",
    "    \"based\": [\"nature\", \"plant\", \"renewable\", \"sustainability\"], \n",
    "    \"compliant\": [\"climate\", \"emission\", \"environmentally\"],\n",
    "    \"compatible\": [\"climate\", \"eco\", \"environmentally\"],\n",
    "    \"conscious\": [\"carbon\", \"climate\", \"eco\", \"environmentally\"],\n",
    "    \"designed\": [\"efficiently\", \"environmentally\", \"sustainably\"],\n",
    "    \"driven\": [\"climate\", \"renewable\", \"sustainability\"],\n",
    "    \"efficient\": [\"carbon\", \"energy\", \"fuel\", \"resource\"],\n",
    "    \"focused\": [\"climate\", \"emission\", \"environmental\", \"sustainability\"],\n",
    "    \"friendly\": [\"carbon\", \"climate\", \"co2\", \"eco\", \"environmentally\"],\n",
    "    \"managed\": [\"environmentally\", \"responsibly\", \"sustainably\"],\n",
    "    \"neutral\": [\"carbon\", \"climate\", \"emission\"],\n",
    "    \"operated\": [\"cleanly\", \"efficiently\", \"sustainably\"],\n",
    "    \"oriented\": [\"climate\", \"environment\", \"renewable\", \"sustainability\"],\n",
    "    \"produced\": [\"cleanly\", \"renewably\", \"sustainably\"],\n",
    "    \"responsible\": [\"carbon\", \"climate\", \"environmentally\"],\n",
    "    \"safe\": [\"climate\", \"eco\", \"environmentally\"],\n",
    "    \"sensitive\": [\"carbon\", \"climate\", \"environmentally\"],\n",
    "    \"sound\": [\"carbon\", \"climate\", \"environmentally\"]\n",
    "}\n",
    "\n",
    "# Neutral terms that become green with proper context (lemmas) - EMISSIONS FOCUSED\n",
    "neutral_nouns = [\n",
    "    \"co2\", \"co2e\", \"cooling\", \"emission\", \"emissions\", \"energy\", \"footprint\", \n",
    "    \"fuel\", \"ghg\", \"heating\", \"methane\", \"pollution\", \"transportation\", \"waste\"\n",
    "]\n",
    "\n",
    "# Multi-word neutral terms that become green with context (lemmas) - EMISSIONS FOCUSED\n",
    "neutral_multiword_nouns = {\n",
    "    \"consumption\": [\"coal\", \"electricity\", \"energy\", \"fuel\", \"gas\", \"oil\", \"power\"],\n",
    "    \"economy\": [\"carbon\"],\n",
    "    \"emission\": [\"annual\", \"baseline\", \"carbon\", \"co2\", \"co2e\", \"direct\", \"ghg\", \"indirect\", \"scope\", \"total\"],\n",
    "    \"emissions\": [\"annual\", \"baseline\", \"carbon\", \"co2\", \"co2e\", \"direct\", \"ghg\", \"indirect\", \"scope\", \"total\"],\n",
    "    \"footprint\": [\"carbon\", \"co2\", \"ecological\", \"emission\", \"environmental\", \"ghg\"],\n",
    "    \"impact\": [\"carbon\", \"climate\", \"ecological\", \"environmental\"],\n",
    "    \"intensity\": [\"carbon\", \"co2\", \"emission\", \"energy\", \"fuel\", \"ghg\"],\n",
    "    \"usage\": [\"electricity\", \"energy\", \"fuel\", \"power\", \"resource\"],\n",
    "    \"use\": [\"electricity\", \"energy\", \"fuel\", \"resource\"]\n",
    "}\n",
    "\n",
    "# Context words that indicate positive change - EMISSIONS FOCUSED\n",
    "improvement_adjectives = [\n",
    "    \"advanced\", \"best\", \"better\", \"boosted\", \"enhanced\", \"excellent\", \"exceptional\", \n",
    "    \"improved\", \"impressive\", \"optimized\", \"optimised\", \"outstanding\", \"positive\", \n",
    "    \"remarkable\", \"strengthened\", \"successful\", \"superior\", \"upgraded\"\n",
    "]\n",
    "\n",
    "# Improvement verbs that indicate positive change\n",
    "improvement_verbs = [\n",
    "    \"advance\", \"advancing\", \"boost\", \"boosting\", \"deliver\", \"delivering\", \"enhance\", \n",
    "    \"enhancing\", \"improve\", \"improving\", \"optimize\", \"optimizing\", \"optimise\", \n",
    "    \"optimising\", \"outperform\", \"outperforming\", \"strengthen\", \"strengthening\", \n",
    "    \"upgrade\", \"upgrading\"\n",
    "]\n",
    "# Improvement adverbs that indicate positive manner\n",
    "improvement_adverbs = [\n",
    "    \"effectively\", \"efficiently\", \"excellently\", \"meaningfully\", \"positively\", \"successfully\"\n",
    "]\n",
    "# Dependency-based green term patterns - EMISSIONS FOCUSED\n",
    "dependency_green_patterns = {\n",
    "    \"improvement_adjectives\": {\n",
    "        \"head_words\": [\"efficiency\", \"generation\", \"performance\", \"process\", \"solution\", \"system\", \"technology\"],\n",
    "        \"dependent_words\": [\"advanced\", \"better\", \"cleaner\", \"enhanced\", \"greener\", \"improved\", \"innovative\", \"optimized\"],\n",
    "        \"dependency_relations\": [\"amod\"],\n",
    "        \"description\": \"Improvement adjectives with green nouns\"\n",
    "    },\n",
    "    \"increase_positive\": {\n",
    "        \"head_words\": [\"boost\", \"develop\", \"enhance\", \"expand\", \"grow\", \"improve\", \"increase\", \"scale\"],\n",
    "        \"dependent_words\": [\"capture\", \"conservation\", \"efficiency\", \"recycling\", \"renewable\", \"sequestration\", \"sustainability\"],\n",
    "        \"dependency_relations\": [\"dobj\"],\n",
    "        \"description\": \"Positive action verbs with green objects\"\n",
    "    },\n",
    "    \"achieve_climate\": {\n",
    "        \"head_words\": [\"achieve\", \"attain\", \"deliver\", \"reach\", \"realize\"],\n",
    "        \"dependent_words\": [\"neutrality\", \"reduction\", \"transition\", \"zero\"],\n",
    "        \"dependency_relations\": [\"dobj\"],\n",
    "        \"description\": \"Achievement verbs with climate goals\"\n",
    "    },\n",
    "    \"transition_to\": {\n",
    "        \"head_words\": [\"change\", \"migrate\", \"move\", \"shift\", \"switch\", \"transition\"],\n",
    "        \"dependent_words\": [\"circular\", \"clean\", \"green\", \"renewable\", \"sustainable\", \"zero\"],\n",
    "        \"dependency_relations\": [\"prep\", \"pobj\"],\n",
    "        \"description\": \"Transition phrases\"\n",
    "    },\n",
    "    \"investment_in\": {\n",
    "        \"head_words\": [\"allocate\", \"finance\", \"funding\", \"invest\", \"investment\", \"spend\"],\n",
    "        \"dependent_words\": [\"carbon\", \"clean\", \"climate\", \"environmental\", \"green\", \"renewable\", \"sustainable\"],\n",
    "        \"dependency_relations\": [\"prep\", \"pobj\"],\n",
    "        \"description\": \"Investment in green areas\"\n",
    "    },\n",
    "    \"commitment_to\": {\n",
    "        \"head_words\": [\"commitment\", \"dedication\", \"pledge\", \"promise\"],\n",
    "        \"dependent_words\": [\"climate\", \"neutrality\", \"reduction\", \"sustainability\", \"zero\"],\n",
    "        \"dependency_relations\": [\"prep\", \"pobj\"],\n",
    "        \"description\": \"Commitments to climate goals\"\n",
    "    },\n",
    "    \"sustainable_adverbs\": {\n",
    "        \"head_words\": [\"develop\", \"generate\", \"grow\", \"manage\", \"manufacture\", \"operate\", \"produce\"],\n",
    "        \"dependent_words\": [\"cleanly\", \"efficiently\", \"environmentally\", \"responsibly\", \"sustainably\"],\n",
    "        \"dependency_relations\": [\"advmod\"],\n",
    "        \"description\": \"Sustainable manner of operations\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEGATION DETECTION SYSTEM - COMPREHENSIVE WORD LISTS\n",
    "\n",
    "# Direct negation words (alphabetically sorted)\n",
    "direct_negation_words = [\n",
    "    \"absence\", \"barely\", \"beneath\", \"below\", \"deficit\", \"devoid\", \"empty\", \"exempt\", \n",
    "    \"excluded\", \"failed\", \"gap\", \"hardly\", \"impossible\", \"inadequate\", \"insufficient\", \n",
    "    \"lacking\", \"minus\", \"never\", \"neither\", \"nil\", \"no\", \"nobody\", \"none\", \"nor\", \n",
    "    \"not\", \"nothing\", \"nowhere\", \"rarely\", \"scarcely\", \"seldom\", \"short\", \"shortfall\", \n",
    "    \"unable\", \"void\", \"without\", \"zero\"\n",
    "]\n",
    "\n",
    "# Negative descriptor words (alphabetically sorted)\n",
    "negative_descriptor_words = [\n",
    "    \"absence\", \"absent\", \"barrier\", \"blocks\", \"blocked\", \"cancel\", \"cancelled\", \n",
    "    \"cease\", \"ceased\", \"challenge\", \"concern\", \"constraint\", \"constraints\", \n",
    "    \"decline\", \"decrease\", \"deficit\", \"degrade\", \"degraded\", \"deteriorate\", \n",
    "    \"deteriorated\", \"difficulties\", \"difficulty\", \"drop\", \"downturn\", \"end\", \n",
    "    \"ended\", \"fail\", \"failed\", \"failure\", \"fall\", \"gap\", \"halt\", \"halted\", \n",
    "    \"harm\", \"hinders\", \"hindered\", \"impedes\", \"impeded\", \"inability\", \"incapable\", \n",
    "    \"inadequate\", \"ineffective\", \"inefficient\", \"insufficient\", \"issues\", \"lack\", \n",
    "    \"lacking\", \"lacks\", \"limited\", \"limiting\", \"loss\", \"missing\", \"obstacle\", \n",
    "    \"prevents\", \"prevented\", \"problem\", \"problems\", \"reduction\", \"reject\", \"rejected\", \n",
    "    \"refuse\", \"refused\", \"setback\", \"shortage\", \"shortfall\", \"shrinkage\", \"stop\", \n",
    "    \"stopped\", \"struggles\", \"struggling\", \"suspend\", \"suspended\", \"threat\", \n",
    "    \"unable\", \"unsuccessful\", \"weakening\", \"worse\", \"worsen\", \"worsening\"\n",
    "]\n",
    "\n",
    "# Phrasal negation patterns (alphabetically sorted)\n",
    "phrasal_negation_patterns = [\n",
    "    \"absence of\", \"anything but\", \"are no plans\", \"are not\", \"aren't\", \"can not\", \n",
    "    \"can't\", \"cannot\", \"cease\", \"could not\", \"couldn't\", \"devoid of\", \"did not\", \n",
    "    \"didn't\", \"do not\", \"does not\", \"doesn't\", \"don't\", \"exempt from\", \"failed to\", \n",
    "    \"failing to\", \"far from\", \"free from\", \"had not\", \"hadn't\", \"has not\", \"hasn't\", \n",
    "    \"have not\", \"haven't\", \"inability to\", \"incapable of\", \"inadequate to\", \n",
    "    \"insufficient to\", \"instead of\", \"is no plan\", \"is not\", \"isn't\", \"lack of\", \n",
    "    \"may not\", \"might not\", \"must not\", \"mustn't\", \"need not\", \"needn't\", \n",
    "    \"never\", \"no attempt to\", \"no chance to\", \"no effort to\", \"no intention to\", \n",
    "    \"no longer\", \"no means to\", \"no opportunity to\", \"no plans to\", \"no way to\", \n",
    "    \"not enough\", \"not yet\", \"other than\", \"rather than\", \"scarcity of\", \n",
    "    \"short of\", \"shortage of\", \"should not\", \"shouldn't\", \"too few\", \"too few to\", \n",
    "    \"too little\", \"too little to\", \"unable to\", \"was not\", \"wasn't\", \"were not\", \n",
    "    \"weren't\", \"will not\", \"without\", \"without any\", \"without proper\", \n",
    "    \"without sufficient\", \"without the\", \"won't\", \"would not\", \"wouldn't\"\n",
    "]\n",
    "\n",
    "# Negation prefixes (alphabetically sorted)\n",
    "negation_prefixes = [\n",
    "    \"anti\", \"counter\", \"de\", \"dis\", \"false\", \"fake\", \"il\", \"im\", \"in\", \"ir\", \n",
    "    \"mis\", \"non\", \"pseudo\", \"un\"\n",
    "]\n",
    "\n",
    "# Words that shouldn't cause negation in positive green contexts\n",
    "protected_green_context_words = [\n",
    "    \"abate\", \"curb\", \"cut\", \"declining\", \"decrease\", \"decreasing\", \"eliminate\", \n",
    "    \"fewer\", \"less\", \"low\", \"lower\", \"minimal\", \"reduce\", \"reduction\", \"slash\", \"zero\"\n",
    "]\n",
    "\n",
    "# Verbs that when negated should exclude green terms in their scope\n",
    "reduction_negative_verbs = [\n",
    "    \"abate\", \"abating\", \"control\", \"controlling\", \"curb\", \"curbing\", \"cut\", \"cutting\", \n",
    "    \"decrease\", \"decreasing\", \"eliminate\", \"eliminating\", \"limit\", \"limiting\", \n",
    "    \"lower\", \"lowering\", \"minimize\", \"minimizing\", \"minimise\", \"minimising\", \n",
    "    \"reduce\", \"reducing\", \"remove\", \"removing\", \"restrict\", \"restricting\", \n",
    "    \"slash\", \"slashing\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5afb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_context_for_neutral_term(doc, neutral_token_start, neutral_token_end, all_existing_terms):\n",
    "    \"\"\"\n",
    "    Find context words (negative/improvement) for a neutral term.\n",
    "    Returns: (context_found, context_word, context_type, context_relationship)\n",
    "    \"\"\"\n",
    "    # Get the main token of the neutral term\n",
    "    main_token = doc[neutral_token_start]\n",
    "    \n",
    "    # For multiword terms, find the syntactic head\n",
    "    if neutral_token_end > neutral_token_start:\n",
    "        tokens_in_term = [doc[i] for i in range(neutral_token_start, neutral_token_end + 1)]\n",
    "        for token in tokens_in_term:\n",
    "            if token.head not in tokens_in_term or token.head == token:\n",
    "                main_token = token\n",
    "                break\n",
    "    \n",
    "    # Apply distance filtering to head_subtree scope\n",
    "    head_subtree_all = list(main_token.head.subtree)\n",
    "    head_subtree_filtered = [\n",
    "        token for token in head_subtree_all \n",
    "        if (main_token.i - 8) <= token.i <= (main_token.i + 4)\n",
    "    ]\n",
    "    \n",
    "    # Define search scopes with restricted head_subtree\n",
    "    search_scopes = [\n",
    "        (\"subtree\", list(main_token.subtree)),\n",
    "        (\"ancestors\", list(main_token.ancestors)),\n",
    "        (\"head_subtree\", head_subtree_filtered)\n",
    "    ]\n",
    "    \n",
    "    # Collect context word lists\n",
    "    negative_context_words = (set(negative_descriptor_words) | \n",
    "                             set(protected_green_context_words) | \n",
    "                             set(reduction_negative_verbs))\n",
    "    \n",
    "    improvement_context_words = (set(improvement_adjectives) | \n",
    "                                set(improvement_verbs) | \n",
    "                                set(improvement_adverbs))\n",
    "    \n",
    "    # Search for context in each scope\n",
    "    for scope_name, scope_tokens in search_scopes:\n",
    "        for token in scope_tokens:\n",
    "            token_lemma = token.lemma_.lower()\n",
    "            \n",
    "            # Skip if this token is part of existing green terms\n",
    "            if is_word_part_of_green_terms(token, all_existing_terms):\n",
    "                continue\n",
    "            \n",
    "            # Check for negative context\n",
    "            if token_lemma in negative_context_words:\n",
    "                if is_context_related_to_term(token, main_token, scope_name):\n",
    "                    return True, token.text, \"negative\", token.dep_\n",
    "            \n",
    "            # Check for improvement context  \n",
    "            if token_lemma in improvement_context_words:\n",
    "                if is_context_related_to_term(token, main_token, scope_name):\n",
    "                    return True, token.text, \"improvement\", token.dep_\n",
    "    \n",
    "    return False, None, None, None\n",
    "\n",
    "def is_context_related_to_term(context_token, neutral_token, scope_name):\n",
    "    \"\"\"\n",
    "    Validate that context word is syntactically related to neutral term.\n",
    "    \"\"\"\n",
    "    # Direct dependency relationship\n",
    "    if context_token.head == neutral_token or neutral_token.head == context_token:\n",
    "        return True\n",
    "    \n",
    "    # Same head (siblings)\n",
    "    if context_token.head == neutral_token.head and scope_name == \"head_subtree\":\n",
    "        return True\n",
    "    \n",
    "    # Ancestor relationship\n",
    "    if scope_name == \"ancestors\":\n",
    "        neutral_ancestors = list(neutral_token.ancestors)\n",
    "        if context_token in neutral_ancestors[:3]:\n",
    "            return True\n",
    "    \n",
    "    # Subtree relationship\n",
    "    if scope_name == \"subtree\":\n",
    "        if context_token in list(neutral_token.subtree):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def find_context_dependent_terms(doc, excluded_positions, all_existing_terms):\n",
    "    \"\"\"\n",
    "    Find neutral terms that become green when paired with context words.\n",
    "    Returns: (found_terms, context_excluded_positions)\n",
    "    \"\"\"\n",
    "    found_terms = []\n",
    "    context_excluded_positions = set()\n",
    "    \n",
    "    # Find single neutral terms\n",
    "    for i, token in enumerate(doc):\n",
    "        if i in excluded_positions or i in context_excluded_positions:\n",
    "            continue\n",
    "        \n",
    "        lemma_lower = token.lemma_.lower()\n",
    "        if lemma_lower in neutral_nouns:\n",
    "            context_found, context_word, context_type, context_rel = find_context_for_neutral_term(\n",
    "                doc, i, i, all_existing_terms\n",
    "            )\n",
    "            \n",
    "            if context_found:\n",
    "                found_terms.append({\n",
    "                    'term': f\"{context_word} {token.text}\",\n",
    "                    'pos': f\"context_dependent_noun\",\n",
    "                    'start_idx': i,\n",
    "                    'end_idx': i,\n",
    "                    'sentence': token.sent,\n",
    "                    'neutral_part': token.text,\n",
    "                    'context_word': context_word,\n",
    "                    'context_type': context_type,\n",
    "                    'context_relationship': context_rel,\n",
    "                    'negated': False\n",
    "                })\n",
    "                context_excluded_positions.add(i)\n",
    "    \n",
    "    # Find multiword neutral terms\n",
    "    tokens = [token.lemma_.lower() for token in doc]\n",
    "    \n",
    "    for base_word, modifiers in neutral_multiword_nouns.items():\n",
    "        for modifier in modifiers:\n",
    "            # Pattern 1: modifier-base (e.g., \"carbon-footprint\")\n",
    "            pattern1 = f\"{modifier}-{base_word}\"\n",
    "            # Pattern 2: modifier base (e.g., \"carbon footprint\")\n",
    "            pattern2 = f\"{modifier} {base_word}\"\n",
    "            \n",
    "            # Search in original text for hyphenated version\n",
    "            text_lower = doc.text.lower()\n",
    "            for match in re.finditer(re.escape(pattern1), text_lower):\n",
    "                start_char = match.start()\n",
    "                end_char = match.end()\n",
    "                \n",
    "                # Find corresponding token positions\n",
    "                start_token_idx = None\n",
    "                end_token_idx = None\n",
    "                \n",
    "                for j, token in enumerate(doc):\n",
    "                    if token.idx <= start_char < token.idx + len(token.text):\n",
    "                        start_token_idx = j\n",
    "                    if token.idx < end_char <= token.idx + len(token.text):\n",
    "                        end_token_idx = j\n",
    "                        break\n",
    "                \n",
    "                if (start_token_idx is not None and end_token_idx is not None and\n",
    "                    start_token_idx not in excluded_positions and start_token_idx not in context_excluded_positions):\n",
    "                    \n",
    "                    context_found, context_word, context_type, context_rel = find_context_for_neutral_term(\n",
    "                        doc, start_token_idx, end_token_idx, all_existing_terms\n",
    "                    )\n",
    "                    \n",
    "                    if context_found:\n",
    "                        found_terms.append({\n",
    "                            'term': f\"{context_word} {pattern1}\",\n",
    "                            'pos': f\"context_dependent_multiword_noun\",\n",
    "                            'start_idx': start_token_idx,\n",
    "                            'end_idx': end_token_idx,\n",
    "                            'sentence': doc[start_token_idx].sent,\n",
    "                            'neutral_part': pattern1,\n",
    "                            'context_word': context_word,\n",
    "                            'context_type': context_type,\n",
    "                            'context_relationship': context_rel,\n",
    "                            'negated': False\n",
    "                        })\n",
    "                        for idx in range(start_token_idx, end_token_idx + 1):\n",
    "                            context_excluded_positions.add(idx)\n",
    "            \n",
    "            # Search for space-separated version\n",
    "            for j in range(len(tokens) - 1):\n",
    "                if (tokens[j] == modifier and tokens[j + 1] == base_word and\n",
    "                    j not in excluded_positions and j not in context_excluded_positions):\n",
    "                    \n",
    "                    context_found, context_word, context_type, context_rel = find_context_for_neutral_term(\n",
    "                        doc, j, j + 1, all_existing_terms\n",
    "                    )\n",
    "                    \n",
    "                    if context_found:\n",
    "                        found_terms.append({\n",
    "                            'term': f\"{context_word} {pattern2}\",\n",
    "                            'pos': f\"context_dependent_multiword_noun\",\n",
    "                            'start_idx': j,\n",
    "                            'end_idx': j + 1,\n",
    "                            'sentence': doc[j].sent,\n",
    "                            'neutral_part': pattern2,\n",
    "                            'context_word': context_word,\n",
    "                            'context_type': context_type,\n",
    "                            'context_relationship': context_rel,\n",
    "                            'negated': False\n",
    "                        })\n",
    "                        context_excluded_positions.add(j)\n",
    "                        context_excluded_positions.add(j + 1)\n",
    "    \n",
    "    return found_terms, context_excluded_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29005e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_word_part_of_green_terms(token, all_found_green_terms):\n",
    "    \"\"\"Check if a token is part of any existing green term.\"\"\"\n",
    "    for green_term in all_found_green_terms:\n",
    "        green_term_span = range(green_term['start_idx'], green_term['end_idx'] + 1)\n",
    "        if token.i in green_term_span:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def was_used_in_green_pattern(token, all_found_green_terms):\n",
    "    \"\"\"Check if this token was used to construct any green dependency pattern.\"\"\"\n",
    "    for green_term in all_found_green_terms:\n",
    "        if green_term['pos'].startswith('dependency_'):\n",
    "            green_term_span = range(green_term['start_idx'], green_term['end_idx'] + 1)\n",
    "            if token.i in green_term_span:\n",
    "                if token.lemma_.lower() in protected_green_context_words:\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def is_negation_related_to_term(negation_token, green_term_token, all_found_green_terms):\n",
    "    \"\"\"\n",
    "    Determine if a negation word is actually negating the green term.\n",
    "    \"\"\"\n",
    "    # Check if this negation word is part of any existing green term\n",
    "    if is_word_part_of_green_terms(negation_token, all_found_green_terms):\n",
    "        return False\n",
    "    \n",
    "    # Check if this negation word was used to construct any green pattern\n",
    "    if was_used_in_green_pattern(negation_token, all_found_green_terms):\n",
    "        return False\n",
    "    \n",
    "    # Check syntactic relationship using dependency parsing\n",
    "    if negation_token.head == green_term_token or green_term_token.head == negation_token:\n",
    "        return True\n",
    "    \n",
    "    # Check if they're in the same sentence and have close syntactic relationship\n",
    "    if negation_token.sent == green_term_token.sent:\n",
    "        negation_ancestors = [negation_token] + list(negation_token.ancestors)\n",
    "        green_ancestors = [green_term_token] + list(green_term_token.ancestors)\n",
    "        \n",
    "        # If they share close ancestors, they're likely related\n",
    "        for neg_ancestor in negation_ancestors[:3]:\n",
    "            if neg_ancestor in green_ancestors[:3]:\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def validate_phrasal_negation(sentence, phrase, green_term_token):\n",
    "    \"\"\"\n",
    "    Validate that a phrasal negation pattern actually applies to the green term.\n",
    "    \"\"\"\n",
    "    sentence_text = sentence.text.lower()\n",
    "    phrase_start = sentence_text.find(phrase)\n",
    "    \n",
    "    if phrase_start == -1:\n",
    "        return False\n",
    "    \n",
    "    # Calculate character positions\n",
    "    green_term_char_start = green_term_token.idx\n",
    "    green_term_char_end = green_term_token.idx + len(green_term_token.text)\n",
    "    \n",
    "    # Convert to sentence-relative positions\n",
    "    sentence_char_start = sentence.start_char\n",
    "    relative_green_start = green_term_char_start - sentence_char_start\n",
    "    relative_green_end = green_term_char_end - sentence_char_start\n",
    "    \n",
    "    phrase_end = phrase_start + len(phrase)\n",
    "    \n",
    "    # Check if the green term appears reasonably close after the negation phrase\n",
    "    if relative_green_start > phrase_end and relative_green_start - phrase_end < 50:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def find_negated_reduction_verb(main_token, all_found_green_terms):\n",
    "    \"\"\"\n",
    "    Check if the green term is the object/target of a negated reduction verb.\n",
    "    Returns: (is_negated, negation_type, negation_text, scope_found)\n",
    "    \"\"\"\n",
    "    # Get all context words that are already being used to create green terms\n",
    "    used_context_words = set()\n",
    "    for green_term in all_found_green_terms:\n",
    "        if 'context_word' in green_term:\n",
    "            context_word = green_term['context_word'].lower()\n",
    "            used_context_words.add(context_word)\n",
    "            used_context_words.add(context_word.rstrip('ed').rstrip('ing').rstrip('s'))\n",
    "    \n",
    "    # Get the context word for this specific green term (if it's context-dependent)\n",
    "    this_term_context_word_lemma = None\n",
    "    for green_term in all_found_green_terms:\n",
    "        term_span = range(green_term['start_idx'], green_term['end_idx'] + 1)\n",
    "        if main_token.i in term_span and 'context_word' in green_term:\n",
    "            context_word = green_term['context_word']\n",
    "            context_doc = nlp(context_word)\n",
    "            if len(context_doc) > 0:\n",
    "                this_term_context_word_lemma = context_doc[0].lemma_.lower()\n",
    "            else:\n",
    "                this_term_context_word_lemma = context_word.lower()\n",
    "            break\n",
    "    \n",
    "    # Check if this green term is the object of a reduction verb\n",
    "    for ancestor in main_token.ancestors:\n",
    "        if ancestor.lemma_.lower() in reduction_negative_verbs:\n",
    "            # Apply distance filtering to verb subtree and head_subtree scopes\n",
    "            verb_subtree_all = list(ancestor.subtree)\n",
    "            verb_subtree_filtered = [\n",
    "                token for token in verb_subtree_all \n",
    "                if (ancestor.i - 5) <= token.i < ancestor.i\n",
    "            ]\n",
    "            \n",
    "            verb_head_subtree_all = list(ancestor.head.subtree)\n",
    "            verb_head_subtree_filtered = [\n",
    "                token for token in verb_head_subtree_all \n",
    "                if (ancestor.i - 5) <= token.i < ancestor.i\n",
    "            ]\n",
    "            \n",
    "            # Check the verb's subtree, ancestors, and head.subtree for negation\n",
    "            verb_scopes = [\n",
    "                (\"verb_subtree\", verb_subtree_filtered),\n",
    "                (\"verb_ancestors\", list(ancestor.ancestors)),\n",
    "                (\"verb_head_subtree\", verb_head_subtree_filtered)\n",
    "            ]\n",
    "            \n",
    "            for scope_name, scope_tokens in verb_scopes:\n",
    "                for token in scope_tokens:\n",
    "                    if (token.dep_ == \"neg\" or \n",
    "                        token.lemma_.lower() in direct_negation_words):\n",
    "                        if not is_word_part_of_green_terms(token, all_found_green_terms):\n",
    "                            if token.lemma_.lower() not in used_context_words:\n",
    "                                if this_term_context_word_lemma is None or token.lemma_.lower() != this_term_context_word_lemma:\n",
    "                                    return True, f\"negated_reduction_verb_{scope_name}\", f\"'{token.text}' negating '{ancestor.text}'\", scope_name\n",
    "    \n",
    "    # Also check direct dependency relationships where green term is object\n",
    "    if main_token.dep_ in [\"dobj\", \"pobj\", \"attr\"]:\n",
    "        head_verb = main_token.head\n",
    "        if head_verb.lemma_.lower() in reduction_negative_verbs:\n",
    "            # Apply distance filtering to direct verb scopes\n",
    "            direct_verb_subtree_all = list(head_verb.subtree)\n",
    "            direct_verb_subtree_filtered = [\n",
    "                token for token in direct_verb_subtree_all \n",
    "                if (head_verb.i - 5) <= token.i < head_verb.i\n",
    "            ]\n",
    "            \n",
    "            direct_verb_head_subtree_all = list(head_verb.head.subtree)\n",
    "            direct_verb_head_subtree_filtered = [\n",
    "                token for token in direct_verb_head_subtree_all \n",
    "                if (head_verb.i - 5) <= token.i < head_verb.i\n",
    "            ]\n",
    "            \n",
    "            # Check if this verb is negated\n",
    "            verb_scopes = [\n",
    "                (\"direct_verb_subtree\", direct_verb_subtree_filtered),\n",
    "                (\"direct_verb_ancestors\", list(head_verb.ancestors)),\n",
    "                (\"direct_verb_head_subtree\", direct_verb_head_subtree_filtered)\n",
    "            ]\n",
    "            \n",
    "            for scope_name, scope_tokens in verb_scopes:\n",
    "                for token in scope_tokens:\n",
    "                    if (token.dep_ == \"neg\" or \n",
    "                        token.lemma_.lower() in direct_negation_words):\n",
    "                        if not is_word_part_of_green_terms(token, all_found_green_terms):\n",
    "                            if token.lemma_.lower() not in used_context_words:\n",
    "                                if this_term_context_word_lemma is None or token.lemma_.lower() != this_term_context_word_lemma:\n",
    "                                    return True, f\"negated_reduction_verb_{scope_name}\", f\"'{token.text}' negating '{head_verb.text}'\", scope_name\n",
    "    \n",
    "    return False, None, None, None\n",
    "\n",
    "def find_negation_in_multiple_scopes(main_token, all_found_green_terms):\n",
    "    \"\"\"\n",
    "    Enhanced negation detection that checks subtree, ancestors, and head.subtree.\n",
    "    Returns: (is_negated, negation_type, negation_text, scope_found)\n",
    "    \"\"\"\n",
    "    # Get all context words that are already being used to create green terms\n",
    "    used_context_words = set()\n",
    "    for green_term in all_found_green_terms:\n",
    "        if 'context_word' in green_term:\n",
    "            context_word = green_term['context_word'].lower()\n",
    "            used_context_words.add(context_word)\n",
    "            used_context_words.add(context_word.rstrip('ed').rstrip('ing').rstrip('s'))\n",
    "    \n",
    "    # Get the context word for this specific green term (if it's context-dependent)\n",
    "    this_term_context_word_lemma = None\n",
    "    for green_term in all_found_green_terms:\n",
    "        term_span = range(green_term['start_idx'], green_term['end_idx'] + 1)\n",
    "        if main_token.i in term_span and 'context_word' in green_term:\n",
    "            context_word = green_term['context_word']\n",
    "            context_doc = nlp(context_word)\n",
    "            if len(context_doc) > 0:\n",
    "                this_term_context_word_lemma = context_doc[0].lemma_.lower()\n",
    "            else:\n",
    "                this_term_context_word_lemma = context_word.lower()\n",
    "            break\n",
    "    \n",
    "    # Apply distance filtering to subtree and head_subtree scopes\n",
    "    subtree_all = list(main_token.subtree)\n",
    "    subtree_filtered = [\n",
    "        token for token in subtree_all \n",
    "        if (main_token.i - 6) <= token.i <= (main_token.i + 3)\n",
    "    ]\n",
    "    \n",
    "    head_subtree_all = list(main_token.head.subtree)\n",
    "    head_subtree_filtered = [\n",
    "        token for token in head_subtree_all \n",
    "        if (main_token.i - 6) <= token.i <= (main_token.i + 3)\n",
    "    ]\n",
    "    \n",
    "    # Define the three scopes to check\n",
    "    scopes = [\n",
    "        (\"subtree\", subtree_filtered),\n",
    "        (\"ancestors\", list(main_token.ancestors)),\n",
    "        (\"head_subtree\", head_subtree_filtered)\n",
    "    ]\n",
    "    \n",
    "    # Method 1: Check spaCy's built-in negation detection in all scopes\n",
    "    for scope_name, scope_tokens in scopes:\n",
    "        for token in scope_tokens:\n",
    "            if token.dep_ == \"neg\":\n",
    "                if not is_word_part_of_green_terms(token, all_found_green_terms):\n",
    "                    if token.lemma_.lower() not in used_context_words:\n",
    "                        if this_term_context_word_lemma is None or token.lemma_.lower() != this_term_context_word_lemma:\n",
    "                            return True, \"spacy_neg\", token.text, scope_name\n",
    "    \n",
    "    # Method 2: Check for direct negation words in all scopes\n",
    "    for scope_name, scope_tokens in scopes:\n",
    "        for token in scope_tokens:\n",
    "            if token.lemma_.lower() in direct_negation_words:\n",
    "                if (not is_word_part_of_green_terms(token, all_found_green_terms) and\n",
    "                    is_negation_related_to_term(token, main_token, all_found_green_terms)):\n",
    "                    if token.lemma_.lower() not in used_context_words:\n",
    "                        if this_term_context_word_lemma is None or token.lemma_.lower() != this_term_context_word_lemma:\n",
    "                            return True, \"direct_negation\", token.text, scope_name\n",
    "    \n",
    "    # Method 3: Check for negative descriptor words in all scopes\n",
    "    for scope_name, scope_tokens in scopes:\n",
    "        for token in scope_tokens:\n",
    "            if token.lemma_.lower() in negative_descriptor_words:\n",
    "                if (not is_word_part_of_green_terms(token, all_found_green_terms) and\n",
    "                    not was_used_in_green_pattern(token, all_found_green_terms) and\n",
    "                    is_negation_related_to_term(token, main_token, all_found_green_terms)):\n",
    "                    if token.lemma_.lower() not in used_context_words:\n",
    "                        if this_term_context_word_lemma is None or token.lemma_.lower() != this_term_context_word_lemma:\n",
    "                            return True, \"negative_descriptor\", token.text, scope_name\n",
    "    \n",
    "    return False, None, None, None\n",
    "\n",
    "def detect_negation_for_term(doc, green_term_start_idx, green_term_end_idx, all_found_green_terms):\n",
    "    \"\"\"\n",
    "    Comprehensive negation detection for a green term using multiple scopes and reduction verbs.\n",
    "    Returns: (is_negated, negation_type, negation_text, scope_found)\n",
    "    \"\"\"\n",
    "    # Get the main token of the green term (head token for multiword terms)\n",
    "    main_token = doc[green_term_start_idx]\n",
    "    \n",
    "    # For multiword terms, try to find the head token\n",
    "    if green_term_end_idx > green_term_start_idx:\n",
    "        tokens_in_term = [doc[i] for i in range(green_term_start_idx, green_term_end_idx + 1)]\n",
    "        for token in tokens_in_term:\n",
    "            if token.head not in tokens_in_term or token.head == token:\n",
    "                main_token = token\n",
    "                break\n",
    "    \n",
    "    # Check for negated reduction verbs first\n",
    "    is_negated, negation_type, negation_text, scope = find_negated_reduction_verb(main_token, all_found_green_terms)\n",
    "    if is_negated:\n",
    "        return True, negation_type, negation_text, scope\n",
    "    \n",
    "    # Enhanced method: check subtree, ancestors, and head.subtree\n",
    "    is_negated, negation_type, negation_text, scope = find_negation_in_multiple_scopes(main_token, all_found_green_terms)\n",
    "    if is_negated:\n",
    "        return True, f\"{negation_type}_{scope}\", negation_text, scope\n",
    "    \n",
    "    # Check for prefix negation on all tokens in the term\n",
    "    for token_idx in range(green_term_start_idx, green_term_end_idx + 1):\n",
    "        token = doc[token_idx]\n",
    "        \n",
    "        if token.i >= 2:\n",
    "            prev_token = doc[token.i - 1]\n",
    "            if prev_token.text == \"-\":\n",
    "                prev_prev_token = doc[token.i - 2]\n",
    "                if prev_prev_token.text.lower() in negation_prefixes:\n",
    "                    return True, \"prefix_negation\", f\"{prev_prev_token.text}-\", \"prefix\"\n",
    "    \n",
    "    # Check for phrasal negation patterns\n",
    "    sentence = main_token.sent\n",
    "    sentence_text = sentence.text.lower()\n",
    "    for phrase in phrasal_negation_patterns:\n",
    "        if phrase in sentence_text:\n",
    "            if validate_phrasal_negation(sentence, phrase, main_token):\n",
    "                return True, \"phrasal_negation\", phrase, \"sentence\"\n",
    "    \n",
    "    return False, None, None, None\n",
    "\n",
    "def filter_negated_terms(all_found_terms, doc):\n",
    "    \"\"\"\n",
    "    Filter out negated terms and return both valid and negated term lists.\n",
    "    \"\"\"\n",
    "    valid_terms = []\n",
    "    negated_terms = []\n",
    "    \n",
    "    for term_info in all_found_terms:\n",
    "        is_negated, negation_type, negation_text, scope = detect_negation_for_term(\n",
    "            doc, \n",
    "            term_info['start_idx'], \n",
    "            term_info['end_idx'], \n",
    "            all_found_terms\n",
    "        )\n",
    "        \n",
    "        if is_negated:\n",
    "            term_info['negated'] = True\n",
    "            term_info['negation_type'] = negation_type\n",
    "            term_info['negation_text'] = negation_text\n",
    "            term_info['negation_scope'] = scope\n",
    "            negated_terms.append(term_info)\n",
    "        else:\n",
    "            term_info['negated'] = False\n",
    "            valid_terms.append(term_info)\n",
    "    \n",
    "    return valid_terms, negated_terms\n",
    "\n",
    "def get_negation_statistics(negated_terms):\n",
    "    \"\"\"\n",
    "    Generate statistics about negation patterns including scope and reduction verb information.\n",
    "    \"\"\"\n",
    "    if not negated_terms:\n",
    "        return {}\n",
    "    \n",
    "    negation_types = Counter()\n",
    "    negated_by_pos = Counter()\n",
    "    negation_scopes = Counter()\n",
    "    reduction_verb_negations = Counter()\n",
    "    \n",
    "    for term in negated_terms:\n",
    "        negation_types[term['negation_type']] += 1\n",
    "        negated_by_pos[term['pos']] += 1\n",
    "        if 'negation_scope' in term:\n",
    "            negation_scopes[term['negation_scope']] += 1\n",
    "        if 'reduction_verb' in term.get('negation_type', ''):\n",
    "            reduction_verb_negations[term['negation_type']] += 1\n",
    "    \n",
    "    return {\n",
    "        'total_negated': len(negated_terms),\n",
    "        'by_type': dict(negation_types),\n",
    "        'by_pos': dict(negated_by_pos),\n",
    "        'by_scope': dict(negation_scopes),\n",
    "        'reduction_verb_negations': dict(reduction_verb_negations),\n",
    "        'examples': [(term['term'], term['negation_type'], term['negation_text'], \n",
    "                     term.get('negation_scope', 'unknown')) \n",
    "                    for term in negated_terms[:8]]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b33a98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_multiword_terms(doc, multiword_dict, pos_tag):\n",
    "    \"\"\"Find multiword terms in document and return positions to exclude from single word counting.\"\"\"\n",
    "    found_terms = []\n",
    "    excluded_positions = set()\n",
    "    \n",
    "    # Convert doc to lowercase tokens for matching\n",
    "    tokens = [token.lemma_.lower() for token in doc]\n",
    "    \n",
    "    for base_word, modifiers in multiword_dict.items():\n",
    "        for modifier in modifiers:\n",
    "            # Pattern 1: modifier-base (e.g., \"eco-friendly\")\n",
    "            pattern1 = f\"{modifier}-{base_word}\"\n",
    "            # Pattern 2: modifier base (e.g., \"eco friendly\") \n",
    "            pattern2 = f\"{modifier} {base_word}\"\n",
    "            \n",
    "            # Search in original text for hyphenated version\n",
    "            text_lower = doc.text.lower()\n",
    "            for match in re.finditer(re.escape(pattern1), text_lower):\n",
    "                start_char = match.start()\n",
    "                end_char = match.end()\n",
    "                \n",
    "                # Find corresponding token positions\n",
    "                start_token_idx = None\n",
    "                end_token_idx = None\n",
    "                \n",
    "                for i, token in enumerate(doc):\n",
    "                    if token.idx <= start_char < token.idx + len(token.text):\n",
    "                        start_token_idx = i\n",
    "                    if token.idx < end_char <= token.idx + len(token.text):\n",
    "                        end_token_idx = i\n",
    "                        break\n",
    "                \n",
    "                if start_token_idx is not None and end_token_idx is not None:\n",
    "                    found_terms.append({\n",
    "                        'term': pattern1,\n",
    "                        'pos': pos_tag,\n",
    "                        'start_idx': start_token_idx,\n",
    "                        'end_idx': end_token_idx,\n",
    "                        'sentence': doc[start_token_idx].sent,\n",
    "                        'negated': False\n",
    "                    })\n",
    "                    # Mark positions as excluded\n",
    "                    for idx in range(start_token_idx, end_token_idx + 1):\n",
    "                        excluded_positions.add(idx)\n",
    "            \n",
    "            # Search for space-separated version\n",
    "            for i in range(len(tokens) - 1):\n",
    "                if tokens[i] == modifier and tokens[i + 1] == base_word:\n",
    "                    found_terms.append({\n",
    "                        'term': pattern2,\n",
    "                        'pos': pos_tag,\n",
    "                        'start_idx': i,\n",
    "                        'end_idx': i + 1,\n",
    "                        'sentence': doc[i].sent,\n",
    "                        'negated': False\n",
    "                    })\n",
    "                    # Mark positions as excluded\n",
    "                    excluded_positions.add(i)\n",
    "                    excluded_positions.add(i + 1)\n",
    "    \n",
    "    return found_terms, excluded_positions\n",
    "\n",
    "def find_single_word_terms(doc, word_list, pos_tag, excluded_positions):\n",
    "    \"\"\"Find single word terms, excluding positions already counted in multiword terms.\"\"\"\n",
    "    found_terms = []\n",
    "    \n",
    "    for i, token in enumerate(doc):\n",
    "        if i in excluded_positions:\n",
    "            continue\n",
    "            \n",
    "        lemma_lower = token.lemma_.lower()\n",
    "        if lemma_lower in word_list:\n",
    "            # Skip \"sustainability\" if followed by \"report\"\n",
    "            if lemma_lower == \"sustainability\" and i + 1 < len(doc):\n",
    "                next_token = doc[i + 1]\n",
    "                if next_token.lemma_.lower() in {\"report\", \"reporting\"}:\n",
    "                    continue\n",
    "\n",
    "            # Skip \"PV\" if it's between brackets like \"(PV)\"\n",
    "            if lemma_lower == \"pv\":\n",
    "                left_char = doc.text[token.idx - 1] if token.idx > 0 else \"\"\n",
    "                right_char = doc.text[token.idx + len(token)] if token.idx + len(token) < len(doc.text) else \"\"\n",
    "                if left_char == \"(\" and right_char == \")\":\n",
    "                    continue\n",
    "\n",
    "            found_terms.append({\n",
    "                'term': lemma_lower,\n",
    "                'pos': pos_tag,\n",
    "                'start_idx': i,\n",
    "                'end_idx': i,\n",
    "                'sentence': token.sent,\n",
    "                'negated': False\n",
    "            })\n",
    "\n",
    "    return found_terms\n",
    "\n",
    "def find_dependency_patterns(doc, excluded_positions):\n",
    "    \"\"\"Find dependency-based green patterns.\"\"\"\n",
    "    found_terms = []\n",
    "    dependency_excluded_positions = set()\n",
    "    \n",
    "    for pattern_name, pattern_info in dependency_green_patterns.items():\n",
    "        head_words = pattern_info[\"head_words\"]\n",
    "        dependent_words = pattern_info[\"dependent_words\"]\n",
    "        dependency_relations = pattern_info[\"dependency_relations\"]\n",
    "        \n",
    "        for token in doc:\n",
    "            # Skip if token is already counted\n",
    "            if token.i in excluded_positions or token.i in dependency_excluded_positions:\n",
    "                continue\n",
    "                \n",
    "            token_lemma = token.lemma_.lower()\n",
    "            \n",
    "            # Check if current token is a head word\n",
    "            if token_lemma in head_words:\n",
    "                # Look for dependents\n",
    "                for child in token.subtree:\n",
    "                    if (child.dep_ in dependency_relations and \n",
    "                        child.lemma_.lower() in dependent_words and\n",
    "                        child.i not in excluded_positions and\n",
    "                        child.i not in dependency_excluded_positions):\n",
    "                        \n",
    "                        # Found a match\n",
    "                        term_text = f\"{child.text} {token.text}\"\n",
    "                        found_terms.append({\n",
    "                            'term': term_text,\n",
    "                            'pos': f\"dependency_{pattern_name}\",\n",
    "                            'start_idx': min(child.i, token.i),\n",
    "                            'end_idx': max(child.i, token.i),\n",
    "                            'sentence': token.sent,\n",
    "                            'pattern': pattern_name,\n",
    "                            'dependency': child.dep_,\n",
    "                            'negated': False\n",
    "                        })\n",
    "                        # Mark both positions as used\n",
    "                        dependency_excluded_positions.add(child.i)\n",
    "                        dependency_excluded_positions.add(token.i)\n",
    "            \n",
    "            # Check if current token is a dependent word\n",
    "            elif token_lemma in dependent_words:\n",
    "                # Look at its head\n",
    "                head = token.head\n",
    "                if (token.dep_ in dependency_relations and\n",
    "                    head.lemma_.lower() in head_words and\n",
    "                    head.i not in excluded_positions and\n",
    "                    head.i not in dependency_excluded_positions):\n",
    "                    \n",
    "                    # Found a match\n",
    "                    term_text = f\"{token.text} {head.text}\"\n",
    "                    found_terms.append({\n",
    "                        'term': term_text,\n",
    "                        'pos': f\"dependency_{pattern_name}\",\n",
    "                        'start_idx': min(token.i, head.i),\n",
    "                        'end_idx': max(token.i, head.i),\n",
    "                        'sentence': token.sent,\n",
    "                        'pattern': pattern_name,\n",
    "                        'dependency': token.dep_,\n",
    "                        'negated': False\n",
    "                    })\n",
    "                    # Mark both positions as used\n",
    "                    dependency_excluded_positions.add(token.i)\n",
    "                    dependency_excluded_positions.add(head.i)\n",
    "    \n",
    "    return found_terms, dependency_excluded_positions\n",
    "\n",
    "def get_context_words(doc, start_idx, end_idx, context_size=5):\n",
    "    \"\"\"Extract context words around the found term.\"\"\"\n",
    "    # Get the sentence containing the term\n",
    "    sentence = doc[start_idx].sent\n",
    "    sent_start = sentence.start\n",
    "    sent_end = sentence.end\n",
    "    \n",
    "    # Calculate context boundaries within the sentence\n",
    "    context_start = max(sent_start, start_idx - context_size)\n",
    "    context_end = min(sent_end, end_idx + context_size + 1)\n",
    "    \n",
    "    # Extract context tokens\n",
    "    context_tokens = []\n",
    "    for i in range(context_start, context_end):\n",
    "        if i == start_idx and start_idx == end_idx:\n",
    "            # Single word term - highlight it\n",
    "            context_tokens.append(f\"**{doc[i].text}**\")\n",
    "        elif i == start_idx:\n",
    "            # Start of multiword term\n",
    "            context_tokens.append(f\"**{doc[i].text}\")\n",
    "        elif i == end_idx:\n",
    "            # End of multiword term\n",
    "            context_tokens.append(f\"{doc[i].text}**\")\n",
    "        elif start_idx < i < end_idx:\n",
    "            # Middle of multiword term\n",
    "            context_tokens.append(doc[i].text)\n",
    "        else:\n",
    "            # Regular context word\n",
    "            context_tokens.append(doc[i].text)\n",
    "    \n",
    "    return \" \".join(context_tokens)\n",
    "\n",
    "def analyze_green_terms_with_negation(doc, doc_name):\n",
    "    \"\"\"\n",
    "    Analyze all green terms in a document with enhanced multi-scope negation detection and context-dependent terms.\n",
    "    Returns: (valid_counts, valid_terms, negated_terms, original_counts)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ANALYZING: {doc_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    all_found_terms = []\n",
    "    all_excluded_positions = set()\n",
    "    \n",
    "    # Step 1: Find multiword terms first (all types together)\n",
    "    multiword_noun_terms, excluded_noun_pos = find_multiword_terms(doc, green_multiword_nouns, \"multiword_noun\")\n",
    "    multiword_adj_terms, excluded_adj_pos = find_multiword_terms(doc, green_multiword_adjectives, \"multiword_adjective\")\n",
    "    multiword_adv_terms, excluded_adv_pos = find_multiword_terms(doc, green_multiword_adverbs, \"multiword_adverb\")\n",
    "    \n",
    "    # Combine all multiword terms and check for overlaps\n",
    "    all_multiword_terms = multiword_noun_terms + multiword_adj_terms + multiword_adv_terms\n",
    "    \n",
    "    # Remove duplicates based on position overlap\n",
    "    filtered_multiword_terms = []\n",
    "    used_positions = set()\n",
    "    \n",
    "    # Sort by start position to process in order\n",
    "    all_multiword_terms.sort(key=lambda x: x['start_idx'])\n",
    "    \n",
    "    for term in all_multiword_terms:\n",
    "        # Check if this term overlaps with any already used positions\n",
    "        term_positions = set(range(term['start_idx'], term['end_idx'] + 1))\n",
    "        if not term_positions.intersection(used_positions):\n",
    "            filtered_multiword_terms.append(term)\n",
    "            used_positions.update(term_positions)\n",
    "            all_excluded_positions.update(term_positions)\n",
    "    \n",
    "    all_found_terms.extend(filtered_multiword_terms)\n",
    "    \n",
    "    # Step 2: Find dependency patterns (excluding already counted positions)\n",
    "    dependency_terms, dependency_excluded_pos = find_dependency_patterns(doc, all_excluded_positions)\n",
    "    \n",
    "    # Filter dependency terms to avoid overlap with multiword terms\n",
    "    filtered_dependency_terms = []\n",
    "    for dep_term in dependency_terms:\n",
    "        dep_positions = set(range(dep_term['start_idx'], dep_term['end_idx'] + 1))\n",
    "        if not dep_positions.intersection(all_excluded_positions):\n",
    "            filtered_dependency_terms.append(dep_term)\n",
    "            all_excluded_positions.update(dep_positions)\n",
    "    \n",
    "    all_found_terms.extend(filtered_dependency_terms)\n",
    "    \n",
    "    # Step 3: Find context-dependent terms (excluding already counted positions)\n",
    "    context_dependent_terms, context_excluded_pos = find_context_dependent_terms(doc, all_excluded_positions, all_found_terms)\n",
    "    \n",
    "    # Filter context-dependent terms to avoid overlap\n",
    "    filtered_context_terms = []\n",
    "    for ctx_term in context_dependent_terms:\n",
    "        ctx_positions = set(range(ctx_term['start_idx'], ctx_term['end_idx'] + 1))\n",
    "        if not ctx_positions.intersection(all_excluded_positions):\n",
    "            filtered_context_terms.append(ctx_term)\n",
    "            all_excluded_positions.update(ctx_positions)\n",
    "    \n",
    "    all_found_terms.extend(filtered_context_terms)\n",
    "    \n",
    "    # Step 4: Find single word terms with position tracking to prevent double counting\n",
    "    # Process in priority order: verbs > nouns > adjectives > adverbs\n",
    "    \n",
    "    # Priority 1: Verbs (actions are often most important)\n",
    "    single_verb_terms = find_single_word_terms(doc, green_verbs, \"verb\", all_excluded_positions)\n",
    "    for term in single_verb_terms:\n",
    "        all_excluded_positions.add(term['start_idx'])\n",
    "    all_found_terms.extend(single_verb_terms)\n",
    "    \n",
    "    # Priority 2: Nouns (concrete green concepts)\n",
    "    single_noun_terms = find_single_word_terms(doc, green_nouns, \"noun\", all_excluded_positions)\n",
    "    for term in single_noun_terms:\n",
    "        all_excluded_positions.add(term['start_idx'])\n",
    "    all_found_terms.extend(single_noun_terms)\n",
    "    \n",
    "    # Priority 3: Adjectives (descriptive green terms)\n",
    "    single_adj_terms = find_single_word_terms(doc, green_adjectives, \"adjective\", all_excluded_positions)\n",
    "    for term in single_adj_terms:\n",
    "        all_excluded_positions.add(term['start_idx'])\n",
    "    all_found_terms.extend(single_adj_terms)\n",
    "    \n",
    "    # Priority 4: Adverbs (manner of green actions)\n",
    "    single_adv_terms = find_single_word_terms(doc, green_adverbs, \"adverb\", all_excluded_positions)\n",
    "    for term in single_adv_terms:\n",
    "        all_excluded_positions.add(term['start_idx'])\n",
    "    all_found_terms.extend(single_adv_terms)\n",
    "    \n",
    "    # Step 5: Apply enhanced multi-scope negation detection\n",
    "    print(f\"Found {len(all_found_terms)} green terms before negation filtering...\")\n",
    "    valid_terms, negated_terms = filter_negated_terms(all_found_terms, doc)\n",
    "    print(f\"After negation filtering: {len(valid_terms)} valid terms, {len(negated_terms)} negated terms\")\n",
    "    \n",
    "    # Count by type for original, valid, and negated terms\n",
    "    original_type_counts = Counter()\n",
    "    valid_type_counts = Counter()\n",
    "    negated_type_counts = Counter()\n",
    "    \n",
    "    for term_info in all_found_terms:\n",
    "        original_type_counts[term_info['pos']] += 1\n",
    "    \n",
    "    for term_info in valid_terms:\n",
    "        valid_type_counts[term_info['pos']] += 1\n",
    "    \n",
    "    for term_info in negated_terms:\n",
    "        negated_type_counts[term_info['pos']] += 1\n",
    "    \n",
    "    # Print comprehensive counts\n",
    "    print(f\"\\nGREEN TERMS COUNTS (ORIGINAL | NEGATED | VALID):\")\n",
    "    print(f\"Nouns: {original_type_counts['noun']} | {negated_type_counts['noun']} | {valid_type_counts['noun']}\")\n",
    "    print(f\"Multiword Nouns: {original_type_counts['multiword_noun']} | {negated_type_counts['multiword_noun']} | {valid_type_counts['multiword_noun']}\")\n",
    "    print(f\"Adjectives: {original_type_counts['adjective']} | {negated_type_counts['adjective']} | {valid_type_counts['adjective']}\")  \n",
    "    print(f\"Multiword Adjectives: {original_type_counts['multiword_adjective']} | {negated_type_counts['multiword_adjective']} | {valid_type_counts['multiword_adjective']}\")\n",
    "    print(f\"Verbs: {original_type_counts['verb']} | {negated_type_counts['verb']} | {valid_type_counts['verb']}\")\n",
    "    print(f\"Adverbs: {original_type_counts['adverb']} | {negated_type_counts['adverb']} | {valid_type_counts['adverb']}\")\n",
    "    print(f\"Multiword Adverbs: {original_type_counts['multiword_adverb']} | {negated_type_counts['multiword_adverb']} | {valid_type_counts['multiword_adverb']}\")\n",
    "    print(f\"Context-Dependent Nouns: {original_type_counts['context_dependent_noun']} | {negated_type_counts['context_dependent_noun']} | {valid_type_counts['context_dependent_noun']}\")\n",
    "    print(f\"Context-Dependent Multiword Nouns: {original_type_counts['context_dependent_multiword_noun']} | {negated_type_counts['context_dependent_multiword_noun']} | {valid_type_counts['context_dependent_multiword_noun']}\")\n",
    "    \n",
    "    # Count dependency patterns\n",
    "    original_dependency_counts = Counter()\n",
    "    valid_dependency_counts = Counter()\n",
    "    negated_dependency_counts = Counter()\n",
    "    \n",
    "    for term_info in all_found_terms:\n",
    "        if term_info['pos'].startswith('dependency_'):\n",
    "            original_dependency_counts[term_info['pos']] += 1\n",
    "    \n",
    "    for term_info in valid_terms:\n",
    "        if term_info['pos'].startswith('dependency_'):\n",
    "            valid_dependency_counts[term_info['pos']] += 1\n",
    "            \n",
    "    for term_info in negated_terms:\n",
    "        if term_info['pos'].startswith('dependency_'):\n",
    "            negated_dependency_counts[term_info['pos']] += 1\n",
    "    \n",
    "    print(f\"Dependency Patterns: {sum(original_dependency_counts.values())} | {sum(negated_dependency_counts.values())} | {sum(valid_dependency_counts.values())}\")\n",
    "    \n",
    "    for dep_type in original_dependency_counts.keys():\n",
    "        pattern_name = dep_type.replace('dependency_', '')\n",
    "        orig_count = original_dependency_counts[dep_type]\n",
    "        neg_count = negated_dependency_counts[dep_type]\n",
    "        val_count = valid_dependency_counts[dep_type]\n",
    "        print(f\"  {pattern_name}: {orig_count} | {neg_count} | {val_count}\")\n",
    "    \n",
    "    print(f\"TOTAL: {sum(original_type_counts.values())} | {sum(negated_type_counts.values())} | {sum(valid_type_counts.values())}\")\n",
    "    \n",
    "    # Generate negation statistics\n",
    "    negation_stats = get_negation_statistics(negated_terms)\n",
    "    if negation_stats:\n",
    "        print(f\"\\nENHANCED NEGATION ANALYSIS:\")\n",
    "        print(f\"Total negated terms: {negation_stats['total_negated']}\")\n",
    "        print(f\"Negation types: {negation_stats['by_type']}\")\n",
    "        print(f\"Negation scopes: {negation_stats['by_scope']}\")\n",
    "        print(f\"Examples of negated terms (with scope):\")\n",
    "        for term, neg_type, neg_text, scope in negation_stats['examples']:\n",
    "            print(f\"  - '{term}' (negated by: {neg_text}, type: {neg_type}, scope: {scope})\")\n",
    "    \n",
    "    # Sort valid terms by their position in the text\n",
    "    valid_terms_sorted = sorted(valid_terms, key=lambda x: x['start_idx'])\n",
    "    \n",
    "    # Print valid terms in order they appear in the text\n",
    "    print(f\"\\nVALID TERMS IN TEXT ORDER:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, term_info in enumerate(valid_terms_sorted[:20], 1):\n",
    "        # Format the term type for display\n",
    "        if term_info['pos'].startswith('dependency_'):\n",
    "            pattern_name = term_info['pos'].replace('dependency_', '').upper()\n",
    "            term_type = f\"DEPENDENCY {pattern_name} TERM\"\n",
    "        elif term_info['pos'].startswith('context_dependent_'):\n",
    "            ctx_type = term_info.get('context_type', 'unknown').upper()\n",
    "            base_type = term_info['pos'].replace('context_dependent_', '').upper().replace('_', ' ')\n",
    "            term_type = f\"CONTEXT-DEPENDENT {base_type} ({ctx_type})\"\n",
    "        else:\n",
    "            term_type = term_info['pos'].upper().replace('_', ' ') + \" TERM\"\n",
    "        \n",
    "        context = get_context_words(doc, term_info['start_idx'], term_info['end_idx'])\n",
    "        print(f\"{i}. {term_type}: {term_info['term']}\")\n",
    "        \n",
    "        # Add dependency info if it's a dependency pattern\n",
    "        if 'dependency' in term_info:\n",
    "            print(f\"   (Dependency: {term_info['dependency']})\")\n",
    "        \n",
    "        # Add context info if it's a context-dependent term\n",
    "        if 'context_word' in term_info:\n",
    "            print(f\"   (Context: '{term_info['context_word']}' -> Neutral: '{term_info['neutral_part']}')\")\n",
    "        \n",
    "        print(f\"   Context: {context}\")\n",
    "        print()\n",
    "    \n",
    "    # If there are negated terms, show some examples\n",
    "    if negated_terms:\n",
    "        print(f\"\\nEXAMPLES OF NEGATED TERMS (EXCLUDED FROM COUNT):\")\n",
    "        print(\"-\" * 40)\n",
    "        negated_terms_sorted = sorted(negated_terms, key=lambda x: x['start_idx'])\n",
    "        for i, term_info in enumerate(negated_terms_sorted[:10], 1):\n",
    "            if term_info['pos'].startswith('dependency_'):\n",
    "                pattern_name = term_info['pos'].replace('dependency_', '').upper()\n",
    "                term_type = f\"DEPENDENCY {pattern_name} TERM\"\n",
    "            elif term_info['pos'].startswith('context_dependent_'):\n",
    "                ctx_type = term_info.get('context_type', 'unknown').upper()\n",
    "                base_type = term_info['pos'].replace('context_dependent_', '').upper().replace('_', ' ')\n",
    "                term_type = f\"CONTEXT-DEPENDENT {base_type} ({ctx_type})\"\n",
    "            else:\n",
    "                term_type = term_info['pos'].upper().replace('_', ' ') + \" TERM\"\n",
    "            \n",
    "            context = get_context_words(doc, term_info['start_idx'], term_info['end_idx'])\n",
    "            print(f\"{i}. {term_type}: {term_info['term']}\")\n",
    "            scope_info = f\" in {term_info.get('negation_scope', 'unknown')} scope\" if 'negation_scope' in term_info else \"\"\n",
    "            print(f\"   Negated by: {term_info['negation_text']} (type: {term_info['negation_type']}{scope_info})\")\n",
    "            \n",
    "            # Add context info if it's a context-dependent term\n",
    "            if 'context_word' in term_info:\n",
    "                print(f\"   (Context: '{term_info['context_word']}' -> Neutral: '{term_info['neutral_part']}')\")\n",
    "            \n",
    "            print(f\"   Context: {context}\")\n",
    "            print()\n",
    "    \n",
    "    return valid_type_counts, valid_terms, negated_terms, original_type_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa79b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "for doc_name, doc in documents.items():\n",
    "    valid_counts, valid_terms, negated_terms, original_counts = analyze_green_terms_with_negation(doc, doc_name)\n",
    "    all_results[doc_name] = {\n",
    "        'valid_counts': valid_counts,\n",
    "        'valid_terms': valid_terms,\n",
    "        'negated_terms': negated_terms,\n",
    "        'original_counts': original_counts,\n",
    "        'total_tokens': len(doc),\n",
    "        'total_sentences': len(list(doc.sents))\n",
    "    }\n",
    "\n",
    "# Cell 10: Print Comprehensive Summary\n",
    "print(f\"\\n{'='*140}\")\n",
    "print(\"COMPREHENSIVE SUMMARY - GREEN TERMS ANALYSIS WITH CONTEXT-DEPENDENT TERMS + MULTI-SCOPE NEGATION\")\n",
    "print(f\"{'='*140}\")\n",
    "\n",
    "print(\"\\n1. ORIGINAL COUNTS (Before Negation Filtering)\")\n",
    "print(f\"{'Document':<25} {'Nouns':<8} {'M-Nouns':<8} {'Adj':<8} {'M-Adj':<8} {'Verbs':<8} {'Adv':<8} {'M-Adv':<8} {'Ctx-N':<8} {'Ctx-MN':<8} {'Dep-Pat':<8} {'Total':<8}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "for doc_name, results in all_results.items():\n",
    "    counts = results['original_counts']\n",
    "    dependency_total = sum(count for pos_type, count in counts.items() if pos_type.startswith('dependency_'))\n",
    "    total = sum(counts.values())\n",
    "    print(f\"{doc_name:<25} {counts['noun']:<8} {counts['multiword_noun']:<8} {counts['adjective']:<8} {counts['multiword_adjective']:<8} {counts['verb']:<8} {counts['adverb']:<8} {counts['multiword_adverb']:<8} {counts['context_dependent_noun']:<8} {counts['context_dependent_multiword_noun']:<8} {dependency_total:<8} {total:<8}\")\n",
    "\n",
    "print(\"\\n2. NEGATED TERMS (Filtered Out)\")\n",
    "print(f\"{'Document':<25} {'Nouns':<8} {'M-Nouns':<8} {'Adj':<8} {'M-Adj':<8} {'Verbs':<8} {'Adv':<8} {'M-Adv':<8} {'Ctx-N':<8} {'Ctx-MN':<8} {'Dep-Pat':<8} {'Total':<8}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "for doc_name, results in all_results.items():\n",
    "    negated_counts = Counter()\n",
    "    for term in results['negated_terms']:\n",
    "        negated_counts[term['pos']] += 1\n",
    "    \n",
    "    dependency_total = sum(count for pos_type, count in negated_counts.items() if pos_type.startswith('dependency_'))\n",
    "    total = sum(negated_counts.values())\n",
    "    print(f\"{doc_name:<25} {negated_counts['noun']:<8} {negated_counts['multiword_noun']:<8} {negated_counts['adjective']:<8} {negated_counts['multiword_adjective']:<8} {negated_counts['verb']:<8} {negated_counts['adverb']:<8} {negated_counts['multiword_adverb']:<8} {negated_counts['context_dependent_noun']:<8} {negated_counts['context_dependent_multiword_noun']:<8} {dependency_total:<8} {total:<8}\")\n",
    "\n",
    "print(\"\\n3. FINAL VALID COUNTS (After Multi-Scope + Reduction Verb Negation Filtering)\")\n",
    "print(f\"{'Document':<25} {'Nouns':<8} {'M-Nouns':<8} {'Adj':<8} {'M-Adj':<8} {'Verbs':<8} {'Adv':<8} {'M-Adv':<8} {'Ctx-N':<8} {'Ctx-MN':<8} {'Dep-Pat':<8} {'Total':<8}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "for doc_name, results in all_results.items():\n",
    "    counts = results['valid_counts']\n",
    "    dependency_total = sum(count for pos_type, count in counts.items() if pos_type.startswith('dependency_'))\n",
    "    total = sum(counts.values())\n",
    "    print(f\"{doc_name:<25} {counts['noun']:<8} {counts['multiword_noun']:<8} {counts['adjective']:<8} {counts['multiword_adjective']:<8} {counts['verb']:<8} {counts['adverb']:<8} {counts['multiword_adverb']:<8} {counts['context_dependent_noun']:<8} {counts['context_dependent_multiword_noun']:<8} {dependency_total:<8} {total:<8}\")\n",
    "\n",
    "print(\"\\n4. CONTEXT-DEPENDENT TERMS ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Analyze context-dependent terms by type\n",
    "all_context_types = Counter()\n",
    "all_context_words = Counter()\n",
    "context_examples = []\n",
    "\n",
    "for doc_name, results in all_results.items():\n",
    "    doc_context_negative = 0\n",
    "    doc_context_improvement = 0\n",
    "    \n",
    "    for term in results['valid_terms']:\n",
    "        if term['pos'].startswith('context_dependent_'):\n",
    "            context_type = term.get('context_type', 'unknown')\n",
    "            context_word = term.get('context_word', 'unknown')\n",
    "            all_context_types[context_type] += 1\n",
    "            all_context_words[context_word.lower()] += 1\n",
    "            \n",
    "            if context_type == 'negative':\n",
    "                doc_context_negative += 1\n",
    "            elif context_type == 'improvement':\n",
    "                doc_context_improvement += 1\n",
    "            \n",
    "            if len(context_examples) < 10:\n",
    "                context_examples.append((doc_name, term['term'], term['context_word'], term['neutral_part'], context_type))\n",
    "    \n",
    "    total_context = doc_context_negative + doc_context_improvement\n",
    "    if total_context > 0:\n",
    "        print(f\"{doc_name}: {total_context} context-dependent terms ({doc_context_negative} negative, {doc_context_improvement} improvement)\")\n",
    "\n",
    "print(f\"\\nOverall context type distribution: {dict(all_context_types)}\")\n",
    "print(f\"Top context words: {dict(all_context_words.most_common(10))}\")\n",
    "\n",
    "print(f\"\\nExamples of context-dependent terms:\")\n",
    "for doc, term, context, neutral, ctx_type in context_examples:\n",
    "    print(f\"  - '{term}' = '{context}' + '{neutral}' ({ctx_type} context) [{doc}]\")\n",
    "\n",
    "print(\"\\n5. NEGATION IMPACT SUMMARY\")\n",
    "print(f\"{'Document':<25} {'Original':<10} {'Negated':<10} {'Valid':<10} {'Negation %':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for doc_name, results in all_results.items():\n",
    "    original_total = sum(results['original_counts'].values())\n",
    "    negated_total = len(results['negated_terms'])\n",
    "    valid_total = sum(results['valid_counts'].values())\n",
    "    negation_pct = (negated_total / original_total * 100) if original_total > 0 else 0\n",
    "    \n",
    "    print(f\"{doc_name:<25} {original_total:<10} {negated_total:<10} {valid_total:<10} {negation_pct:<11.1f}%\")\n",
    "\n",
    "print(f\"\\n{'='*140}\")\n",
    "print(\"ANALYSIS COMPLETE - Enhanced with Context-Dependent Terms + Multi-Scope Negation Detection\")\n",
    "print(\"Key features:\")\n",
    "print(\"1. Context-dependent terms: neutral terms that become green with negative/improvement context\")\n",
    "print(\"2. Multi-scope negation detection: subtree, ancestors, head.subtree, and sentence-level\")\n",
    "print(\"3. Reduction verb negation: detects when positive verbs are negated\")\n",
    "print(\"4. Comprehensive term classification and overlap prevention\")\n",
    "print(f\"{'='*140}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e0035c",
   "metadata": {},
   "source": [
    "## Green term density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a67d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_specificity_weight(pos_type):\n",
    "    \"\"\"Assign priority weights to different term types for overlap resolution.\"\"\"\n",
    "    weights = {\n",
    "        'context_dependent_multiword_noun': 5,\n",
    "        'multiword_noun': 4,\n",
    "        'multiword_adjective': 4,\n",
    "        'multiword_adverb': 4,\n",
    "        'context_dependent_noun': 3,\n",
    "        'dependency_': 3,\n",
    "        'noun': 1,\n",
    "        'adjective': 1,\n",
    "        'verb': 1,\n",
    "        'adverb': 1\n",
    "    }\n",
    "    \n",
    "    # Check for dependency patterns first\n",
    "    if pos_type.startswith('dependency_'):\n",
    "        return 3\n",
    "    \n",
    "    # Check other patterns\n",
    "    for pattern, weight in weights.items():\n",
    "        if pattern in pos_type:\n",
    "            return weight\n",
    "    return 0\n",
    "\n",
    "def resolve_overlapping_terms(all_terms, doc):\n",
    "    \"\"\"\n",
    "    Resolve overlapping terms by prioritizing longer, more specific terms.\n",
    "    Returns final terms with no overlapping positions.\n",
    "    \"\"\"\n",
    "    # Sort by specificity weight (highest first), then by length (longest first)\n",
    "    sorted_terms = sorted(all_terms, \n",
    "                         key=lambda x: (get_term_specificity_weight(x['pos']), \n",
    "                                       x['end_idx'] - x['start_idx']), \n",
    "                         reverse=True)\n",
    "    \n",
    "    used_positions = set()\n",
    "    final_terms = []\n",
    "    overlap_log = []\n",
    "    \n",
    "    for term in sorted_terms:\n",
    "        term_positions = set(range(term['start_idx'], term['end_idx'] + 1))\n",
    "        \n",
    "        # Check for overlap with already used positions\n",
    "        overlap = term_positions.intersection(used_positions)\n",
    "        if not overlap:\n",
    "            final_terms.append(term)\n",
    "            used_positions.update(term_positions)\n",
    "        else:\n",
    "            overlap_log.append({\n",
    "                'rejected_term': term['term'],\n",
    "                'pos_type': term['pos'],\n",
    "                'overlapping_positions': list(overlap)\n",
    "            })\n",
    "    \n",
    "    return final_terms, overlap_log\n",
    "\n",
    "def extract_green_word_tokens(term_info, doc, excluded_positions):\n",
    "    \"\"\"\n",
    "    Extract individual word tokens from a green term, excluding already counted positions.\n",
    "    Excludes stop words for consistency with total word count.\n",
    "    Returns list of token indices that comprise this term.\n",
    "    \"\"\"\n",
    "    start_idx = term_info['start_idx']\n",
    "    end_idx = term_info['end_idx']\n",
    "    \n",
    "    term_token_indices = []\n",
    "    \n",
    "    for token_idx in range(start_idx, end_idx + 1):\n",
    "        if token_idx not in excluded_positions:\n",
    "            token = doc[token_idx]\n",
    "            # Only count meaningful tokens (exclude punctuation, whitespace, and stop words)\n",
    "            if (not token.is_punct and \n",
    "                not token.is_space and \n",
    "                token.text.strip() and\n",
    "                len(token.text.strip()) > 0 and\n",
    "                not token.is_stop):\n",
    "                term_token_indices.append(token_idx)\n",
    "    \n",
    "    return term_token_indices\n",
    "\n",
    "def count_total_content_words(doc, exclude_stop_words=True):\n",
    "    \"\"\"\n",
    "    Count total content words in document, optionally excluding stop words.\n",
    "    Returns (total_words, stop_words_count, punctuation_count)\n",
    "    \"\"\"\n",
    "    total_words = 0\n",
    "    stop_words_count = 0\n",
    "    punctuation_count = 0\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.is_punct or token.is_space:\n",
    "            if token.is_punct:\n",
    "                punctuation_count += 1\n",
    "            continue\n",
    "            \n",
    "        if not token.text.strip():\n",
    "            continue\n",
    "            \n",
    "        if token.is_stop:\n",
    "            stop_words_count += 1\n",
    "            if not exclude_stop_words:\n",
    "                total_words += 1\n",
    "        else:\n",
    "            total_words += 1\n",
    "    \n",
    "    return total_words, stop_words_count, punctuation_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35873d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_green_term_frequency(all_results, documents, exclude_stop_words=True):\n",
    "    \"\"\"\n",
    "    Comprehensive frequency analysis of green terms with precise word counting.\n",
    "    \n",
    "    Args:\n",
    "        all_results: Dictionary with green term analysis results for each document\n",
    "        documents: Dictionary with spaCy Doc objects for each document\n",
    "        exclude_stop_words: Whether to exclude stop words from total word count\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with frequency analysis results for each document\n",
    "    \"\"\"\n",
    "    frequency_results = {}\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"GREEN TERM FREQUENCY ANALYSIS - DETAILED WORD COUNTING\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Stop words {'EXCLUDED' if exclude_stop_words else 'INCLUDED'} from total word count\")\n",
    "    print()\n",
    "    \n",
    "    for doc_name, results in all_results.items():\n",
    "        print(f\"Analyzing frequency for: {doc_name}\")\n",
    "        doc = documents[doc_name]\n",
    "        \n",
    "        # Resolve overlapping terms to prevent double counting\n",
    "        all_valid_terms = results['valid_terms']\n",
    "        final_terms, overlap_log = resolve_overlapping_terms(all_valid_terms, doc)\n",
    "        \n",
    "        # Extract green word tokens with position tracking\n",
    "        green_word_positions = set()\n",
    "        word_breakdown = {}\n",
    "        term_contributions = []\n",
    "        \n",
    "        for term_info in final_terms:\n",
    "            term_token_indices = extract_green_word_tokens(term_info, doc, green_word_positions)\n",
    "            \n",
    "            if term_token_indices:\n",
    "                # Update position tracking\n",
    "                green_word_positions.update(term_token_indices)\n",
    "                \n",
    "                # Store breakdown for validation\n",
    "                term_tokens = [doc[idx].text for idx in term_token_indices]\n",
    "                word_breakdown[term_info['term']] = {\n",
    "                    'tokens': term_tokens,\n",
    "                    'token_indices': term_token_indices,\n",
    "                    'pos_type': term_info['pos'],\n",
    "                    'word_count': len(term_token_indices)\n",
    "                }\n",
    "                \n",
    "                term_contributions.append({\n",
    "                    'term': term_info['term'],\n",
    "                    'pos_type': term_info['pos'],\n",
    "                    'word_count': len(term_token_indices),\n",
    "                    'tokens': term_tokens\n",
    "                })\n",
    "        \n",
    "        # Count total content words in document\n",
    "        total_words, stop_words_count, punctuation_count = count_total_content_words(doc, exclude_stop_words)\n",
    "        \n",
    "        # Final green word count and validation\n",
    "        green_word_count = len(green_word_positions)\n",
    "        \n",
    "        # Check for stop words in green terms (quality control)\n",
    "        stop_words_in_green = 0\n",
    "        green_stop_words = []\n",
    "        for pos in green_word_positions:\n",
    "            token = doc[pos]\n",
    "            if token.is_stop:\n",
    "                stop_words_in_green += 1\n",
    "                green_stop_words.append(token.text)\n",
    "        \n",
    "        # Calculate frequency\n",
    "        green_term_frequency = (green_word_count / total_words * 100) if total_words > 0 else 0\n",
    "        \n",
    "        # Create unique word analysis\n",
    "        unique_green_words = set()\n",
    "        unique_green_lemmas = set()\n",
    "        for pos in green_word_positions:\n",
    "            token = doc[pos]\n",
    "            unique_green_words.add(token.text.lower())\n",
    "            unique_green_lemmas.add(token.lemma_.lower())\n",
    "        \n",
    "        # Store comprehensive results\n",
    "        frequency_results[doc_name] = {\n",
    "            # Core metrics\n",
    "            'green_word_count': green_word_count,\n",
    "            'total_words': total_words,\n",
    "            'green_term_frequency': round(green_term_frequency, 3),\n",
    "            \n",
    "            # Document stats\n",
    "            'total_tokens': len(doc),\n",
    "            'total_sentences': len(list(doc.sents)),\n",
    "            'stop_words_in_doc': stop_words_count,\n",
    "            'punctuation_tokens': punctuation_count,\n",
    "            \n",
    "            # Green term details\n",
    "            'unique_green_words': len(unique_green_words),\n",
    "            'unique_green_lemmas': len(unique_green_lemmas),\n",
    "            'total_green_terms': len(final_terms),\n",
    "            'original_terms_before_overlap_resolution': len(all_valid_terms),\n",
    "            \n",
    "            # Quality control\n",
    "            'stop_words_in_green_terms': stop_words_in_green,\n",
    "            'green_stop_words': green_stop_words,\n",
    "            'overlapping_terms_resolved': len(all_valid_terms) - len(final_terms),\n",
    "            'overlap_log': overlap_log,\n",
    "            \n",
    "            # Detailed breakdown\n",
    "            'word_breakdown': word_breakdown,\n",
    "            'term_contributions': term_contributions,\n",
    "            'green_word_positions': sorted(list(green_word_positions)),\n",
    "            \n",
    "            # Validation data\n",
    "            'unique_words_list': sorted(list(unique_green_words)),\n",
    "            'unique_lemmas_list': sorted(list(unique_green_lemmas))\n",
    "        }\n",
    "        \n",
    "        print(f\"  → {green_word_count} green words out of {total_words} total words = {green_term_frequency:.3f}%\")\n",
    "        if overlap_log:\n",
    "            print(f\"  → Resolved {len(overlap_log)} overlapping terms\")\n",
    "        \n",
    "    return frequency_results\n",
    "\n",
    "def validate_frequency_results(frequency_results, documents):\n",
    "    \"\"\"Perform validation checks on frequency analysis results.\"\"\"\n",
    "    validation_report = {}\n",
    "    \n",
    "    for doc_name, results in frequency_results.items():\n",
    "        doc = documents[doc_name]\n",
    "        validation_issues = []\n",
    "        \n",
    "        # Check 1: Verify no double counting\n",
    "        position_check = len(results['green_word_positions']) == len(set(results['green_word_positions']))\n",
    "        if not position_check:\n",
    "            validation_issues.append(\"Duplicate positions found in green_word_positions\")\n",
    "        \n",
    "        # Check 2: Verify positions are valid\n",
    "        max_pos = max(results['green_word_positions']) if results['green_word_positions'] else 0\n",
    "        if max_pos >= len(doc):\n",
    "            validation_issues.append(f\"Invalid position {max_pos} (doc length: {len(doc)})\")\n",
    "        \n",
    "        # Check 3: Verify word count consistency\n",
    "        calculated_words = sum(term['word_count'] for term in results['term_contributions'])\n",
    "        if calculated_words != results['green_word_count']:\n",
    "            validation_issues.append(f\"Word count mismatch: {calculated_words} vs {results['green_word_count']}\")\n",
    "        \n",
    "        # Check 4: Verify frequency calculation\n",
    "        expected_freq = (results['green_word_count'] / results['total_words'] * 100) if results['total_words'] > 0 else 0\n",
    "        if abs(expected_freq - results['green_term_frequency']) > 0.001:\n",
    "            validation_issues.append(f\"Frequency calculation error: {expected_freq} vs {results['green_term_frequency']}\")\n",
    "        \n",
    "        validation_report[doc_name] = {\n",
    "            'passed_validation': len(validation_issues) == 0,\n",
    "            'issues': validation_issues,\n",
    "            'checks_performed': 4\n",
    "        }\n",
    "    \n",
    "    return validation_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9821db2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the frequency analysis\n",
    "frequency_results = analyze_green_term_frequency(all_results, documents, exclude_stop_words=True)\n",
    "\n",
    "# Perform validation\n",
    "validation_report = validate_frequency_results(frequency_results, documents)\n",
    "\n",
    "# Print validation results\n",
    "print(f\"\\nVALIDATION REPORT:\")\n",
    "print(\"-\" * 50)\n",
    "all_passed = True\n",
    "for doc_name, validation in validation_report.items():\n",
    "    status = \"PASSED\" if validation['passed_validation'] else \"FAILED\"\n",
    "    print(f\"{doc_name}: {status}\")\n",
    "    if validation['issues']:\n",
    "        for issue in validation['issues']:\n",
    "            print(f\"  - {issue}\")\n",
    "    all_passed = all_passed and validation['passed_validation']\n",
    "\n",
    "print(f\"\\nOverall validation: {'ALL PASSED' if all_passed else 'ISSUES DETECTED'}\")\n",
    "\n",
    "# Calculate summary statistics\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FREQUENCY ANALYSIS SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "frequencies = [result['green_term_frequency'] for result in frequency_results.values()]\n",
    "word_counts = [result['green_word_count'] for result in frequency_results.values()]\n",
    "total_words = [result['total_words'] for result in frequency_results.values()]\n",
    "\n",
    "print(f\"Frequency Statistics:\")\n",
    "print(f\"  Mean: {np.mean(frequencies):.3f}%\")\n",
    "print(f\"  Std Dev: {np.std(frequencies):.3f}%\")\n",
    "print(f\"  Min: {min(frequencies):.3f}% ({[k for k, v in frequency_results.items() if v['green_term_frequency'] == min(frequencies)][0]})\")\n",
    "print(f\"  Max: {max(frequencies):.3f}% ({[k for k, v in frequency_results.items() if v['green_term_frequency'] == max(frequencies)][0]})\")\n",
    "\n",
    "print(f\"\\nWord Count Statistics:\")\n",
    "print(f\"  Total green words across all docs: {sum(word_counts)}\")\n",
    "print(f\"  Total content words across all docs: {sum(total_words)}\")\n",
    "print(f\"  Overall frequency: {(sum(word_counts) / sum(total_words) * 100):.3f}%\")\n",
    "\n",
    "# Document ranking by frequency\n",
    "print(f\"\\nDOCUMENT RANKING BY GREEN TERM FREQUENCY:\")\n",
    "print(\"-\" * 60)\n",
    "ranked_docs = sorted(frequency_results.items(), key=lambda x: x[1]['green_term_frequency'], reverse=True)\n",
    "for i, (doc_name, result) in enumerate(ranked_docs, 1):\n",
    "    print(f\"{i}. {doc_name}\")\n",
    "    print(f\"   Frequency: {result['green_term_frequency']:.3f}%\")\n",
    "    print(f\"   Green words: {result['green_word_count']} / {result['total_words']} total words\")\n",
    "    print(f\"   Green terms: {result['total_green_terms']} terms\")\n",
    "    print(f\"   Unique words: {result['unique_green_words']} unique\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea7fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed breakdown by document\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"DETAILED FREQUENCY BREAKDOWN BY DOCUMENT\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "for doc_name, result in frequency_results.items():\n",
    "    print(f\"\\n{doc_name.upper()}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Core metrics\n",
    "    print(f\"GREEN TERM FREQUENCY: {result['green_term_frequency']:.3f}%\")\n",
    "    print(f\"Green words: {result['green_word_count']:,} | Total words: {result['total_words']:,}\")\n",
    "    print(f\"Total tokens in doc: {result['total_tokens']:,} | Sentences: {result['total_sentences']:,}\")\n",
    "    print(f\"Unique green words: {result['unique_green_words']} | Unique lemmas: {result['unique_green_lemmas']}\")\n",
    "    \n",
    "    # Quality metrics\n",
    "    if result['overlapping_terms_resolved'] > 0:\n",
    "        print(f\"Overlapping terms resolved: {result['overlapping_terms_resolved']}\")\n",
    "    if result['stop_words_in_green_terms'] > 0:\n",
    "        print(f\"Stop words in green terms: {result['stop_words_in_green_terms']} ({result['green_stop_words']})\")\n",
    "    \n",
    "    # Top contributing terms\n",
    "    top_contributors = sorted(result['term_contributions'], \n",
    "                             key=lambda x: x['word_count'], reverse=True)[:10]\n",
    "    print(f\"\\nTop 10 terms by word contribution:\")\n",
    "    for i, contrib in enumerate(top_contributors, 1):\n",
    "        term_type = contrib['pos_type'].replace('_', ' ').title()\n",
    "        print(f\"  {i:2d}. {contrib['term']} ({term_type}) - {contrib['word_count']} words\")\n",
    "    \n",
    "    # Word type distribution\n",
    "    type_distribution = Counter()\n",
    "    for contrib in result['term_contributions']:\n",
    "        type_distribution[contrib['pos_type']] += contrib['word_count']\n",
    "    \n",
    "    print(f\"\\nWord contribution by term type:\")\n",
    "    for term_type, word_count in type_distribution.most_common():\n",
    "        percentage = (word_count / result['green_word_count'] * 100) if result['green_word_count'] > 0 else 0\n",
    "        clean_type = term_type.replace('_', ' ').title()\n",
    "        print(f\"  {clean_type}: {word_count} words ({percentage:.1f}%)\")\n",
    "\n",
    "# Create final integrated summary table\n",
    "print(f\"\\n{'='*120}\")\n",
    "print(\"INTEGRATED SUMMARY - GREEN TERMS ANALYSIS + FREQUENCY ANALYSIS\")\n",
    "print(f\"{'='*120}\")\n",
    "\n",
    "print(f\"{'Document':<25} {'Terms':<6} {'Words':<6} {'Total':<8} {'Freq%':<7} {'Unique':<7} {'Negated':<8} {'Neg%':<6}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for doc_name in all_results.keys():\n",
    "    # Get term counts\n",
    "    valid_total = sum(all_results[doc_name]['valid_counts'].values())\n",
    "    negated_total = len(all_results[doc_name]['negated_terms'])\n",
    "    negation_pct = (negated_total / (valid_total + negated_total) * 100) if (valid_total + negated_total) > 0 else 0\n",
    "    \n",
    "    # Get frequency data\n",
    "    freq_data = frequency_results[doc_name]\n",
    "    \n",
    "    print(f\"{doc_name:<25} {valid_total:<6} {freq_data['green_word_count']:<6} {freq_data['total_words']:<8} {freq_data['green_term_frequency']:<7.2f} {freq_data['unique_green_words']:<7} {negated_total:<8} {negation_pct:<6.1f}\")\n",
    "\n",
    "# Export summary for further analysis\n",
    "frequency_summary = {\n",
    "    'analysis_metadata': {\n",
    "        'exclude_stop_words': True,\n",
    "        'overlap_resolution': True,\n",
    "        'validation_passed': all_passed,\n",
    "        'analysis_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    },\n",
    "    'document_results': frequency_results,\n",
    "    'overall_statistics': {\n",
    "        'mean_frequency': np.mean(frequencies),\n",
    "        'std_frequency': np.std(frequencies),\n",
    "        'min_frequency': min(frequencies),\n",
    "        'max_frequency': max(frequencies),\n",
    "        'total_documents': len(frequency_results),\n",
    "        'total_green_words': sum(word_counts),\n",
    "        'total_content_words': sum(total_words),\n",
    "        'overall_frequency': (sum(word_counts) / sum(total_words) * 100) if sum(total_words) > 0 else 0\n",
    "    },\n",
    "    'validation_report': validation_report\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*120}\")\n",
    "print(\"FREQUENCY ANALYSIS COMPLETE\")\n",
    "print(f\"Analysis stored in 'frequency_summary' variable\")\n",
    "print(f\"Individual document results in 'frequency_results' variable\")\n",
    "print(f\"Validation {'PASSED' if all_passed else 'ISSUES DETECTED'}\")\n",
    "print(f\"{'='*120}\")\n",
    "\n",
    "# Key findings summary\n",
    "print(f\"\\nKEY FINDINGS:\")\n",
    "highest_freq_doc = max(frequency_results.items(), key=lambda x: x[1]['green_term_frequency'])\n",
    "lowest_freq_doc = min(frequency_results.items(), key=lambda x: x[1]['green_term_frequency'])\n",
    "print(f\"Highest frequency: {highest_freq_doc[0]} ({highest_freq_doc[1]['green_term_frequency']:.3f}%)\")\n",
    "print(f\"Lowest frequency: {lowest_freq_doc[0]} ({lowest_freq_doc[1]['green_term_frequency']:.3f}%)\")\n",
    "freq_range = highest_freq_doc[1]['green_term_frequency'] - lowest_freq_doc[1]['green_term_frequency']\n",
    "print(f\"Frequency range: {freq_range:.3f} percentage points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_communication_score_dataframe(frequency_results, all_results):\n",
    "    \"\"\"\n",
    "    Create a focused DataFrame for communication score analysis.\n",
    "    Rows: Organizations (company-year combinations)\n",
    "    Columns: Key metrics for communication score\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for doc_name, freq_data in frequency_results.items():\n",
    "        # Get corresponding term analysis data\n",
    "        term_data = all_results[doc_name]\n",
    "        \n",
    "        # Extract organization and year from document name\n",
    "        parts = doc_name.split('_')\n",
    "        year = parts[-1]\n",
    "        org_name = '_'.join(parts[:-1])\n",
    "        \n",
    "        # Calculate key metrics\n",
    "        valid_terms_count = sum(term_data['valid_counts'].values())\n",
    "        negated_terms_count = len(term_data['negated_terms'])\n",
    "        total_terms_before_negation = sum(term_data['original_counts'].values())\n",
    "        negation_rate = (negated_terms_count / total_terms_before_negation * 100) if total_terms_before_negation > 0 else 0\n",
    "        \n",
    "        # Count different term types\n",
    "        noun_terms = term_data['valid_counts'].get('noun', 0)\n",
    "        multiword_noun_terms = term_data['valid_counts'].get('multiword_noun', 0)\n",
    "        adj_terms = term_data['valid_counts'].get('adjective', 0)\n",
    "        multiword_adj_terms = term_data['valid_counts'].get('multiword_adjective', 0)\n",
    "        verb_terms = term_data['valid_counts'].get('verb', 0)\n",
    "        adv_terms = term_data['valid_counts'].get('adverb', 0)\n",
    "        multiword_adv_terms = term_data['valid_counts'].get('multiword_adverb', 0)\n",
    "        context_terms = (term_data['valid_counts'].get('context_dependent_noun', 0) + \n",
    "                        term_data['valid_counts'].get('context_dependent_multiword_noun', 0))\n",
    "        dependency_terms = sum(count for pos_type, count in term_data['valid_counts'].items() \n",
    "                              if pos_type.startswith('dependency_'))\n",
    "        \n",
    "        row = {\n",
    "            'organization': org_name,\n",
    "            'year': int(year),\n",
    "            'gt_freq_pct': round(freq_data['green_term_frequency'], 3),\n",
    "            'gt_words': freq_data['green_word_count'],\n",
    "            'total_words': freq_data['total_words'],\n",
    "            'unique_gt_words': freq_data['unique_green_words'],\n",
    "            'unique_gt_lemmas': freq_data['unique_green_lemmas'],\n",
    "            'gt_terms_total': valid_terms_count,\n",
    "            'gt_terms_negated': negated_terms_count,\n",
    "            'negation_rate_pct': round(negation_rate, 2),\n",
    "            'noun_terms': noun_terms,\n",
    "            'multiword_noun_terms': multiword_noun_terms,\n",
    "            'adj_terms': adj_terms,\n",
    "            'multiword_adj_terms': multiword_adj_terms,\n",
    "            'verb_terms': verb_terms,\n",
    "            'adv_terms': adv_terms,\n",
    "            'multiword_adv_terms': multiword_adv_terms,\n",
    "            'context_dep_terms': context_terms,\n",
    "            'dependency_terms': dependency_terms,\n",
    "            'total_sentences': freq_data['total_sentences'],\n",
    "            'avg_words_per_sent': round(freq_data['total_words'] / freq_data['total_sentences'], 1) if freq_data['total_sentences'] > 0 else 0,\n",
    "            'overlaps_resolved': freq_data['overlapping_terms_resolved']\n",
    "        }\n",
    "        \n",
    "        data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create the communication score DataFrame\n",
    "comm_score_df = create_communication_score_dataframe(frequency_results, all_results)\n",
    "\n",
    "# Sort by organization and year for better readability\n",
    "comm_score_df = comm_score_df.sort_values(['organization', 'year']).reset_index(drop=True)\n",
    "\n",
    "print(\"COMMUNICATION SCORE DATAFRAME CREATED\")\n",
    "print(\"=\"*80)\n",
    "print(comm_score_df.to_string(index=False))\n",
    "\n",
    "# Export to Excel\n",
    "import os\n",
    "excel_path = \"data/NLP/Results/Communication_Score_df_Density.xlsx\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(excel_path), exist_ok=True)\n",
    "\n",
    "comm_score_df.to_excel(excel_path, index=False, sheet_name='Communication_Score')\n",
    "\n",
    "print(f\"\\nExported to Excel: {excel_path}\")\n",
    "print(f\"DataFrame shape: {comm_score_df.shape[0]} organizations × {comm_score_df.shape[1]} metrics\")\n",
    "\n",
    "# Column descriptions for reference\n",
    "print(f\"\\nCOLUMN DESCRIPTIONS:\")\n",
    "column_descriptions = {\n",
    "    'organization': 'Organization',\n",
    "    'year': 'Report year',\n",
    "    'gt_freq_pct': 'Green term frequency (%)',\n",
    "    'gt_words': 'Count of green words',\n",
    "    'total_words': 'Count total words',\n",
    "    'unique_gt_words': 'Unique green words',\n",
    "    'unique_gt_lemmas': 'Unique green lemmas',\n",
    "    'gt_terms_total': 'Total green terms found',\n",
    "    'gt_terms_negated': 'Green terms that were negated',\n",
    "    'negation_rate_pct': 'Negation rate (%)',\n",
    "    'noun_terms': 'Green noun terms',\n",
    "    'multiword_noun_terms': 'Multiword green noun terms',\n",
    "    'adj_terms': 'Green adjective terms',\n",
    "    'multiword_adj_terms': 'Multiword green adjective terms',\n",
    "    'verb_terms': 'Green verb terms',\n",
    "    'adv_terms': 'Green adverb terms',\n",
    "    'multiword_adv_terms': 'Multiword green adverb terms',\n",
    "    'context_dep_terms': 'Context-dependent terms',\n",
    "    'dependency_terms': 'Dependency pattern terms',\n",
    "    'total_sentences': 'Total sentences in document',\n",
    "    'avg_words_per_sent': 'Average words per sentence',\n",
    "    'overlaps_resolved': 'Overlapping terms resolved'\n",
    "}\n",
    "\n",
    "for col, desc in column_descriptions.items():\n",
    "    print(f\"  {col:<20}: {desc}\")\n",
    "\n",
    "print(f\"\\nData saved as: {excel_path}\")\n",
    "print(f\"Variable available as: comm_score_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c740d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Define file path and output path\n",
    "output_path = \"data/NLP/Results/Communication_Score_df_Density.xlsx\"\n",
    "\n",
    "# Save the DataFrame to Excel\n",
    "comm_score_df.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "# Load the workbook and sheet\n",
    "wb = load_workbook(output_path)\n",
    "ws = wb.active  # There's only one sheet since we saved just one DataFrame\n",
    "\n",
    "# Auto-adjust column widths based on the longest string in each column\n",
    "for col in ws.columns:\n",
    "    max_length = 0\n",
    "    col_letter = get_column_letter(col[0].column)\n",
    "    for cell in col:\n",
    "        if cell.value:\n",
    "            max_length = max(max_length, len(str(cell.value)))\n",
    "    ws.column_dimensions[col_letter].width = max_length + 3  # Add padding\n",
    "\n",
    "# Define grey fill for alternating rows\n",
    "grey_fill = PatternFill(start_color=\"D9D9D9\", end_color=\"D9D9D9\", fill_type=\"solid\")\n",
    "\n",
    "# Alternate row colors by company\n",
    "prev_company = None\n",
    "use_grey = False\n",
    "for row in range(2, ws.max_row + 1):\n",
    "    current_company = ws[f\"A{row}\"].value  # Column A has the company names\n",
    "    if current_company != prev_company:\n",
    "        use_grey = not use_grey\n",
    "        prev_company = current_company\n",
    "\n",
    "    if use_grey:\n",
    "        for col in range(1, ws.max_column + 1):\n",
    "            ws.cell(row=row, column=col).fill = grey_fill\n",
    "\n",
    "# Save the final cleaned and formatted workbook\n",
    "wb.save(output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
