{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f78cbe",
   "metadata": {},
   "source": [
    "# Vague and Hedge Language Analysis\n",
    "\n",
    "## Overview\n",
    "This module detects unclear communication patterns through vague language and hedge words analysis. It identifies language that reduces commitment specificity and increases ambiguity, directly supporting the Language Vagueness dimension of the communication assessment framework.\n",
    "\n",
    "## Hedge Words Detection\n",
    "- **Strong hedge words**: High uncertainty expressions (\"may\", \"could\", \"might\", \"potentially\", \"possibly\")\n",
    "- **Mild hedge words**: Moderate uncertainty markers (\"generally\", \"typically\", \"usually\", \"appears\", \"seems\")\n",
    "- **Sentence-level analysis**: Tracks sentences with 3+ hedge words indicating excessive uncertainty\n",
    "- **Intensity scoring**: ((strong hedge × 1.5) + (mild hedge × 1.0)) ÷ meaningful words × 100\n",
    "\n",
    "## Vague Language Categories\n",
    "- **Temporal vagueness**: \"soon\", \"eventually\", \"ongoing\", \"long-term\", \"progressively\"\n",
    "- **Scope ambiguity**: \"various\", \"multiple\", \"several\", \"certain\", \"range of\"  \n",
    "- **Commitment vagueness**: \"working towards\", \"striving for\", \"exploring\", \"considering\"\n",
    "- **Context-dependent assessment**: Some terms only count as vague when lacking specific quantification or comparison\n",
    "\n",
    "## Specialized Analysis Components\n",
    "1. **Commitment timeline analysis**: Percentage of commitment words with specific timelines vs. vague timeframes\n",
    "2. **Context-dependent word processing**: Distinguishes vague usage from quantified/compared contexts\n",
    "3. **Combined metrics**: Integrated unclear communication density across both hedge and vague categories\n",
    "\n",
    "## Variables Produced for Communication Scoring\n",
    "According to the analysis framework:\n",
    "- **Vague and Hedge Words** → Language Vagueness dimension\n",
    "- **Vague Language Intensity** → Language Vagueness dimension\n",
    "- **Hedge Language Intensity** → Language Vagueness dimension\n",
    "\n",
    "## Quality Control Features\n",
    "- **Meaningful word calculation**: Excludes stopwords, punctuation, and whitespace for accurate density metrics\n",
    "- **Category-specific tracking**: Separate analysis of temporal, scope, and commitment vagueness\n",
    "- **Intensity weighting**: Stronger vague/hedge terms receive 1.5x weight to reflect greater ambiguity impact\n",
    "\n",
    "## Theoretical Foundation\n",
    "Based on research distinguishing vague language from legitimate legal hedging, enabling detection of communication patterns that reduce accountability while maintaining plausible deniability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395af05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy_layout import spaCyLayout\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Load spaCy model and configure for large documents\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.max_length = 1_500_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2128fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Toggle between \"test\" and \"actual\"\n",
    "MODE = \"actual\"   \n",
    "\n",
    "# Define configuration based on mode\n",
    "if MODE == \"test\":\n",
    "    report_names = [ \n",
    "        \"Axpo_Holding_AG\", \"NEOEN_SA\"\n",
    "    ]\n",
    "    folders = {\n",
    "        \"2021\": Path(\"data/NLP/Testing/Reports/Clean/2021\"),\n",
    "        \"2022\": Path(\"data/NLP/Testing/Reports/Clean/2022\")\n",
    "    }\n",
    "\n",
    "elif MODE == \"actual\":\n",
    "    report_names = [ \n",
    "        \"Akenerji_Elektrik_Uretim_AS\",\n",
    "        \"Arendals_Fossekompani_ASA\",\n",
    "        \"Atlantica_Sustainable_Infrastructure_PLC\",\n",
    "        \"CEZ\",\n",
    "        \"EDF\",\n",
    "        \"EDP_Energias_de_Portugal_SA\",\n",
    "        \"Endesa\",\n",
    "        \"ERG_SpA\",\n",
    "        \"Orsted\",\n",
    "        \"Polska_Grupa_Energetyczna_PGE_SA\",\n",
    "        \"Romande_Energie_Holding_SA\",\n",
    "        \"Scatec_ASA\",\n",
    "        \"Solaria_Energia_y_Medio_Ambiente_SA\",\n",
    "        \"Terna_Energy_SA\"\n",
    "    ]\n",
    "\n",
    "    folders = {\n",
    "        \"2021\": Path(\"data/NLP/Reports/Cleanest/2021\"),\n",
    "        \"2022\": Path(\"data/NLP/Reports/Cleanest/2022\")\n",
    "    }\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid MODE. Use 'test' or 'actual'.\")\n",
    "\n",
    "# Check availability\n",
    "for name in report_names:\n",
    "    file_name = f\"{name}.txt\"\n",
    "    in_2021 = (folders[\"2021\"] / file_name).exists()\n",
    "    in_2022 = (folders[\"2022\"] / file_name).exists()\n",
    "    print(f\"{file_name}: 2021: {'YES' if in_2021 else 'NO'} | 2022: {'YES' if in_2022 else 'NO'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a05eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store processed docs\n",
    "documents = {}\n",
    "\n",
    "# Load and process all documents\n",
    "for version, folder_path in folders.items():\n",
    "    for name in report_names:\n",
    "        txt_path = folder_path / f\"{name}.txt\"\n",
    "        try:\n",
    "            with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            doc_key = f\"{name}_{version}\"\n",
    "            documents[doc_key] = nlp(text)\n",
    "            print(f\"Processed {doc_key}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {txt_path.name}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1ff7bc",
   "metadata": {},
   "source": [
    "## Hedge words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3639b5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive hedge word classifications\n",
    "HEDGE_WORDS = {\n",
    "    \"strong_hedge\": [\n",
    "        # Modal verbs expressing uncertainty\n",
    "        \"might\", \"could\", \"may\", \"would\", \"should\", \"ought\",\n",
    "        \n",
    "        # Adverbs of uncertainty (context-independent)\n",
    "        \"possibly\", \"potentially\", \"probably\", \"likely\", \"unlikely\", \"perhaps\",\n",
    "        \"maybe\", \"conceivably\", \"presumably\", \"supposedly\", \"allegedly\", \"apparently\",\n",
    "        \"seemingly\", \"arguably\", \"debatably\", \"questionably\", \"tentatively\",\n",
    "        \n",
    "        # Verbs indicating uncertainty\n",
    "        \"appears\", \"seems\", \"suggests\", \"indicates\", \"implies\", \"assumes\", \"believes\",\n",
    "        \"estimates\", \"speculates\", \"suspects\", \"expects\", \"anticipates\", \"predicts\",\n",
    "        \"presumes\", \"supposes\", \"imagines\", \"thinks\", \"feels\", \"considers\",\n",
    "        \n",
    "        # Adjectives expressing uncertainty\n",
    "        \"uncertain\", \"unclear\", \"ambiguous\", \"doubtful\", \"questionable\", \"debatable\",\n",
    "        \"controversial\", \"disputed\", \"alleged\", \"supposed\", \"presumed\", \"potential\",\n",
    "        \"possible\", \"probable\", \"speculative\", \"hypothetical\",\n",
    "        \"theoretical\", \"tentative\", \"provisional\", \"conditional\", \"contingent\",\n",
    "        \n",
    "        # Phrases and expressions\n",
    "        \"it appears\", \"it seems\", \"it suggests\", \"it indicates\", \"it implies\",\n",
    "        \"one might\", \"one could\", \"we believe\", \"we think\", \"we assume\", \"we estimate\",\n",
    "        \"tend to\", \"tends to\", \"inclined to\", \"prone to\", \"apt to\"\n",
    "    ],\n",
    "    \n",
    "    \"mild_hedge\": [\n",
    "        # Frequency adverbs (context-independent)\n",
    "        \"often\", \"commonly\", \"regularly\", \"ordinarily\", \"customarily\", \n",
    "        \"habitually\", \"routinely\", \"traditionally\", \"predominantly\", \"mainly\", \"chiefly\", \n",
    "        \"primarily\", \"principally\", \"mostly\", \"notably\", \"markedly\",\n",
    "        \n",
    "        # Degree adverbs (context-independent)\n",
    "        \"somewhat\", \"fairly\", \"quite\", \"rather\", \"reasonably\", \"moderately\",\n",
    "        \"partially\", \"partly\", \"nearly\", \"almost\", \"virtually\",\n",
    "        \n",
    "        # Multi-word expressions (context-independent)\n",
    "        \"to some extent\", \"to a degree\", \"in part\", \"in general\", \"on the whole\", \n",
    "        \"by and large\", \"for the most part\", \"more or less\", \"in essence\", \"in principle\", \n",
    "        \"broadly speaking\", \"loosely speaking\", \"generally speaking\", \"relatively speaking\",\n",
    "        \n",
    "        # Other qualifying expressions\n",
    "        \"tends to\", \"inclines toward\", \"leans toward\", \"appears to be\", \"seems to be\",\n",
    "        \"proportionally\", \"correspondingly\", \"accordingly\", \"consequently\", \"effectively\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Context-dependent hedge words that require POS and dependency analysis\n",
    "CONTEXT_DEPENDENT_HEDGE_WORDS = {\n",
    "    \"about\": {\n",
    "        \"hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\", \"amod\"]},  # \"about 5 turbines\"\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"nummod\"]},  # \"about five\"\n",
    "        ],\n",
    "        \"non_hedge_contexts\": [\n",
    "            {\"pos\": [\"ADP\"], \"dep\": [\"prep\"]},  # \"talk about something\"\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"], \"head_pos\": [\"VERB\"]},  # \"bring about change\"\n",
    "        ],\n",
    "        \"hedge_type\": \"mild\"\n",
    "    },\n",
    "    \n",
    "    \"around\": {\n",
    "        \"hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\", \"amod\"]},  # \"around 10 million\"\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"nummod\"]},  # \"around five\"\n",
    "        ],\n",
    "        \"non_hedge_contexts\": [\n",
    "            {\"pos\": [\"ADP\"], \"dep\": [\"prep\"]},  # \"around the building\"\n",
    "        ],\n",
    "        \"hedge_type\": \"mild\"\n",
    "    },\n",
    "    \n",
    "    \"roughly\": {\n",
    "        \"hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"], \"siblings_has_num\": True},  # \"roughly 50%\"\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"amod\"]},  # \"roughly equivalent\"\n",
    "        ],\n",
    "        \"non_hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"], \"head_lemma\": [\"handle\", \"treat\", \"push\"]},  # \"handle roughly\"\n",
    "        ],\n",
    "        \"hedge_type\": \"mild\"\n",
    "    },\n",
    "    \n",
    "    \"approximately\": {\n",
    "        \"hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\", \"amod\", \"nummod\"]},  # Always hedge when used as adverb\n",
    "        ],\n",
    "        \"hedge_type\": \"mild\"\n",
    "    },\n",
    "    \n",
    "    \"generally\": {\n",
    "        \"hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"], \"position\": \"sentence_start\"},  # \"Generally, we see...\"\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"], \"head_pos\": [\"VERB\", \"ADJ\"]},  # \"generally accepted\"\n",
    "        ],\n",
    "        \"non_hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"], \"head_lemma\": [\"speak\", \"refer\"]},  # \"generally speaking\"\n",
    "        ],\n",
    "        \"hedge_type\": \"mild\"\n",
    "    },\n",
    "    \n",
    "    \"typically\": {\n",
    "        \"hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"]},  # Usually hedge when modifying verbs/adjectives\n",
    "        ],\n",
    "        \"hedge_type\": \"mild\"\n",
    "    },\n",
    "    \n",
    "    \"usually\": {\n",
    "        \"hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"]},  # Usually hedge when modifying verbs/adjectives\n",
    "        ],\n",
    "        \"hedge_type\": \"mild\"\n",
    "    },\n",
    "    \n",
    "    \"normally\": {\n",
    "        \"hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"]},  # Usually hedge when modifying verbs/adjectives\n",
    "        ],\n",
    "        \"hedge_type\": \"mild\"\n",
    "    },\n",
    "    \n",
    "    \"largely\": {\n",
    "        \"hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"], \"head_pos\": [\"VERB\", \"ADJ\"]},  # \"largely responsible\"\n",
    "        ],\n",
    "        \"non_hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"], \"head_lemma\": [\"scale\", \"size\"]},  # \"largely scaled\"\n",
    "        ],\n",
    "        \"hedge_type\": \"mild\"\n",
    "    },\n",
    "    \n",
    "    \"substantially\": {\n",
    "        \"hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"], \"head_pos\": [\"VERB\", \"ADJ\"]},  # \"substantially higher\"\n",
    "        ],\n",
    "        \"hedge_type\": \"mild\"\n",
    "    },\n",
    "    \n",
    "    \"considerably\": {\n",
    "        \"hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"], \"head_pos\": [\"VERB\", \"ADJ\"]},  # \"considerably more\"\n",
    "        ],\n",
    "        \"hedge_type\": \"mild\"\n",
    "    },\n",
    "    \n",
    "    \"significantly\": {\n",
    "        \"hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"], \"head_pos\": [\"VERB\", \"ADJ\"]},  # \"significantly higher\"\n",
    "        ],\n",
    "        \"hedge_type\": \"mild\"\n",
    "    },\n",
    "    \n",
    "    \"relatively\": {\n",
    "        \"hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"], \"head_pos\": [\"ADJ\", \"ADV\"]},  # \"relatively small\"\n",
    "        ],\n",
    "        \"non_hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"], \"head_lemma\": [\"relate\", \"compare\"]},  # \"relatively speaking\"\n",
    "        ],\n",
    "        \"hedge_type\": \"mild\"\n",
    "    },\n",
    "    \n",
    "    \"practically\": {\n",
    "        \"hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"], \"head_pos\": [\"ADJ\", \"ADV\"]},  # \"practically impossible\"\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"], \"head_lemma\": [\"eliminate\", \"zero\", \"nothing\"]},  # \"practically zero\"\n",
    "        ],\n",
    "        \"non_hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"], \"head_pos\": [\"VERB\"]},  # \"practically implement\"\n",
    "        ],\n",
    "        \"hedge_type\": \"mild\"\n",
    "    },\n",
    "    \n",
    "    \"essentially\": {\n",
    "        \"hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"], \"head_pos\": [\"ADJ\", \"VERB\"]},  # \"essentially the same\"\n",
    "        ],\n",
    "        \"hedge_type\": \"mild\"\n",
    "    },\n",
    "    \n",
    "    \"basically\": {\n",
    "        \"hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"], \"head_pos\": [\"ADJ\", \"VERB\"]},  # \"basically correct\"\n",
    "        ],\n",
    "        \"hedge_type\": \"mild\"\n",
    "    },\n",
    "    \n",
    "    \"fundamentally\": {\n",
    "        \"hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"], \"head_pos\": [\"ADJ\", \"VERB\"]},  # \"fundamentally different\"\n",
    "        ],\n",
    "        \"non_hedge_contexts\": [\n",
    "            {\"pos\": [\"ADV\"], \"dep\": [\"advmod\"], \"head_lemma\": [\"based\", \"rooted\"]},  # \"fundamentally based\"\n",
    "        ],\n",
    "        \"hedge_type\": \"mild\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Combined hedge word set for quick lookup (includes context-dependent words)\n",
    "ALL_HEDGE_WORDS = set(HEDGE_WORDS[\"strong_hedge\"] + HEDGE_WORDS[\"mild_hedge\"] + list(CONTEXT_DEPENDENT_HEDGE_WORDS.keys()))\n",
    "\n",
    "print(f\"Hedge word dictionaries loaded:\")\n",
    "print(f\"Strong hedge words: {len(HEDGE_WORDS['strong_hedge'])}\")\n",
    "print(f\"Mild hedge words: {len(HEDGE_WORDS['mild_hedge'])}\")\n",
    "print(f\"Context-dependent hedge words: {len(CONTEXT_DEPENDENT_HEDGE_WORDS)}\")\n",
    "print(f\"Total hedge words: {len(ALL_HEDGE_WORDS)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cdd38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_meaningful_words(doc):\n",
    "    \"\"\"\n",
    "    Count meaningful words excluding stopwords, punctuation, and whitespace.\n",
    "    Uses spaCy's built-in stopword detection.\n",
    "    \"\"\"\n",
    "    meaningful_count = 0\n",
    "    \n",
    "    for token in doc:\n",
    "        if (not token.is_punct and \n",
    "            not token.is_space and \n",
    "            not token.is_stop and\n",
    "            len(token.text.strip()) > 0):\n",
    "            meaningful_count += 1\n",
    "    \n",
    "    return meaningful_count\n",
    "\n",
    "def get_token_lemma_lower(token):\n",
    "    \"\"\"Get lowercase lemma of token for consistent matching.\"\"\"\n",
    "    return token.lemma_.lower().strip()\n",
    "\n",
    "def check_context_dependent_hedge(token):\n",
    "    \"\"\"\n",
    "    Check if a context-dependent word is being used as a hedge word based on linguistic context.\n",
    "    \n",
    "    Args:\n",
    "        token: spaCy token object\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (is_hedge, hedge_type) or (False, None)\n",
    "    \"\"\"\n",
    "    lemma = token.lemma_.lower()\n",
    "    \n",
    "    if lemma not in CONTEXT_DEPENDENT_HEDGE_WORDS:\n",
    "        return False, None\n",
    "    \n",
    "    word_config = CONTEXT_DEPENDENT_HEDGE_WORDS[lemma]\n",
    "    hedge_type = word_config[\"hedge_type\"]\n",
    "    \n",
    "    # Check hedge contexts\n",
    "    for context in word_config.get(\"hedge_contexts\", []):\n",
    "        if matches_context(token, context):\n",
    "            return True, hedge_type\n",
    "    \n",
    "    # Check non-hedge contexts (if matches, it's NOT a hedge)\n",
    "    for context in word_config.get(\"non_hedge_contexts\", []):\n",
    "        if matches_context(token, context):\n",
    "            return False, None\n",
    "    \n",
    "    # If no specific context matched, default to hedge (conservative approach)\n",
    "    return True, hedge_type\n",
    "\n",
    "def matches_context(token, context):\n",
    "    \"\"\"\n",
    "    Check if a token matches a specific linguistic context.\n",
    "    \n",
    "    Args:\n",
    "        token: spaCy token object\n",
    "        context: Dictionary with context criteria\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if token matches the context\n",
    "    \"\"\"\n",
    "    # Check POS tag\n",
    "    if \"pos\" in context:\n",
    "        if token.pos_ not in context[\"pos\"]:\n",
    "            return False\n",
    "    \n",
    "    # Check dependency relation\n",
    "    if \"dep\" in context:\n",
    "        if token.dep_ not in context[\"dep\"]:\n",
    "            return False\n",
    "    \n",
    "    # Check head word POS\n",
    "    if \"head_pos\" in context:\n",
    "        if token.head.pos_ not in context[\"head_pos\"]:\n",
    "            return False\n",
    "    \n",
    "    # Check head word lemma\n",
    "    if \"head_lemma\" in context:\n",
    "        if token.head.lemma_.lower() not in context[\"head_lemma\"]:\n",
    "            return False\n",
    "    \n",
    "    # Check if siblings contain numbers (for approximation words)\n",
    "    if context.get(\"siblings_has_num\", False):\n",
    "        has_number_sibling = False\n",
    "        for child in token.head.children:\n",
    "            if child.pos_ == \"NUM\" or child.like_num:\n",
    "                has_number_sibling = True\n",
    "                break\n",
    "        if not has_number_sibling:\n",
    "            return False\n",
    "    \n",
    "    # Check position in sentence\n",
    "    if \"position\" in context:\n",
    "        if context[\"position\"] == \"sentence_start\":\n",
    "            # Check if token is within first 3 tokens of sentence\n",
    "            sent_start = token.sent.start\n",
    "            if token.i - sent_start > 2:\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def is_hedge_word(lemma, hedge_type=None, token=None):\n",
    "    \"\"\"\n",
    "    Check if a lemma is a hedge word, considering context for ambiguous words.\n",
    "    \n",
    "    Args:\n",
    "        lemma: lowercase lemma to check\n",
    "        hedge_type: 'strong', 'mild', or None (for any type)\n",
    "        token: spaCy token object (needed for context-dependent words)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (is_hedge, hedge_type) or (False, None) if not a hedge word\n",
    "    \"\"\"\n",
    "    # First check context-dependent words if token is provided\n",
    "    if token is not None and lemma in CONTEXT_DEPENDENT_HEDGE_WORDS:\n",
    "        return check_context_dependent_hedge(token)\n",
    "    \n",
    "    # Then check regular hedge words\n",
    "    if hedge_type == \"strong\":\n",
    "        return (True, \"strong\") if lemma in HEDGE_WORDS[\"strong_hedge\"] else (False, None)\n",
    "    elif hedge_type == \"mild\":\n",
    "        return (True, \"mild\") if lemma in HEDGE_WORDS[\"mild_hedge\"] else (False, None)\n",
    "    else:\n",
    "        # Check both types\n",
    "        if lemma in HEDGE_WORDS[\"strong_hedge\"]:\n",
    "            return (True, \"strong\")\n",
    "        elif lemma in HEDGE_WORDS[\"mild_hedge\"]:\n",
    "            return (True, \"mild\")\n",
    "        else:\n",
    "            return (False, None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd60ed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hedge_words_in_document(doc, document_name):\n",
    "    \"\"\"\n",
    "    Find all hedge words in a document with their context and sentence information.\n",
    "    Uses contextual analysis for ambiguous words.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Complete hedge word analysis for the document\n",
    "    \"\"\"\n",
    "    hedge_analysis = {\n",
    "        'document_name': document_name,\n",
    "        'strong_hedge_words': [],\n",
    "        'mild_hedge_words': [],\n",
    "        'strong_count': 0,\n",
    "        'mild_count': 0,\n",
    "        'total_hedge_count': 0,\n",
    "        'meaningful_words': 0,\n",
    "        'sentences_with_hedges': [],\n",
    "        'high_hedge_sentences': []\n",
    "    }\n",
    "    \n",
    "    # Count meaningful words in document\n",
    "    hedge_analysis['meaningful_words'] = count_meaningful_words(doc)\n",
    "    \n",
    "    # Track sentences containing hedge words\n",
    "    sentence_hedge_counts = {}\n",
    "    \n",
    "    # Process each token for hedge words\n",
    "    for token in doc:\n",
    "        if token.is_punct or token.is_space or len(token.text.strip()) < 2:\n",
    "            continue\n",
    "            \n",
    "        lemma = get_token_lemma_lower(token)\n",
    "        \n",
    "        # Check for hedge words with contextual analysis\n",
    "        is_hedge, hedge_type = is_hedge_word(lemma, token=token)\n",
    "        \n",
    "        if is_hedge:\n",
    "            # Get sentence context\n",
    "            sent_text = token.sent.text.strip()\n",
    "            sent_start = token.sent.start\n",
    "            \n",
    "            # Track hedge words per sentence\n",
    "            if sent_start not in sentence_hedge_counts:\n",
    "                sentence_hedge_counts[sent_start] = {\n",
    "                    'text': sent_text, \n",
    "                    'count': 0, \n",
    "                    'hedge_words': []\n",
    "                }\n",
    "            sentence_hedge_counts[sent_start]['count'] += 1\n",
    "            sentence_hedge_counts[sent_start]['hedge_words'].append(lemma)\n",
    "            \n",
    "            # Create hedge word information\n",
    "            hedge_info = {\n",
    "                'token': token.text,\n",
    "                'lemma': lemma,\n",
    "                'position': token.i,\n",
    "                'pos': token.pos_,\n",
    "                'dep': token.dep_,\n",
    "                'sentence_start': sent_start,\n",
    "                'sentence_text': sent_text[:200] + \"...\" if len(sent_text) > 200 else sent_text,\n",
    "                'context_before': doc[max(0, token.i-3):token.i].text,\n",
    "                'context_after': doc[token.i+1:min(len(doc), token.i+4)].text\n",
    "            }\n",
    "            \n",
    "            # Categorize by hedge type\n",
    "            if hedge_type == \"strong\":\n",
    "                hedge_analysis['strong_hedge_words'].append(hedge_info)\n",
    "                hedge_analysis['strong_count'] += 1\n",
    "            else:\n",
    "                hedge_analysis['mild_hedge_words'].append(hedge_info)\n",
    "                hedge_analysis['mild_count'] += 1\n",
    "    \n",
    "    # Calculate total hedge count\n",
    "    hedge_analysis['total_hedge_count'] = hedge_analysis['strong_count'] + hedge_analysis['mild_count']\n",
    "    \n",
    "    # Process sentences with hedge words\n",
    "    hedge_analysis['sentences_with_hedges'] = [\n",
    "        {\n",
    "            'sentence': info['text'],\n",
    "            'hedge_count': info['count'],\n",
    "            'hedge_words': info['hedge_words']\n",
    "        }\n",
    "        for info in sentence_hedge_counts.values()\n",
    "        if info['count'] > 0\n",
    "    ]\n",
    "    \n",
    "    # Identify high-hedge sentences (3+ hedge words)\n",
    "    hedge_analysis['high_hedge_sentences'] = [\n",
    "        sentence_info for sentence_info in hedge_analysis['sentences_with_hedges']\n",
    "        if sentence_info['hedge_count'] >= 3\n",
    "    ]\n",
    "    \n",
    "    return hedge_analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec714f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hedge_densities(hedge_analysis):\n",
    "    \"\"\"\n",
    "    Calculate hedge word density metrics and ratios.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Density calculations as percentages\n",
    "    \"\"\"\n",
    "    meaningful_words = hedge_analysis['meaningful_words']\n",
    "    strong_count = hedge_analysis['strong_count']\n",
    "    mild_count = hedge_analysis['mild_count']\n",
    "    total_hedge_count = hedge_analysis['total_hedge_count']\n",
    "    \n",
    "    # Handle zero meaningful words\n",
    "    if meaningful_words == 0:\n",
    "        return {\n",
    "            'total_hedge_density': 0.0,\n",
    "            'strong_hedge_density': 0.0,\n",
    "            'mild_hedge_density': 0.0,\n",
    "            'strong_vs_mild_ratio': 0.0,\n",
    "            'hedge_intensity_score': 0.0,\n",
    "            'sentences_with_hedges_pct': 0.0\n",
    "        }\n",
    "    \n",
    "    # Calculate basic densities (as percentages)\n",
    "    total_density = (total_hedge_count / meaningful_words) * 100\n",
    "    strong_density = (strong_count / meaningful_words) * 100\n",
    "    mild_density = (mild_count / meaningful_words) * 100\n",
    "    \n",
    "    # Calculate strong vs mild ratio\n",
    "    if mild_count > 0:\n",
    "        strong_vs_mild_ratio = strong_count / mild_count\n",
    "    else:\n",
    "        strong_vs_mild_ratio = float('inf') if strong_count > 0 else 0.0\n",
    "    \n",
    "    # Calculate weighted hedge intensity score (strong hedges weighted 1.5x)\n",
    "    hedge_intensity_score = (((strong_count * 1.5) + (mild_count * 1)) / meaningful_words) * 100\n",
    "    \n",
    "    # Estimate percentage of sentences with hedges\n",
    "    total_sentences = len(set(hw['sentence_start'] for hw in \n",
    "                             hedge_analysis['strong_hedge_words'] + hedge_analysis['mild_hedge_words']))\n",
    "    estimated_total_sentences = max(total_sentences, meaningful_words // 15)\n",
    "    sentences_with_hedges_pct = (len(hedge_analysis['sentences_with_hedges']) / estimated_total_sentences) * 100\n",
    "    \n",
    "    return {\n",
    "        'total_hedge_density': round(total_density, 4),\n",
    "        'strong_hedge_density': round(strong_density, 4),\n",
    "        'mild_hedge_density': round(mild_density, 4),\n",
    "        'strong_vs_mild_ratio': round(strong_vs_mild_ratio, 4) if strong_vs_mild_ratio != float('inf') else strong_vs_mild_ratio,\n",
    "        'hedge_intensity_score': round(hedge_intensity_score, 4),\n",
    "        'sentences_with_hedges_pct': round(sentences_with_hedges_pct, 2)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd292297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_hedge_words_all_documents(documents):\n",
    "    \"\"\"\n",
    "    Analyze hedge words across all documents and calculate density statistics.\n",
    "    \n",
    "    Args:\n",
    "        documents: Dictionary of {doc_name: spacy_doc}\n",
    "    \n",
    "    Returns:\n",
    "        dict: Complete hedge word analysis results\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "    density_stats = {\n",
    "        'total_documents': len(documents),\n",
    "        'documents_with_high_hedging': [],\n",
    "        'documents_with_low_hedging': []\n",
    "    }\n",
    "    \n",
    "    # Lists to collect density values for statistics\n",
    "    total_densities = []\n",
    "    strong_densities = []\n",
    "    mild_densities = []\n",
    "    \n",
    "    # Process each document\n",
    "    for doc_name, doc in documents.items():\n",
    "        # Analyze hedge words in document\n",
    "        hedge_analysis = find_hedge_words_in_document(doc, doc_name)\n",
    "        \n",
    "        # Calculate densities\n",
    "        densities = calculate_hedge_densities(hedge_analysis)\n",
    "        \n",
    "        # Combine results\n",
    "        document_result = {**hedge_analysis, **densities}\n",
    "        all_results[doc_name] = document_result\n",
    "        \n",
    "        # Collect density values for statistics\n",
    "        total_densities.append(densities['total_hedge_density'])\n",
    "        strong_densities.append(densities['strong_hedge_density'])\n",
    "        mild_densities.append(densities['mild_hedge_density'])\n",
    "        \n",
    "        # Categorize documents by hedging level\n",
    "        total_density = densities['total_hedge_density']\n",
    "        if total_density > 1.0:\n",
    "            density_stats['documents_with_high_hedging'].append((doc_name, total_density))\n",
    "        elif total_density < 0.5:\n",
    "            density_stats['documents_with_low_hedging'].append((doc_name, total_density))\n",
    "    \n",
    "    # Calculate density statistics across all documents\n",
    "    if total_densities:\n",
    "        density_stats.update({\n",
    "            # Average densities\n",
    "            'average_total_hedge_density': round(sum(total_densities) / len(total_densities), 4),\n",
    "            'average_strong_hedge_density': round(sum(strong_densities) / len(strong_densities), 4),\n",
    "            'average_mild_hedge_density': round(sum(mild_densities) / len(mild_densities), 4),\n",
    "            \n",
    "            # Min/Max densities\n",
    "            'min_total_hedge_density': round(min(total_densities), 4),\n",
    "            'max_total_hedge_density': round(max(total_densities), 4),\n",
    "            'min_strong_hedge_density': round(min(strong_densities), 4),\n",
    "            'max_strong_hedge_density': round(max(strong_densities), 4),\n",
    "            'min_mild_hedge_density': round(min(mild_densities), 4),\n",
    "            'max_mild_hedge_density': round(max(mild_densities), 4),\n",
    "            \n",
    "            # Density ranges\n",
    "            'range_total_hedge_density': round(max(total_densities) - min(total_densities), 4),\n",
    "            'range_strong_hedge_density': round(max(strong_densities) - min(strong_densities), 4),\n",
    "            'range_mild_hedge_density': round(max(mild_densities) - min(mild_densities), 4)\n",
    "        })\n",
    "    \n",
    "    # Sort documents by hedging level\n",
    "    density_stats['documents_with_high_hedging'].sort(key=lambda x: x[1], reverse=True)\n",
    "    density_stats['documents_with_low_hedging'].sort(key=lambda x: x[1])\n",
    "    \n",
    "    return {\n",
    "        'document_results': all_results,\n",
    "        'density_statistics': density_stats\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a305e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hedge_summary_table(analysis_results):\n",
    "    \"\"\"Create pandas DataFrame summarizing hedge analysis results.\"\"\"\n",
    "    document_results = analysis_results['document_results']\n",
    "    \n",
    "    summary_data = []\n",
    "    for doc_name, results in document_results.items():\n",
    "        summary_data.append({\n",
    "            'Document': doc_name,\n",
    "            'Meaningful Words': results['meaningful_words'],\n",
    "            'Total Hedge Words': results['total_hedge_count'],\n",
    "            'Strong Hedge Words': results['strong_count'],\n",
    "            'Mild Hedge Words': results['mild_count'],\n",
    "            'Total Hedge Density (%)': results['total_hedge_density'],\n",
    "            'Strong Hedge Density (%)': results['strong_hedge_density'],\n",
    "            'Mild Hedge Density (%)': results['mild_hedge_density'],\n",
    "            'Hedge Intensity Score': results['hedge_intensity_score'],\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(summary_data)\n",
    "    return df #.sort_values('Total Hedge Density (%)', ascending=False)\n",
    "\n",
    "def display_hedge_analysis_results(analysis_results):\n",
    "    \"\"\"Display comprehensive hedge analysis results.\"\"\"\n",
    "    density_stats = analysis_results['density_statistics']\n",
    "    \n",
    "    print(\"HEDGE WORD ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nBASIC STATISTICS:\")\n",
    "    print(f\"Total Documents: {density_stats['total_documents']}\")\n",
    "    \n",
    "    # Density statistics across all documents\n",
    "    print(f\"\\nDENSITY STATISTICS ACROSS ALL DOCUMENTS:\")\n",
    "    print(f\"Average Total Hedge Density: {density_stats['average_total_hedge_density']:.4f}%\")\n",
    "    print(f\"Average Strong Hedge Density: {density_stats['average_strong_hedge_density']:.4f}%\")\n",
    "    print(f\"Average Mild Hedge Density: {density_stats['average_mild_hedge_density']:.4f}%\")\n",
    "    \n",
    "    print(f\"\\nDENSITY RANGES:\")\n",
    "    print(f\"Total Hedge Density Range: {density_stats['min_total_hedge_density']:.4f}% - {density_stats['max_total_hedge_density']:.4f}% (range: {density_stats['range_total_hedge_density']:.4f}%)\")\n",
    "    print(f\"Strong Hedge Density Range: {density_stats['min_strong_hedge_density']:.4f}% - {density_stats['max_strong_hedge_density']:.4f}% (range: {density_stats['range_strong_hedge_density']:.4f}%)\")\n",
    "    print(f\"Mild Hedge Density Range: {density_stats['min_mild_hedge_density']:.4f}% - {density_stats['max_mild_hedge_density']:.4f}% (range: {density_stats['range_mild_hedge_density']:.4f}%)\")\n",
    "    \n",
    "    # High and low hedging documents\n",
    "    if density_stats['documents_with_high_hedging']:\n",
    "        print(f\"\\nHIGH HEDGING DOCUMENTS (>1% density):\")\n",
    "        for doc_name, density in density_stats['documents_with_high_hedging'][:5]:\n",
    "            print(f\"  {doc_name}: {density:.4f}%\")\n",
    "    \n",
    "    if density_stats['documents_with_low_hedging']:\n",
    "        print(f\"\\nLOW HEDGING DOCUMENTS (<0.5% density):\")\n",
    "        for doc_name, density in density_stats['documents_with_low_hedging'][:5]:\n",
    "            print(f\"  {doc_name}: {density:.4f}%\")\n",
    "    \n",
    "    # Summary table\n",
    "    print(f\"\\nDOCUMENT SUMMARY TABLE:\")\n",
    "    summary_df = create_hedge_summary_table(analysis_results)\n",
    "    print(summary_df.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "def show_hedge_examples(analysis_results, document_name, max_examples=5):\n",
    "    \"\"\"Show specific hedge word examples from a document with contextual information.\"\"\"\n",
    "    if document_name not in analysis_results['document_results']:\n",
    "        print(f\"Document '{document_name}' not found in results.\")\n",
    "        return\n",
    "    \n",
    "    results = analysis_results['document_results'][document_name]\n",
    "    \n",
    "    print(f\"\\nHEDGE WORD EXAMPLES FROM: {document_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Strong hedge examples\n",
    "    if results['strong_hedge_words']:\n",
    "        print(f\"\\nSTRONG HEDGE WORDS ({len(results['strong_hedge_words'])} total):\")\n",
    "        for i, hedge in enumerate(results['strong_hedge_words'][:max_examples]):\n",
    "            context_info = \"\"\n",
    "            if hedge['lemma'] in CONTEXT_DEPENDENT_HEDGE_WORDS:\n",
    "                context_info = f\" [POS: {hedge['pos']}, DEP: {hedge['dep']}]\"\n",
    "            print(f\"{i+1}. '{hedge['token']}' (lemma: {hedge['lemma']}){context_info}\")\n",
    "            print(f\"   Context: ...{hedge['context_before']} [{hedge['token']}] {hedge['context_after']}...\")\n",
    "            print()\n",
    "    \n",
    "    # Mild hedge examples  \n",
    "    if results['mild_hedge_words']:\n",
    "        print(f\"\\nMILD HEDGE WORDS ({len(results['mild_hedge_words'])} total):\")\n",
    "        for i, hedge in enumerate(results['mild_hedge_words'][:max_examples]):\n",
    "            context_info = \"\"\n",
    "            if hedge['lemma'] in CONTEXT_DEPENDENT_HEDGE_WORDS:\n",
    "                context_info = f\" [POS: {hedge['pos']}, DEP: {hedge['dep']}]\"\n",
    "            print(f\"{i+1}. '{hedge['token']}' (lemma: {hedge['lemma']}){context_info}\")\n",
    "            print(f\"   Context: ...{hedge['context_before']} [{hedge['token']}] {hedge['context_after']}...\")\n",
    "            print()\n",
    "    \n",
    "    # High-hedge sentences\n",
    "    if results['high_hedge_sentences']:\n",
    "        print(f\"\\nHIGH-HEDGE SENTENCES ({len(results['high_hedge_sentences'])} total):\")\n",
    "        for i, sent in enumerate(results['high_hedge_sentences'][:3]):\n",
    "            print(f\"{i+1}. Hedge words ({sent['hedge_count']}): {', '.join(sent['hedge_words'])}\")\n",
    "            print(f\"   Sentence: {sent['sentence'][:300]}{'...' if len(sent['sentence']) > 300 else ''}\")\n",
    "            print()\n",
    "\n",
    "def find_context_dependent_examples(doc, max_true=2, max_false=2):\n",
    "    \"\"\"\n",
    "    Find examples of context-dependent hedge words - both true hedges and false hedges.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {'true_hedges': [...], 'false_hedges': [...]}\n",
    "    \"\"\"\n",
    "    true_examples = []\n",
    "    false_examples = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.is_punct or token.is_space or len(token.text.strip()) < 2:\n",
    "            continue\n",
    "            \n",
    "        lemma = get_token_lemma_lower(token)\n",
    "        \n",
    "        # Check if it's a context-dependent word\n",
    "        if lemma in CONTEXT_DEPENDENT_HEDGE_WORDS:\n",
    "            is_hedge, hedge_type = is_hedge_word(lemma, token=token)\n",
    "            \n",
    "            # Create example info\n",
    "            example_info = {\n",
    "                'token': token.text,\n",
    "                'lemma': lemma,\n",
    "                'pos': token.pos_,\n",
    "                'dep': token.dep_,\n",
    "                'context': token.sent.text.strip()[:150] + \"...\" if len(token.sent.text) > 150 else token.sent.text.strip()\n",
    "            }\n",
    "            \n",
    "            if is_hedge and len(true_examples) < max_true:\n",
    "                true_examples.append(example_info)\n",
    "            elif not is_hedge and len(false_examples) < max_false:\n",
    "                false_examples.append(example_info)\n",
    "            \n",
    "            # Stop if we have enough examples\n",
    "            if len(true_examples) >= max_true and len(false_examples) >= max_false:\n",
    "                break\n",
    "    \n",
    "    return {'true_hedges': true_examples, 'false_hedges': false_examples}\n",
    "\n",
    "def show_context_dependent_examples(doc, document_name):\n",
    "    \"\"\"Show context-dependent hedge word examples with brief definition.\"\"\"\n",
    "    examples = find_context_dependent_examples(doc)\n",
    "    \n",
    "    if not examples['true_hedges'] and not examples['false_hedges']:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nCONTEXT-DEPENDENT HEDGE WORDS:\")\n",
    "    print(\"Context-dependent words can be hedges or not depending on grammatical usage.\")\n",
    "    \n",
    "    # True hedge examples\n",
    "    if examples['true_hedges']:\n",
    "        print(f\"\\nWords functioning as HEDGES in context:\")\n",
    "        for i, ex in enumerate(examples['true_hedges'], 1):\n",
    "            print(f\"{i}. '{ex['token']}' (POS: {ex['pos']}, DEP: {ex['dep']})\")\n",
    "            print(f\"   Context: {ex['context']}\")\n",
    "            print()\n",
    "    \n",
    "    # False hedge examples  \n",
    "    if examples['false_hedges']:\n",
    "        print(f\"Words NOT functioning as hedges in context:\")\n",
    "        for i, ex in enumerate(examples['false_hedges'], 1):\n",
    "            print(f\"{i}. '{ex['token']}' (POS: {ex['pos']}, DEP: {ex['dep']})\")\n",
    "            print(f\"   Context: {ex['context']}\")\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the hedge word analysis\n",
    "print(\"Starting hedge word analysis...\")\n",
    "hedge_results = analyze_hedge_words_all_documents(documents)\n",
    "\n",
    "# Show examples from most hedged document\n",
    "if hedge_results:\n",
    "    # Get the document with highest hedge density from all documents\n",
    "    document_results = hedge_results['document_results']\n",
    "    if document_results:\n",
    "        # Find document with highest total hedge density\n",
    "        most_hedged_doc = max(document_results.items(), key=lambda x: x[1]['total_hedge_density'])[0]\n",
    "        \n",
    "        # Show hedge examples from the most hedged document\n",
    "        print(f\"\\nHEDGE LANGUAGE EXAMPLES FROM: {most_hedged_doc}\")\n",
    "        print(\"=\" * 50)\n",
    "        show_hedge_examples(hedge_results, most_hedged_doc, max_examples=10)\n",
    "        \n",
    "        # Add context-dependent examples if the document exists in documents variable\n",
    "        if 'documents' in globals() and most_hedged_doc in documents:\n",
    "            show_context_dependent_examples(documents[most_hedged_doc], most_hedged_doc)\n",
    "\n",
    "# Display results\n",
    "summary_table = display_hedge_analysis_results(hedge_results)\n",
    "\n",
    "print(f\"\\nHedge word analysis complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d16a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples from most and least hedged documents\n",
    "density_stats = hedge_results['density_statistics']\n",
    "if density_stats['documents_with_high_hedging']:\n",
    "    most_hedged_doc = density_stats['documents_with_high_hedging'][0][0]\n",
    "    print(f\"\\nEXAMPLES FROM MOST HEDGED DOCUMENT\")\n",
    "    print(\"=\" * 50)\n",
    "    show_hedge_examples(hedge_results, most_hedged_doc, max_examples=3)\n",
    "    \n",
    "    # Add context-dependent examples\n",
    "    if most_hedged_doc in documents:\n",
    "        show_context_dependent_examples(documents[most_hedged_doc], most_hedged_doc)\n",
    "\n",
    "if density_stats['documents_with_low_hedging']:\n",
    "    least_hedged_doc = density_stats['documents_with_low_hedging'][0][0]\n",
    "    print(f\"\\nEXAMPLES FROM LEAST HEDGED DOCUMENT\")\n",
    "    print(\"=\" * 50)\n",
    "    show_hedge_examples(hedge_results, least_hedged_doc, max_examples=3)\n",
    "    \n",
    "    # Add context-dependent examples\n",
    "    if least_hedged_doc in documents:\n",
    "        show_context_dependent_examples(documents[least_hedged_doc], least_hedged_doc)\n",
    "\n",
    "# Document rankings by different metrics\n",
    "print(f\"\\nDOCUMENT RANKINGS BY HEDGE METRICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Top 5 most hedged documents\n",
    "print(\"\\nTOP 5 MOST HEDGED DOCUMENTS (by total density):\")\n",
    "top_hedged = summary_table.nlargest(5, 'Total Hedge Density (%)')\n",
    "for idx, (_, row) in enumerate(top_hedged.iterrows(), 1):\n",
    "    print(f\"{idx}. {row['Document']}: {row['Total Hedge Density (%)']:.4f}%\")\n",
    "\n",
    "# Top 5 strongest hedging documents\n",
    "print(\"\\nTOP 5 DOCUMENTS WITH STRONGEST HEDGING:\")\n",
    "top_strong = summary_table.nlargest(5, 'Strong Hedge Density (%)')\n",
    "for idx, (_, row) in enumerate(top_strong.iterrows(), 1):\n",
    "    print(f\"{idx}. {row['Document']}: {row['Strong Hedge Density (%)']:.4f}%\")\n",
    "\n",
    "# Top 5 by hedge intensity score\n",
    "print(\"\\nTOP 5 DOCUMENTS BY HEDGE INTENSITY SCORE:\")\n",
    "top_intensity = summary_table.nlargest(5, 'Hedge Intensity Score')\n",
    "for idx, (_, row) in enumerate(top_intensity.iterrows(), 1):\n",
    "    print(f\"{idx}. {row['Document']}: {row['Hedge Intensity Score']:.4f}\")\n",
    "\n",
    "print(f\"\\nComplete hedge word analysis finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89877a60",
   "metadata": {},
   "source": [
    "## Vague language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03066156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strong Vague Language Categories - high ambiguity terms\n",
    "STRONG_VAGUE_LANGUAGE = {\n",
    "    'temporal_vagueness': [\n",
    "        'soon', 'eventually', 'in the future', 'ongoing', 'continuously', \n",
    "        'progressively', 'increasingly', 'gradually', 'over time', 'long-term',\n",
    "        'short-term', 'medium-term', 'ultimately', 'periodically', 'shortly',\n",
    "        'presently', 'currently', 'lately', 'recently', 'frequently'\n",
    "    ],\n",
    "    'scope_ambiguity': [\n",
    "        'various', 'multiple', 'several', 'numerous', 'many', 'some', \n",
    "        'certain', 'particular', 'diverse', 'wide range', 'broad spectrum',\n",
    "        'variety of', 'range of', 'selection of', 'array of', 'number of'\n",
    "    ],\n",
    "    'commitment_vagueness': [\n",
    "    # Original terms\n",
    "    'working towards', 'striving for', 'aiming to', 'seeking to',\n",
    "    'endeavoring', 'exploring', 'investigating', 'considering',\n",
    "    'evaluating', 'looking into', 'planning to', 'intending to',\n",
    "    'committed to', 'dedicated to', 'focused on', 'pursuing',\n",
    "    'attempting', 'trying to', 'hoping to',\n",
    "    \n",
    "    # Vague action-oriented commitments (kept the vague ones)\n",
    "    'working on', 'developing', 'implementing', 'establishing',\n",
    "    'advancing', 'driving', 'promoting', 'facilitating',\n",
    "    'supporting', 'enabling', 'fostering', 'encouraging',\n",
    "    \n",
    "    # Progress-oriented terms (kept vague progress terms)\n",
    "    'moving towards', 'progressing towards', 'heading towards',\n",
    "    'improving', 'enhancing', 'strengthening', 'optimizing',\n",
    "    'transforming',\n",
    "    \n",
    "    # Preparation terms (kept preparation-related commitments)\n",
    "    'preparing', 'preparing for', 'getting ready', 'setting up',\n",
    "    \n",
    "    # Future-oriented vague terms\n",
    "    'will work on', 'will develop', 'will implement', 'will establish',\n",
    "    'will improve', 'will enhance', 'will strengthen',\n",
    "    'continue to', 'ongoing efforts', 'future plans', 'next steps'\n",
    "],\n",
    "    'degree_vagueness': [\n",
    "        'significant', 'substantial', 'considerable', 'meaningful', \n",
    "        'notable', 'impressive', 'major', 'minor', 'moderate', \n",
    "        'extensive', 'comprehensive', 'robust', 'strong', 'weak',\n",
    "        'dramatic', 'marked', 'pronounced', 'modest', 'limited'\n",
    "    ]\n",
    "    \n",
    "}\n",
    "\n",
    "# Mild Vague Language Categories - moderate ambiguity terms\n",
    "MILD_VAGUE_LANGUAGE = {\n",
    "    'relative_terms': [\n",
    "        'better', 'improved', 'enhanced', 'upgraded', 'advanced', \n",
    "        'superior', 'increased', 'reduced', 'higher', 'lower',\n",
    "        'greater', 'lesser', 'faster', 'slower', 'more', 'less',\n",
    "        'newer', 'older', 'larger', 'smaller', 'wider', 'narrower'\n",
    "    ],\n",
    "    'general_descriptors': [\n",
    "        'appropriate', 'suitable', 'relevant', 'effective', 'efficient', \n",
    "        'optimal', 'adequate', 'proper', 'reasonable', 'acceptable',\n",
    "        'satisfactory', 'desirable', 'favorable', 'beneficial',\n",
    "        'valuable', 'useful', 'practical', 'viable', 'feasible'\n",
    "    ],\n",
    "    'process_vagueness': [\n",
    "        'initiatives', 'measures', 'efforts', 'activities', 'actions', \n",
    "        'approaches', 'solutions', 'methods', 'techniques', 'procedures', \n",
    "        'processes', 'operations', 'practices', 'mechanisms', 'frameworks', 'systems'\n",
    "    ],\n",
    "    'outcome_vagueness': [\n",
    "        'positive impact', 'improvement', 'enhancement', 'optimization', \n",
    "        'advancement', 'progress', 'benefits', 'success',\n",
    "        'achievement', 'development', 'growth', 'innovation',\n",
    "        'transformation', 'breakthrough', 'gains', 'advancement'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Context-dependent words that can be vague or specific based on quantification\n",
    "CONTEXT_DEPENDENT_WORDS = [\n",
    "    'significant', 'substantial', 'better', 'improved', 'reduced', \n",
    "    'increased', 'enhanced', 'advanced', 'superior', 'effective',\n",
    "    'efficient', 'optimal', 'major', 'minor', 'considerable',\n",
    "    'meaningful', 'notable', 'impressive', 'extensive', 'comprehensive',\n",
    "    # Verb forms\n",
    "    'improve', 'improving', 'improves', 'develop', 'developing', 'develops',\n",
    "    'enhance', 'enhancing', 'enhances', 'advance', 'advancing', 'advances',\n",
    "    'reduce', 'reducing', 'reduces', 'increase', 'increasing', 'increases',\n",
    "    'optimize', 'optimizing', 'optimizes', 'strengthen', 'strengthening', 'strengthens',\n",
    "    'expand', 'expanding', 'expands', 'grow', 'growing', 'grows',\n",
    "    'transform', 'transforming', 'transforms', 'upgrade', 'upgrading', 'upgrades'\n",
    "]\n",
    "\n",
    "# Patterns indicating quantified context (making words NOT vague)\n",
    "QUANTIFIED_PATTERNS = [\n",
    "    r'\\b\\d+(\\.\\d+)?%',  # percentages\n",
    "    r'\\b\\d+(\\.\\d+)?\\s*(tonnes?|kg|g|tons?|pounds?|lbs?)',  # weights\n",
    "    r'\\b\\d+(\\.\\d+)?\\s*(million|billion|thousand|k)\\b',  # large numbers\n",
    "    # r'\\b\\d{4}\\b',  # years\n",
    "    r'\\bp\\s*<\\s*0\\.\\d+',  # p-values\n",
    "    r'\\b\\d+(\\.\\d+)?\\s*(times?|fold)\\b',  # multiples\n",
    "    r'\\b\\d+(\\.\\d+)?\\s*-\\s*\\d+(\\.\\d+)?%',  # ranges\n",
    "    r'\\b\\d+(\\.\\d+)?\\s*(dollars?|\\$|euros?|€)',  # monetary amounts\n",
    "]\n",
    "\n",
    "# Comparative context patterns indicating specific rather than vague usage\n",
    "COMPARATIVE_PATTERNS = [\n",
    "    # Original patterns\n",
    "    r'\\bcompared to\\b', r'\\bvs\\.?\\b', r'\\bversus\\b', r'\\bthan\\s+\\d{4}\\b',\n",
    "    r'\\bfrom\\s+\\d+.*to\\s+\\d+', r'\\bbaseline\\b', r'\\bprevious\\s+year\\b',\n",
    "    r'\\blast\\s+year\\b', r'\\bprior\\s+to\\b', r'\\bagainst\\s+\\d{4}\\b',\n",
    "    \n",
    "    # Time-based comparisons (safe patterns)\n",
    "    r'\\bthan\\s+(last|previous|prior)\\s+(year|quarter|month|period)\\b',\n",
    "    r'\\bfrom\\s+(last|previous|prior)\\s+(year|quarter|month|period)\\b',\n",
    "    r'\\bover\\s+the\\s+(last|previous|prior)\\s+\\d+\\s+(years?|months?|quarters?)\\b',\n",
    "    r'\\byear[\\-\\s]over[\\-\\s]year\\b', r'\\bmonth[\\-\\s]over[\\-\\s]month\\b',\n",
    "    r'\\bquarter[\\-\\s]over[\\-\\s]quarter\\b', \n",
    "    r'\\bsince\\s+\\d{4}\\b', r'\\bfrom\\s+\\d{4}\\s+to\\s+\\d{4}\\b',\n",
    "    r'\\bthan\\s+in\\s+\\d{4}\\b', r'\\bcompared\\s+with\\s+\\d{4}\\b',\n",
    "    \n",
    "    # Benchmarking and targets (specific patterns)\n",
    "    r'\\bvs\\.?\\s+(target|goal|objective|benchmark)\\b',\n",
    "    r'\\bagainst\\s+(target|goal|objective|benchmark|plan)\\b',\n",
    "    r'\\bcompared\\s+to\\s+(target|goal|objective|benchmark)\\b',\n",
    "    r'\\brelative\\s+to\\s+(target|goal|objective|benchmark)\\b',\n",
    "    r'\\bversus\\s+(target|goal|objective|plan)\\b',\n",
    "    \n",
    "    # Industry and peer comparisons (safe and specific)\n",
    "    r'\\bvs\\.?\\s+(industry|sector|market|peers?|competitors?)\\b',\n",
    "    r'\\bcompared\\s+(to|with)\\s+(industry|sector|market|peers?|competitors?)\\b',\n",
    "    r'\\bagainst\\s+(industry|sector|market|peers?|competitors?)\\b',\n",
    "    r'\\brelative\\s+to\\s+(industry|sector|market|peers?|competitors?)\\b',\n",
    "    r'\\bversus\\s+(industry|sector|market|peers?|competitors?)\\b',\n",
    "    r'\\bbenchmarked\\s+against\\b', r'\\bin\\s+comparison\\s+(to|with)\\b',\n",
    "    \n",
    "    # Performance comparisons (specific to avoid false matches)\n",
    "    r'\\boutperform\\w*\\b', r'\\bunderperform\\w*\\b',\n",
    "    \n",
    "    # Percentage and ratio comparisons (very specific)\n",
    "    r'\\b\\d+(\\.\\d+)?%\\s+(higher|lower|above|below)\\b',\n",
    "    r'\\b(up|down|increased?|decreased?)\\s+by\\s+\\d+(\\.\\d+)?%\\b',\n",
    "    r'\\b\\d+(\\.\\d+)?\\s*x\\s+(higher|lower|more|less)\\b',\n",
    "    \n",
    "    # Trend indicators (safe patterns with \"from\")\n",
    "    r'\\bimproved?\\s+from\\b', r'\\bdeclined?\\s+from\\b', r'\\brose\\s+from\\b',\n",
    "    r'\\bfell\\s+from\\b', r'\\bgrew\\s+from\\b', r'\\bdropped\\s+from\\b',\n",
    "    \n",
    "    # Historical comparisons (specific context only)\n",
    "    r'\\bsince\\s+(inception|launch|start)\\b',\n",
    "    r'\\bhistorical\\s+(average|level|performance)\\b',\n",
    "    r'\\bbase\\s+(year|period|level)\\b', r'\\binitial\\s+(level|target)\\b'\n",
    "]\n",
    "\n",
    "# Create flattened lists for easier processing\n",
    "ALL_STRONG_VAGUE = []\n",
    "for category, words in STRONG_VAGUE_LANGUAGE.items():\n",
    "    ALL_STRONG_VAGUE.extend(words)\n",
    "\n",
    "ALL_MILD_VAGUE = []\n",
    "for category, words in MILD_VAGUE_LANGUAGE.items():\n",
    "    ALL_MILD_VAGUE.extend(words)\n",
    "\n",
    "ALL_VAGUE_WORDS = set(ALL_STRONG_VAGUE + ALL_MILD_VAGUE + CONTEXT_DEPENDENT_WORDS)\n",
    "\n",
    "print(\"Vague Language Dictionaries Loaded:\")\n",
    "print(f\"Strong vague words: {len(ALL_STRONG_VAGUE)}\")\n",
    "print(f\"Mild vague words: {len(ALL_MILD_VAGUE)}\")\n",
    "print(f\"Context-dependent words: {len(CONTEXT_DEPENDENT_WORDS)}\")\n",
    "print(f\"Total unique vague words: {len(ALL_VAGUE_WORDS)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4022b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_quantified_context(target_token, token_window=5):\n",
    "    \"\"\"Check if token has quantified context within specified token window in the same sentence.\"\"\"\n",
    "    \n",
    "    # Get all tokens in the sentence\n",
    "    sentence_tokens = list(target_token.sent)\n",
    "    \n",
    "    # Find the index of our target token within the sentence\n",
    "    target_idx = None\n",
    "    for i, token in enumerate(sentence_tokens):\n",
    "        if token == target_token:\n",
    "            target_idx = i\n",
    "            break\n",
    "    \n",
    "    if target_idx is None:\n",
    "        return False\n",
    "    \n",
    "    # Define the token window around the target token (within sentence boundaries)\n",
    "    start_idx = max(0, target_idx - token_window)\n",
    "    end_idx = min(len(sentence_tokens), target_idx + token_window + 1)\n",
    "    \n",
    "    # Get the context tokens\n",
    "    context_tokens = sentence_tokens[start_idx:end_idx]\n",
    "    \n",
    "    # Reconstruct the context text from tokens for pattern matching\n",
    "    context_text = \" \".join([token.text for token in context_tokens])\n",
    "    \n",
    "    # Check for quantified patterns in the context\n",
    "    for pattern in QUANTIFIED_PATTERNS:\n",
    "        if re.search(pattern, context_text, re.IGNORECASE):\n",
    "            return True\n",
    "    \n",
    "    # Check for NUM tokens (excluding years 1990-2050)\n",
    "    for token in context_tokens:\n",
    "        if token.pos_ == \"NUM\":\n",
    "            # Try to check if it's a year to exclude\n",
    "            try:\n",
    "                num_value = int(token.text)\n",
    "                if 1990 <= num_value <= 2050:\n",
    "                    continue  # Skip years\n",
    "            except ValueError:\n",
    "                pass  # Not a simple integer\n",
    "            \n",
    "            # If we get here, it's either not a year or not a simple integer\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def has_comparative_context(target_token, token_window=5):\n",
    "    \"\"\"Check if token has comparative context within specified token window in the same sentence.\"\"\"\n",
    "    \n",
    "    # Get all tokens in the sentence\n",
    "    sentence_tokens = list(target_token.sent)\n",
    "    \n",
    "    # Find the index of our target token within the sentence\n",
    "    target_idx = None\n",
    "    for i, token in enumerate(sentence_tokens):\n",
    "        if token == target_token:\n",
    "            target_idx = i\n",
    "            break\n",
    "    \n",
    "    if target_idx is None:\n",
    "        return False\n",
    "    \n",
    "    # Define the token window around the target token (within sentence boundaries)\n",
    "    start_idx = max(0, target_idx - token_window)\n",
    "    end_idx = min(len(sentence_tokens), target_idx + token_window + 1)\n",
    "    \n",
    "    # Get the context tokens\n",
    "    context_tokens = sentence_tokens[start_idx:end_idx]\n",
    "    \n",
    "    # Reconstruct the context text from tokens\n",
    "    context_text = \" \".join([token.text for token in context_tokens])\n",
    "    \n",
    "    # Check for comparative patterns in the context\n",
    "    for pattern in COMPARATIVE_PATTERNS:\n",
    "        if re.search(pattern, context_text, re.IGNORECASE):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def classify_word_vagueness_token(token):\n",
    "    \"\"\"\n",
    "    Classify word's vagueness level based on context using token object.\n",
    "    Returns: 'strong_vague', 'mild_vague', 'context_quantified', 'context_compared', or 'not_vague'\n",
    "    \"\"\"\n",
    "    word = token.text\n",
    "    word_lower = word.lower()\n",
    "    \n",
    "    # Check if context-dependent word\n",
    "    if word_lower in CONTEXT_DEPENDENT_WORDS:\n",
    "        # Check for quantified or comparative context using token-based functions\n",
    "        has_quant = has_quantified_context(token)\n",
    "        has_comp = has_comparative_context(token)\n",
    "        \n",
    "        if has_quant and has_comp:\n",
    "            return 'context_quantified'  # Prioritize quantified if both\n",
    "        elif has_quant:\n",
    "            return 'context_quantified'\n",
    "        elif has_comp:\n",
    "            return 'context_compared'\n",
    "        else:\n",
    "            # Without context, classify as vague\n",
    "            if word_lower in ALL_STRONG_VAGUE:\n",
    "                return 'strong_vague'\n",
    "            else:\n",
    "                return 'mild_vague'\n",
    "    \n",
    "    # Check if degree_vagueness word used as VERB - don't count as vague\n",
    "    if word_lower in STRONG_VAGUE_LANGUAGE['degree_vagueness'] and token.pos_ == \"VERB\":\n",
    "        return 'not_vague'\n",
    "\n",
    "    # Check regular vague categories\n",
    "    if word_lower in ALL_STRONG_VAGUE:\n",
    "        return 'strong_vague'\n",
    "    elif word_lower in ALL_MILD_VAGUE:\n",
    "        return 'mild_vague'\n",
    "    else:\n",
    "        return 'not_vague'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e38f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_vague_language_in_document(doc, document_name):\n",
    "    \"\"\"\n",
    "    Find all vague language instances in a document using spaCy processing.\n",
    "    Similar structure to find_hedge_words_in_document but for vague language.\n",
    "    \"\"\"\n",
    "    vague_analysis = {\n",
    "        'document_name': document_name,\n",
    "        'strong_vague_words': [],\n",
    "        'mild_vague_words': [],\n",
    "        'context_specific_words': [],\n",
    "        'quantified_context_words': [],\n",
    "        'compared_context_words': [],\n",
    "        'quantified_context_examples': [],\n",
    "        'compared_context_examples': [],\n",
    "        'vague_context_examples': [],\n",
    "        'total_context_dependent_found': 0,\n",
    "        'strong_count': 0,\n",
    "        'mild_count': 0,\n",
    "        'context_specific_count': 0,\n",
    "        'quantified_context_count': 0,\n",
    "        'compared_context_count': 0,  \n",
    "        'total_vague_count': 0,\n",
    "        'meaningful_words': 0,\n",
    "        'vague_word_contexts': [],\n",
    "        'category_counts': defaultdict(int)\n",
    "    }\n",
    "    \n",
    "    # Reuse existing function for meaningful word count\n",
    "    vague_analysis['meaningful_words'] = count_meaningful_words(doc)\n",
    "    \n",
    "    # Get document text for context analysis\n",
    "    doc_text = doc.text\n",
    "    \n",
    "    # Process each token for vague language\n",
    "    for token in doc:\n",
    "        if token.is_punct or token.is_space or len(token.text.strip()) < 2:\n",
    "            continue\n",
    "        \n",
    "        token_text = token.text\n",
    "        word_position = token.idx\n",
    "        \n",
    "        # Check if word is potentially vague\n",
    "        if token_text.lower() in ALL_VAGUE_WORDS:\n",
    "            \n",
    "            # Track if this is a context-dependent word (regardless of final classification)\n",
    "            if token_text.lower() in CONTEXT_DEPENDENT_WORDS:\n",
    "                vague_analysis['total_context_dependent_found'] += 1\n",
    "            \n",
    "            vagueness_type = classify_word_vagueness_token(token)\n",
    "            \n",
    "            # Create vague word information\n",
    "            if vagueness_type != 'not_vague':\n",
    "                # Create 10-token window context (within same sentence)\n",
    "                sentence_tokens = list(token.sent)\n",
    "                \n",
    "                # Find the index of our target token within the sentence\n",
    "                target_idx = None\n",
    "                for i, sent_token in enumerate(sentence_tokens):\n",
    "                    if sent_token == token:\n",
    "                        target_idx = i\n",
    "                        break\n",
    "                \n",
    "                if target_idx is not None:\n",
    "                    # Define 10-token window around the target token (within sentence boundaries)\n",
    "                    token_window = 10\n",
    "                    start_idx = max(0, target_idx - token_window)\n",
    "                    end_idx = min(len(sentence_tokens), target_idx + token_window + 1)\n",
    "                    \n",
    "                    # Get the context tokens and reconstruct text\n",
    "                    context_tokens = sentence_tokens[start_idx:end_idx]\n",
    "                    context = \" \".join([t.text for t in context_tokens])\n",
    "                else:\n",
    "                    # Fallback to sentence text if token not found\n",
    "                    context = token.sent.text.strip()\n",
    "                \n",
    "                vague_info = {\n",
    "                    'word': token_text,\n",
    "                    'lemma': token.lemma_.lower(),\n",
    "                    'type': vagueness_type,\n",
    "                    'position': word_position,\n",
    "                    'context': context,\n",
    "                    'sentence_text': token.sent.text.strip()[:200] + \"...\" if len(token.sent.text) > 200 else token.sent.text.strip()\n",
    "                }\n",
    "                \n",
    "                # Categorize by vagueness type\n",
    "                if vagueness_type == 'strong_vague':\n",
    "                    # Only count truly vague words in total counts\n",
    "                    vague_analysis['vague_word_contexts'].append(vague_info)\n",
    "                    vague_analysis['total_vague_count'] += 1\n",
    "                    vague_analysis['strong_vague_words'].append(vague_info)\n",
    "                    vague_analysis['strong_count'] += 1\n",
    "                    \n",
    "                    # Check if this is also a context-dependent word that ended up being vague\n",
    "                    if token_text.lower() in CONTEXT_DEPENDENT_WORDS:\n",
    "                        vague_analysis['vague_context_examples'].append(vague_info)\n",
    "\n",
    "                    # Find specific strong category\n",
    "                    for category, words in STRONG_VAGUE_LANGUAGE.items():\n",
    "                        if token_text.lower() in words:\n",
    "                            vague_analysis['category_counts'][f'strong_{category}'] += 1\n",
    "                            break\n",
    "                \n",
    "                elif vagueness_type == 'mild_vague':\n",
    "                    # Only count truly vague words in total counts\n",
    "                    vague_analysis['vague_word_contexts'].append(vague_info)\n",
    "                    vague_analysis['total_vague_count'] += 1\n",
    "                    vague_analysis['mild_vague_words'].append(vague_info)\n",
    "                    vague_analysis['mild_count'] += 1\n",
    "                    \n",
    "                    # Check if this is also a context-dependent word that ended up being vague\n",
    "                    if token_text.lower() in CONTEXT_DEPENDENT_WORDS:\n",
    "                        vague_analysis['vague_context_examples'].append(vague_info)\n",
    "                    \n",
    "                    # Find specific mild category\n",
    "                    for category, words in MILD_VAGUE_LANGUAGE.items():\n",
    "                        if token_text.lower() in words:\n",
    "                            vague_analysis['category_counts'][f'mild_{category}'] += 1\n",
    "                            break\n",
    "                \n",
    "                elif vagueness_type == 'context_quantified':\n",
    "                    # Track quantified context separately - these are NOT vague due to quantified context\n",
    "                    vague_analysis['quantified_context_words'].append(vague_info)\n",
    "                    vague_analysis['quantified_context_count'] += 1\n",
    "                    vague_analysis['context_specific_words'].append(vague_info)  # Keep for backward compatibility\n",
    "                    vague_analysis['context_specific_count'] += 1\n",
    "                    vague_analysis['category_counts']['context_quantified'] += 1\n",
    "                    vague_analysis['quantified_context_examples'].append(vague_info)\n",
    "                \n",
    "                elif vagueness_type == 'context_compared':\n",
    "                    # Track compared context separately - these are NOT vague due to comparative context\n",
    "                    vague_analysis['compared_context_words'].append(vague_info)\n",
    "                    vague_analysis['compared_context_count'] += 1\n",
    "                    vague_analysis['context_specific_words'].append(vague_info)  # Keep for backward compatibility\n",
    "                    vague_analysis['context_specific_count'] += 1\n",
    "                    vague_analysis['category_counts']['context_compared'] += 1\n",
    "                    vague_analysis['compared_context_examples'].append(vague_info)\n",
    "    \n",
    "    return vague_analysis\n",
    "\n",
    "def calculate_vague_language_densities(vague_analysis):\n",
    "    \"\"\"Calculate vague language density metrics similar to hedge word densities.\"\"\"\n",
    "    meaningful_words = vague_analysis['meaningful_words']\n",
    "    if meaningful_words == 0:\n",
    "        return {\n",
    "            'total_vague_density': 0.0,\n",
    "            'strong_vague_density': 0.0,\n",
    "            'mild_vague_density': 0.0,\n",
    "            'context_specific_density': 0.0,\n",
    "            'strong_mild_ratio': 0.0,\n",
    "            'vague_intensity_score': 0.0\n",
    "        }\n",
    "    \n",
    "    strong_count = vague_analysis['strong_count']\n",
    "    mild_count = vague_analysis['mild_count']\n",
    "    context_count = vague_analysis['context_specific_count']\n",
    "    total_vague = strong_count + mild_count\n",
    "    \n",
    "    # Calculate densities as percentages\n",
    "    total_density = (total_vague / meaningful_words) * 100\n",
    "    strong_density = (strong_count / meaningful_words) * 100\n",
    "    mild_density = (mild_count / meaningful_words) * 100\n",
    "    context_density = (context_count / meaningful_words) * 100\n",
    "    \n",
    "    # Calculate ratios\n",
    "    strong_mild_ratio = strong_count / mild_count if mild_count > 0 else 0.0\n",
    "    \n",
    "    # Calculate weighted intensity score (strong vague weighted 1.5x)\n",
    "    intensity_score = (((strong_count * 1.5) + (mild_count * 1)) / meaningful_words) * 100\n",
    "    \n",
    "    return {\n",
    "        'total_vague_density': round(total_density, 4),\n",
    "        'strong_vague_density': round(strong_density, 4),\n",
    "        'mild_vague_density': round(mild_density, 4),\n",
    "        'context_specific_density': round(context_density, 4),\n",
    "        'strong_mild_ratio': round(strong_mild_ratio, 4),\n",
    "        'vague_intensity_score': round(intensity_score, 4)\n",
    "    }\n",
    "\n",
    "def calculate_context_metrics(vague_analysis):\n",
    "    \"\"\"Calculate context-dependent word metrics using total found context words.\"\"\"\n",
    "    total_context_found = vague_analysis['total_context_dependent_found']\n",
    "    quantified_count = vague_analysis['quantified_context_count']\n",
    "    compared_count = vague_analysis['compared_context_count']\n",
    "    \n",
    "    if total_context_found == 0:\n",
    "        return {\n",
    "            'quantified_context_pct': 0.0,\n",
    "            'compared_context_pct': 0.0,\n",
    "            'total_context_pct': 0.0\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'quantified_context_pct': round((quantified_count / total_context_found) * 100, 4),\n",
    "        'compared_context_pct': round((compared_count / total_context_found) * 100, 4),\n",
    "        'total_context_pct': round(((quantified_count + compared_count) / total_context_found) * 100, 4)\n",
    "    }\n",
    "\n",
    "def analyze_commitment_vagueness(vague_analysis, doc_text):\n",
    "    \"\"\"Analyze commitment-related vague language without concrete timelines.\"\"\"\n",
    "    commitment_words = STRONG_VAGUE_LANGUAGE['commitment_vagueness']\n",
    "    \n",
    "    total_commitment_words_found = 0\n",
    "    commitment_words_with_timelines = 0\n",
    "    vague_commitments = []\n",
    "    commitments_with_timelines = []\n",
    "    \n",
    "    for context_info in vague_analysis['vague_word_contexts']:\n",
    "        word = context_info['word'].lower()\n",
    "        if any(commit_word in word for commit_word in commitment_words):\n",
    "            total_commitment_words_found += 1\n",
    "            \n",
    "            # Check for concrete timeline in context\n",
    "            context = context_info['context']\n",
    "            has_timeline = bool(re.search(r'\\b(by|until|before|after)\\s+\\d{4}\\b|'\n",
    "                                        r'\\b(january|february|march|april|may|june|'\n",
    "                                        r'july|august|september|october|november|december)\\s+\\d{4}\\b|'\n",
    "                                        r'\\b(jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)\\.?\\s+\\d{4}\\b|'\n",
    "                                        r'\\b\\d{1,2}\\s+(months?|years?|weeks?|days?)\\b|'\n",
    "                                        \n",
    "                                        # End of year/period references\n",
    "                                        r'\\bby\\s+(the\\s+)?end\\s+of\\s+\\d{4}\\b|'\n",
    "                                        r'\\bby\\s+end[\\-\\s]\\d{4}\\b|'\n",
    "                                        \n",
    "                                        # Within timeframe references\n",
    "                                        r'\\bwithin\\s+\\d{1,2}\\s+(years?|months?|quarters?)\\b|'\n",
    "                                        r'\\bover\\s+the\\s+next\\s+\\d{1,2}\\s+(years?|months?|quarters?)\\b|'\n",
    "                                        r'\\bin\\s+the\\s+next\\s+\\d{1,2}\\s+(years?|months?|quarters?)\\b|'\n",
    "                                        \n",
    "                                        # Quarter references\n",
    "                                        r'\\bby\\s+[Qq][1-4]\\s+\\d{4}\\b|'\n",
    "                                        r'\\b[Qq][1-4]\\s+\\d{4}\\b|'\n",
    "                                        r'\\bby\\s+(first|second|third|fourth)\\s+quarter\\s+\\d{4}\\b|'\n",
    "                                        \n",
    "                                        # During/throughout references\n",
    "                                        r'\\bduring\\s+\\d{4}\\b|'\n",
    "                                        r'\\bthroughout\\s+\\d{4}\\b|'\n",
    "                                        r'\\bin\\s+\\d{4}\\b|'\n",
    "                                        \n",
    "                                        # Part of year references\n",
    "                                        r'\\bby\\s+(early|mid|late)\\s+\\d{4}\\b|'\n",
    "                                        r'\\bby\\s+(spring|summer|fall|autumn|winter)\\s+\\d{4}\\b|'\n",
    "                                        \n",
    "                                        # Fiscal year references\n",
    "                                        r'\\bby\\s+[Ff][Yy]\\s*\\d{2,4}\\b|'\n",
    "                                        r'\\b[Ff]iscal\\s+[Yy]ear\\s+\\d{2,4}\\b|'\n",
    "                                        \n",
    "                                        # No later than / starting references\n",
    "                                        r'\\bno\\s+later\\s+than\\s+\\d{4}\\b|'\n",
    "                                        r'\\bstarting\\s+(in\\s+)?\\d{4}\\b|'\n",
    "                                        r'\\bbeginning\\s+(in\\s+)?\\d{4}\\b|'\n",
    "                                        \n",
    "                                        # Date ranges\n",
    "                                        r'\\bbetween\\s+\\d{4}\\s+and\\s+\\d{4}\\b|'\n",
    "                                        r'\\bfrom\\s+\\d{4}\\s+(through|to|until)\\s+\\d{4}\\b|'\n",
    "                                        \n",
    "                                        # Specific month references\n",
    "                                        r'\\bby\\s+(january|february|march|april|may|june|july|august|september|october|november|december)\\b|'\n",
    "                                        r'\\bby\\s+(jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)\\.?\\b|'\n",
    "                                        \n",
    "                                        # Target year references\n",
    "                                        r'\\btarget\\s+(of\\s+)?\\d{4}\\b|'\n",
    "                                        r'\\bgoal\\s+(of\\s+)?\\d{4}\\b', \n",
    "                                        context, re.IGNORECASE))\n",
    "            \n",
    "            if has_timeline:\n",
    "                commitment_words_with_timelines += 1\n",
    "                commitments_with_timelines.append(context_info)\n",
    "            else:\n",
    "                vague_commitments.append(context_info)\n",
    "    \n",
    "    # Calculate ratio of commitments WITH timelines (consistent with context analysis)\n",
    "    commitment_timeline_pct = ((commitment_words_with_timelines / total_commitment_words_found) * 100 \n",
    "                                if total_commitment_words_found > 0 else 0.0)\n",
    "    \n",
    "    return {\n",
    "        'total_commitment_words_found': total_commitment_words_found,\n",
    "        'commitment_words_with_timelines': commitment_words_with_timelines,\n",
    "        'vague_commitment_count': len(vague_commitments),\n",
    "        'vague_commitment_examples': vague_commitments[:5],\n",
    "        'commitments_with_timeline_examples': commitments_with_timelines[:5],\n",
    "        'commitment_timeline_pct': round(commitment_timeline_pct, 4),\n",
    "        'commitment_vagueness_ratio': len(vague_commitments) / len(vague_analysis['vague_word_contexts']) \n",
    "                                    if vague_analysis['vague_word_contexts'] else 0\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65cbeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_vague_language_all_documents(documents):\n",
    "    \"\"\"\n",
    "    Analyze vague language across all documents. \n",
    "    Similar structure to analyze_hedge_words_all_documents.\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "    density_stats = {\n",
    "        'total_documents': len(documents),\n",
    "        'documents_with_high_vagueness': [],\n",
    "        'documents_with_low_vagueness': []\n",
    "    }\n",
    "    \n",
    "    # Collect density values for statistics\n",
    "    total_densities = []\n",
    "    strong_densities = []\n",
    "    mild_densities = []\n",
    "    intensity_scores = []\n",
    "    \n",
    "    # Process each document\n",
    "    for doc_name, doc in documents.items():\n",
    "        # Analyze vague language in document\n",
    "        vague_analysis = find_vague_language_in_document(doc, doc_name)\n",
    "        \n",
    "        # Calculate densities\n",
    "        densities = calculate_vague_language_densities(vague_analysis)\n",
    "        \n",
    "        # Calculate context metrics\n",
    "        context_metrics = calculate_context_metrics(vague_analysis)\n",
    "        \n",
    "        # Analyze commitment vagueness\n",
    "        commitment_analysis = analyze_commitment_vagueness(vague_analysis, doc.text)\n",
    "        \n",
    "        # Count quantified terms for comparison\n",
    "        quantified_count = 0\n",
    "        for pattern in QUANTIFIED_PATTERNS:\n",
    "            quantified_count += len(re.findall(pattern, doc.text, re.IGNORECASE))\n",
    "        \n",
    "        # Combine results\n",
    "        document_result = {\n",
    "            **vague_analysis, \n",
    "            **densities,\n",
    "            **context_metrics,\n",
    "            'commitment_analysis': commitment_analysis,\n",
    "            'quantified_terms': quantified_count,\n",
    "        }\n",
    "        all_results[doc_name] = document_result\n",
    "        \n",
    "        # Collect density values for statistics\n",
    "        total_densities.append(densities['total_vague_density'])\n",
    "        strong_densities.append(densities['strong_vague_density'])\n",
    "        mild_densities.append(densities['mild_vague_density'])\n",
    "        intensity_scores.append(densities['vague_intensity_score'])\n",
    "        \n",
    "        # Classify documents by vagueness level\n",
    "        if densities['total_vague_density'] > 5.0:  # High vagueness threshold\n",
    "            density_stats['documents_with_high_vagueness'].append(doc_name)\n",
    "        elif densities['total_vague_density'] < 3.0:  # Low vagueness threshold\n",
    "            density_stats['documents_with_low_vagueness'].append(doc_name)\n",
    "    \n",
    "    # Calculate cross-document statistics\n",
    "    if total_densities:\n",
    "        density_stats.update({\n",
    "            'average_total_vague_density': round(np.mean(total_densities), 4),\n",
    "            'average_strong_vague_density': round(np.mean(strong_densities), 4),\n",
    "            'average_mild_vague_density': round(np.mean(mild_densities), 4),\n",
    "            'average_vague_intensity_score': round(np.mean(intensity_scores), 4),\n",
    "            'min_total_vague_density': round(np.min(total_densities), 4),\n",
    "            'max_total_vague_density': round(np.max(total_densities), 4),\n",
    "            'range_total_vague_density': round(np.max(total_densities) - np.min(total_densities), 4),\n",
    "            'min_strong_vague_density': round(np.min(strong_densities), 4),\n",
    "            'max_strong_vague_density': round(np.max(strong_densities), 4),\n",
    "            'min_mild_vague_density': round(np.min(mild_densities), 4),\n",
    "            'max_mild_vague_density': round(np.max(mild_densities), 4)\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'document_results': all_results,\n",
    "        'density_statistics': density_stats\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23edf53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vague_language_summary_table(analysis_results):\n",
    "    \"\"\"Create pandas DataFrame summarizing vague language analysis results.\"\"\"\n",
    "    document_results = analysis_results['document_results']\n",
    "    \n",
    "    summary_data = []\n",
    "    for doc_name, results in document_results.items():\n",
    "        summary_data.append({\n",
    "            'Document': doc_name,\n",
    "            'Meaningful Words': results['meaningful_words'],\n",
    "            'Total Vague Words': results['total_vague_count'],\n",
    "            'Strong Vague Words': results['strong_count'],\n",
    "            'Mild Vague Words': results['mild_count'],\n",
    "            'Total Vague Density (%)': results['total_vague_density'],\n",
    "            'Strong Vague Density (%)': results['strong_vague_density'],\n",
    "            'Mild Vague Density (%)': results['mild_vague_density'],\n",
    "            'Vague Intensity Score': results['vague_intensity_score'],\n",
    "            'Vague Commitments': results['commitment_analysis']['vague_commitment_count'],\n",
    "            'total_commitment_words_found': results['commitment_analysis']['total_commitment_words_found'],\n",
    "            'Commitment (with) Timeline (%)': results['commitment_analysis']['commitment_timeline_pct'],\n",
    "            'quantified_context_words': results['quantified_context_count'],\n",
    "            'compared_context_words': results['compared_context_count'], \n",
    "            'total_context_words': results['total_context_dependent_found'],            \n",
    "            'quantified_context (%)': results['quantified_context_pct'],\n",
    "            'compared_context (%)': results['compared_context_pct'],\n",
    "            'total_context (%)': results['total_context_pct']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(summary_data)\n",
    "    return df # .sort_values('Total Vague Density (%)', ascending=False)\n",
    "\n",
    "def display_vague_language_analysis_results(analysis_results):\n",
    "    \"\"\"Display comprehensive vague language analysis results.\"\"\"\n",
    "    density_stats = analysis_results['density_statistics']\n",
    "    \n",
    "    print(\"VAGUE LANGUAGE ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nBASIC STATISTICS:\")\n",
    "    print(f\"Total Documents: {density_stats['total_documents']}\")\n",
    "    \n",
    "    # Density statistics across all documents\n",
    "    print(f\"\\nDENSITY STATISTICS ACROSS ALL DOCUMENTS:\")\n",
    "    print(f\"Average Total Vague Density: {density_stats['average_total_vague_density']:.4f}%\")\n",
    "    print(f\"Average Strong Vague Density: {density_stats['average_strong_vague_density']:.4f}%\")\n",
    "    print(f\"Average Mild Vague Density: {density_stats['average_mild_vague_density']:.4f}%\")\n",
    "    print(f\"Average Vague Intensity Score: {density_stats['average_vague_intensity_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nDENSITY RANGES:\")\n",
    "    print(f\"Total Vague Density Range: {density_stats['min_total_vague_density']:.4f}% - {density_stats['max_total_vague_density']:.4f}% (range: {density_stats['range_total_vague_density']:.4f}%)\")\n",
    "    print(f\"Strong Vague Density Range: {density_stats['min_strong_vague_density']:.4f}% - {density_stats['max_strong_vague_density']:.4f}%\")\n",
    "    print(f\"Mild Vague Density Range: {density_stats['min_mild_vague_density']:.4f}% - {density_stats['max_mild_vague_density']:.4f}%\")\n",
    "    \n",
    "    # High and low vagueness documents\n",
    "    if density_stats['documents_with_high_vagueness']:\n",
    "        print(f\"\\nHIGH VAGUENESS DOCUMENTS (>5% density):\")\n",
    "        for doc in density_stats['documents_with_high_vagueness']:\n",
    "            doc_density = analysis_results['document_results'][doc]['total_vague_density']\n",
    "            print(f\"  {doc}: {doc_density:.4f}%\")\n",
    "    \n",
    "    if density_stats['documents_with_low_vagueness']:\n",
    "        print(f\"\\nLOW VAGUENESS DOCUMENTS (<3% density):\")\n",
    "        for doc in density_stats['documents_with_low_vagueness']:\n",
    "            doc_density = analysis_results['document_results'][doc]['total_vague_density']\n",
    "            print(f\"  {doc}: {doc_density:.4f}%\")\n",
    "    \n",
    "    # Create and display summary table\n",
    "    summary_table = create_vague_language_summary_table(analysis_results)\n",
    "    print(f\"\\nDOCUMENT SUMMARY TABLE:\")\n",
    "    print(summary_table.to_string(index=False))\n",
    "    \n",
    "    return summary_table\n",
    "\n",
    "def show_vague_language_examples(analysis_results, doc_name=None, max_examples=10):\n",
    "    \"\"\"Show examples of vague language usage from documents.\"\"\"\n",
    "    document_results = analysis_results['document_results']\n",
    "    \n",
    "    if doc_name:\n",
    "        if doc_name not in document_results:\n",
    "            print(f\"Document '{doc_name}' not found\")\n",
    "            return\n",
    "        docs_to_show = {doc_name: document_results[doc_name]}\n",
    "    else:\n",
    "        # Show examples from highest vague density document\n",
    "        sorted_docs = sorted(document_results.items(), \n",
    "                           key=lambda x: x[1]['total_vague_density'], \n",
    "                           reverse=True)\n",
    "        docs_to_show = {sorted_docs[0][0]: sorted_docs[0][1]} if sorted_docs else {}\n",
    "    \n",
    "    for doc_name, results in docs_to_show.items():\n",
    "        print(f\"\\nVAGUE LANGUAGE EXAMPLES FROM: {doc_name}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Strong vague examples\n",
    "        if results['strong_vague_words']:\n",
    "            print(f\"\\nSTRONG VAGUE WORDS ({len(results['strong_vague_words'])} total):\")\n",
    "            for i, vague in enumerate(results['strong_vague_words'][:max_examples]):\n",
    "                print(f\"{i+1}. '{vague['word']}' (lemma: {vague['lemma']})\")\n",
    "                print(f\"   Context: {vague['context']}\")\n",
    "                print()\n",
    "        \n",
    "        # Mild vague examples\n",
    "        if results['mild_vague_words']:\n",
    "            print(f\"\\nMILD VAGUE WORDS ({len(results['mild_vague_words'])} total):\")\n",
    "            for i, vague in enumerate(results['mild_vague_words'][:max_examples]):\n",
    "                print(f\"{i+1}. '{vague['word']}' (lemma: {vague['lemma']})\")\n",
    "                print(f\"   Context: {vague['context']}\")\n",
    "                print()\n",
    "        \n",
    "        # COMMITMENT EXAMPLES (4 examples total)\n",
    "        commitment_analysis = results['commitment_analysis']\n",
    "        \n",
    "        # Vague commitments (without timeline) - 2 examples\n",
    "        if commitment_analysis['vague_commitment_examples']:\n",
    "            print(f\"\\nVAGUE COMMITMENTS (WITHOUT TIMELINE) - {len(commitment_analysis['vague_commitment_examples'])} total:\")\n",
    "            for i, commit in enumerate(commitment_analysis['vague_commitment_examples'][:2]):\n",
    "                print(f\"{i+1}. '{commit['word']}'\")\n",
    "                print(f\"   Context: {commit['context']}\")\n",
    "                print()\n",
    "        \n",
    "        # Commitments with timeline - 2 examples\n",
    "        if commitment_analysis.get('commitments_with_timeline_examples'):\n",
    "            print(f\"\\nCOMMITMENTS (WITH TIMELINE) - {commitment_analysis['commitment_words_with_timelines']} total:\")\n",
    "            for i, commit in enumerate(commitment_analysis['commitments_with_timeline_examples'][:2]):\n",
    "                print(f\"{i+1}. '{commit['word']}'\")\n",
    "                print(f\"   Context: {commit['context']}\")\n",
    "                print()\n",
    "        \n",
    "        # CONTEXT-DEPENDENT EXAMPLES (6 examples total)\n",
    "        \n",
    "        # Vague context words (counted as vague) - 2 examples\n",
    "        if results.get('vague_context_examples'):\n",
    "            print(f\"\\nCONTEXT WORDS (COUNTED AS VAGUE) - 2 examples:\")\n",
    "            for i, context in enumerate(results['vague_context_examples'][:2]):\n",
    "                print(f\"{i+1}. '{context['word']}' (lemma: {context['lemma']})\")\n",
    "                print(f\"   Context: {context['context']}\")\n",
    "                print()\n",
    "        \n",
    "        # Quantified context words (not counted due to quantification) - 2 examples\n",
    "        if results.get('quantified_context_examples'):\n",
    "            print(f\"\\nCONTEXT WORDS (NOT COUNTED - QUANTIFIED) - {results['quantified_context_count']} total:\")\n",
    "            for i, context in enumerate(results['quantified_context_examples'][:2]):\n",
    "                print(f\"{i+1}. '{context['word']}' (lemma: {context['lemma']})\")\n",
    "                print(f\"   Context: {context['context']}\")\n",
    "                print()\n",
    "        \n",
    "        # Compared context words (not counted due to comparison) - 2 examples\n",
    "        if results.get('compared_context_examples'):\n",
    "            print(f\"\\nCONTEXT WORDS (NOT COUNTED - COMPARED) - {results['compared_context_count']} total:\")\n",
    "            for i, context in enumerate(results['compared_context_examples'][:2]):\n",
    "                print(f\"{i+1}. '{context['word']}' (lemma: {context['lemma']})\")\n",
    "                print(f\"   Context: {context['context']}\")\n",
    "                print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf51a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vague_language_analysis():\n",
    "    \"\"\"Run vague language analysis using existing documents variable.\"\"\"\n",
    "    try:\n",
    "        # Check if documents variable exists (from hedge word analysis)\n",
    "        if 'documents' not in globals():\n",
    "            print(\"Error: 'documents' variable not found. Please run the hedge words analysis first.\")\n",
    "            return None\n",
    "        \n",
    "        print(\"Starting vague language analysis...\")\n",
    "        \n",
    "        # Run the analysis\n",
    "        vague_results = analyze_vague_language_all_documents(documents)\n",
    "        \n",
    "        # Display results\n",
    "        summary_table = display_vague_language_analysis_results(vague_results)\n",
    "        \n",
    "        print(f\"\\nVague language analysis complete.\")\n",
    "        \n",
    "        # Store results globally for further use\n",
    "        global vague_language_results\n",
    "        vague_language_results = vague_results\n",
    "        \n",
    "        return vague_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during vague language analysis: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run the analysis\n",
    "vague_analysis_results = run_vague_language_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e904df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples from most vague document\n",
    "if vague_analysis_results:\n",
    "    show_vague_language_examples(vague_analysis_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed65b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINED HEDGE AND VAGUE WORDS ANALYSIS - CREATE DATAFRAME\n",
    "\n",
    "def create_hedge_vague_analysis_dataframe(hedge_results, vague_results):\n",
    "    \"\"\"\n",
    "    Create a DataFrame combining hedge and vague word analysis.\n",
    "    Rows: Organizations (company-year combinations)\n",
    "    Columns: All metrics from both hedge and vague word analysis plus combined metrics\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    # Get document names from hedge results (assuming both analyses cover same documents)\n",
    "    hedge_documents = hedge_results['document_results']\n",
    "    vague_documents = vague_results['document_results']\n",
    "    \n",
    "    for doc_name in hedge_documents.keys():\n",
    "        if doc_name not in vague_documents:\n",
    "            print(f\"Warning: {doc_name} not found in vague results, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        # Extract organization and year from document name\n",
    "        parts = doc_name.split('_')\n",
    "        year = parts[-1]\n",
    "        org_name = '_'.join(parts[:-1])\n",
    "        \n",
    "        # Get hedge word results\n",
    "        hedge_data = hedge_documents[doc_name]\n",
    "        vague_data = vague_documents[doc_name]\n",
    "        \n",
    "        # Basic document metrics (should be same for both analyses)\n",
    "        meaningful_words = hedge_data['meaningful_words']\n",
    "        \n",
    "        # HEDGE WORD METRICS\n",
    "        total_hedge_count = hedge_data['total_hedge_count']\n",
    "        hedge_strong_count = hedge_data['strong_count']\n",
    "        hedge_mild_count = hedge_data['mild_count']\n",
    "        total_hedge_density = hedge_data['total_hedge_density']\n",
    "        strong_hedge_density = hedge_data['strong_hedge_density']\n",
    "        mild_hedge_density = hedge_data['mild_hedge_density']\n",
    "        hedge_strong_mild_ratio = hedge_data['strong_vs_mild_ratio']\n",
    "        hedge_intensity_score = hedge_data['hedge_intensity_score']\n",
    "        high_hedge_sentences = len(hedge_data['high_hedge_sentences'])\n",
    "        \n",
    "        # VAGUE WORD METRICS\n",
    "        total_vague_count = vague_data['total_vague_count']\n",
    "        vague_strong_count = vague_data['strong_count']\n",
    "        vague_mild_count = vague_data['mild_count']\n",
    "        total_vague_density = vague_data['total_vague_density']\n",
    "        strong_vague_density = vague_data['strong_vague_density']\n",
    "        mild_vague_density = vague_data['mild_vague_density']\n",
    "        vague_strong_mild_ratio = vague_data['strong_mild_ratio']\n",
    "        vague_intensity_score = vague_data['vague_intensity_score']\n",
    "        vague_commitments = vague_data['commitment_analysis']['vague_commitment_count']\n",
    "        total_commitment_words_found = vague_data['commitment_analysis']['total_commitment_words_found']\n",
    "        commitment_timeline_pct = vague_data['commitment_analysis']['commitment_timeline_pct']\n",
    "        quantified_terms = vague_data['quantified_terms']\n",
    "        quantified_context_words = vague_data['quantified_context_count']\n",
    "        compared_context_words = vague_data['compared_context_count']\n",
    "        total_context_words = vague_data['total_context_dependent_found']\n",
    "        quantified_context_pct = vague_data['quantified_context_pct']\n",
    "        compared_context_pct = vague_data['compared_context_pct']\n",
    "        total_context_pct = vague_data['total_context_pct']\n",
    "        \n",
    "        # COMBINED METRICS\n",
    "        total_unclear_words = total_hedge_count + total_vague_count\n",
    "        total_unclear_density = (total_unclear_words / meaningful_words) * 100\n",
    "        hedge_vague_ratio = total_hedge_count / total_vague_count if total_vague_count > 0 else 0\n",
    "        \n",
    "        # Combined intensity score: ((strong_hedge × 1.5 + mild_hedge × 1 + strong_vague × 1.5 + mild_vague × 1) / meaningful_words) × 100\n",
    "        combined_intensity_score = (((hedge_strong_count * 1.5) + (hedge_mild_count * 1) + \n",
    "                                   (vague_strong_count * 1.5) + (vague_mild_count * 1)) / meaningful_words) * 100\n",
    "        \n",
    "        # Combined strong/mild ratios\n",
    "        total_strong = hedge_strong_count + vague_strong_count\n",
    "        total_mild = hedge_mild_count + vague_mild_count\n",
    "        combined_strong_mild_ratio = total_strong / total_mild if total_mild > 0 else 0\n",
    "        \n",
    "        row = {\n",
    "            # Basic identifiers\n",
    "            'organization': org_name,\n",
    "            'year': int(year),\n",
    "            'meaningful_words': meaningful_words,\n",
    "            \n",
    "            # HEDGE WORD METRICS\n",
    "            'total_hedge_words': total_hedge_count,\n",
    "            'hedge_strong_count': hedge_strong_count,\n",
    "            'hedge_mild_count': hedge_mild_count,\n",
    "            'total_hedge_density': round(total_hedge_density, 4),\n",
    "            'strong_hedge_density': round(strong_hedge_density, 4),\n",
    "            'mild_hedge_density': round(mild_hedge_density, 4),\n",
    "            'hedge_strong_mild_ratio': round(hedge_strong_mild_ratio, 4),\n",
    "            'hedge_intensity_score': round(hedge_intensity_score, 4),\n",
    "            'high_hedge_sentences': high_hedge_sentences,\n",
    "            \n",
    "            # VAGUE WORD METRICS\n",
    "            'total_vague_words': total_vague_count,\n",
    "            'vague_strong_count': vague_strong_count,\n",
    "            'vague_mild_count': vague_mild_count,\n",
    "            'total_vague_density': round(total_vague_density, 4),\n",
    "            'strong_vague_density': round(strong_vague_density, 4),\n",
    "            'mild_vague_density': round(mild_vague_density, 4),\n",
    "            'vague_strong_mild_ratio': round(vague_strong_mild_ratio, 4),\n",
    "            'vague_intensity_score': round(vague_intensity_score, 4),\n",
    "            'vague_commitments': vague_commitments,\n",
    "            'total_commitment_words_found': total_commitment_words_found,\n",
    "            'commitment_timeline_pct': round(commitment_timeline_pct, 4),\n",
    "            'quantified_terms': quantified_terms,\n",
    "            'quantified_context_words': quantified_context_words,\n",
    "            'compared_context_words': compared_context_words,\n",
    "            'total_context_words': total_context_words,\n",
    "            'quantified_context_pct': round(quantified_context_pct, 4),\n",
    "            'compared_context_pct': round(compared_context_pct, 4),\n",
    "            'total_context_pct': round(total_context_pct, 4),\n",
    "            \n",
    "            # COMBINED METRICS\n",
    "            'total_unclear_words': total_unclear_words,\n",
    "            'total_unclear_density': round(total_unclear_density, 4),\n",
    "            'combined_strong_mild_ratio': round(combined_strong_mild_ratio, 4),\n",
    "            'hedge_vague_ratio': round(hedge_vague_ratio, 4),\n",
    "            'combined_intensity_score': round(combined_intensity_score, 4)\n",
    "        }\n",
    "        \n",
    "        data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create the combined hedge and vague analysis DataFrame\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create the dataframe\n",
    "try:\n",
    "    hedge_vague_df = create_hedge_vague_analysis_dataframe(hedge_results, vague_analysis_results)\n",
    "    \n",
    "    # Sort by organization and year for better readability\n",
    "    hedge_vague_df = hedge_vague_df.sort_values(['organization', 'year']).reset_index(drop=True)\n",
    "    \n",
    "    print(\"HEDGE & VAGUE WORDS COMBINED ANALYSIS DATAFRAME CREATED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"DataFrame shape: {hedge_vague_df.shape[0]} organizations × {hedge_vague_df.shape[1]} metrics\")\n",
    "    \n",
    "    # Export to Excel\n",
    "    excel_path = \"data/NLP/Results/Communication_Score_df_Hedge_Vague.xlsx\"\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(excel_path), exist_ok=True)\n",
    "    \n",
    "    # Export to Excel\n",
    "    hedge_vague_df.to_excel(excel_path, index=False, sheet_name='Hedge_Vague_Analysis')\n",
    "    \n",
    "    print(f\"\\nExported to Excel: {excel_path}\")\n",
    "    print(f\"Variable available as: hedge_vague_df\")\n",
    "    \n",
    "    # Column descriptions for reference\n",
    "    print(f\"\\nCOLUMN DESCRIPTIONS:\")\n",
    "    column_descriptions = {\n",
    "        # Basic identifiers\n",
    "        'organization': 'Organization name',\n",
    "        'year': 'Report year',\n",
    "        'meaningful_words': 'Total meaningful words in document',\n",
    "        \n",
    "        # Hedge word metrics\n",
    "        'total_hedge_words': 'Total hedge words found',\n",
    "        'hedge_strong_count': 'Strong hedge words count',\n",
    "        'hedge_mild_count': 'Mild hedge words count', \n",
    "        'total_hedge_density': 'Overall hedge density percentage',\n",
    "        'strong_hedge_density': 'Strong hedge density percentage',\n",
    "        'mild_hedge_density': 'Mild hedge density percentage',\n",
    "        'hedge_strong_mild_ratio': 'Ratio of strong to mild hedge words',\n",
    "        'hedge_intensity_score': 'Hedge intensity score',\n",
    "        'high_hedge_sentences': 'Number of sentences with 3+ hedge words',\n",
    "        \n",
    "        # Vague word metrics\n",
    "        'total_vague_words': 'Total vague words found',\n",
    "        'vague_strong_count': 'Strong vague words count',\n",
    "        'vague_mild_count': 'Mild vague words count',\n",
    "        'total_vague_density': 'Overall vague density percentage',\n",
    "        'strong_vague_density': 'Strong vague density percentage',\n",
    "        'mild_vague_density': 'Mild vague density percentage',\n",
    "        'vague_strong_mild_ratio': 'Ratio of strong to mild vague words',\n",
    "        'vague_intensity_score': 'Vague intensity score',\n",
    "        'vague_commitments': 'Number of vague commitments',\n",
    "        'total_commitment_words_found': 'Total commitment words found in document',\n",
    "        'commitment_timeline_pct': 'Percentage of commitment words with specific timelines',\n",
    "        'quantified_terms': 'Number of quantified terms',\n",
    "        'quantified_context_words': 'Number of context words that were quantified',\n",
    "        'compared_context_words': 'Number of context words that were compared', \n",
    "        'total_context_words': 'Total context-dependent words found',\n",
    "        'quantified_context_pct': 'Ratio of quantified to total context words',\n",
    "        'compared_context_pct': 'Ratio of compared to total context words',\n",
    "        'total_context_pct': 'Ratio of (quantified + compared) to total context words',\n",
    "        \n",
    "        # Combined metrics\n",
    "        'total_unclear_words': 'Combined hedge and vague words',\n",
    "        'total_unclear_density': 'Combined unclear communication density',\n",
    "        'combined_strong_mild_ratio': 'Combined strong to mild ratio',\n",
    "        'hedge_vague_ratio': 'Ratio of hedge to vague words',\n",
    "        'combined_intensity_score': 'Weighted combined intensity score'\n",
    "    }\n",
    "    \n",
    "    for col, desc in column_descriptions.items():\n",
    "        if col in hedge_vague_df.columns:\n",
    "            print(f\"  {col:<25}: {desc}\")\n",
    "    \n",
    "    print(f\"\\nData saved as: {excel_path}\")\n",
    "    print(f\"Variable available as: hedge_vague_df\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"COMBINED ANALYSIS SUMMARY STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total organizations analyzed: {hedge_vague_df['organization'].nunique()}\")\n",
    "    print(f\"Total documents: {len(hedge_vague_df)}\")\n",
    "    print(f\"Year range: {hedge_vague_df['year'].min()} - {hedge_vague_df['year'].max()}\")\n",
    "    \n",
    "    print(f\"\\nOverall Communication Metrics (averages):\")\n",
    "    print(f\"  Total meaningful words: {hedge_vague_df['meaningful_words'].mean():.0f}\")\n",
    "    print(f\"  Total unclear words: {hedge_vague_df['total_unclear_words'].mean():.1f}\")\n",
    "    print(f\"  Total unclear density: {hedge_vague_df['total_unclear_density'].mean():.2f}%\")\n",
    "    \n",
    "    print(f\"\\nHedge vs Vague Breakdown (averages):\")\n",
    "    print(f\"  Hedge words: {hedge_vague_df['total_hedge_words'].mean():.1f}\")\n",
    "    print(f\"  Vague words: {hedge_vague_df['total_vague_words'].mean():.1f}\")\n",
    "    print(f\"  Hedge density: {hedge_vague_df['total_hedge_density'].mean():.3f}%\")\n",
    "    print(f\"  Vague density: {hedge_vague_df['total_vague_density'].mean():.3f}%\")\n",
    "    print(f\"  Hedge/Vague ratio: {hedge_vague_df['hedge_vague_ratio'].mean():.2f}\")\n",
    "    \n",
    "    print(f\"\\nIntensity Scores (averages):\")\n",
    "    print(f\"  Hedge intensity: {hedge_vague_df['hedge_intensity_score'].mean():.2f}\")\n",
    "    print(f\"  Vague intensity: {hedge_vague_df['vague_intensity_score'].mean():.2f}\")\n",
    "    print(f\"  Combined intensity: {hedge_vague_df['combined_intensity_score'].mean():.2f}\")\n",
    "    \n",
    "except NameError as e:\n",
    "    print(f\"Error: Missing required variables. Please run hedge and vague analysis first.\")\n",
    "    print(f\"Error details: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating combined dataframe: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24177137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Define file path and output path\n",
    "output_path = \"data/NLP/Results/Communication_Score_df_Hedge_Vague.xlsx\"\n",
    "\n",
    "# Save the DataFrame to Excel\n",
    "hedge_vague_df.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "# Load the workbook and sheet\n",
    "wb = load_workbook(output_path)\n",
    "ws = wb.active  # There's only one sheet since we saved just one DataFrame\n",
    "\n",
    "# Auto-adjust column widths based on the longest string in each column\n",
    "for col in ws.columns:\n",
    "    max_length = 0\n",
    "    col_letter = get_column_letter(col[0].column)\n",
    "    for cell in col:\n",
    "        if cell.value:\n",
    "            max_length = max(max_length, len(str(cell.value)))\n",
    "    ws.column_dimensions[col_letter].width = max_length + 3  # Add padding\n",
    "\n",
    "# Define grey fill for alternating rows\n",
    "grey_fill = PatternFill(start_color=\"D9D9D9\", end_color=\"D9D9D9\", fill_type=\"solid\")\n",
    "\n",
    "# Alternate row colors by company\n",
    "prev_company = None\n",
    "use_grey = False\n",
    "for row in range(2, ws.max_row + 1):\n",
    "    current_company = ws[f\"A{row}\"].value  # Column A has the company names\n",
    "    if current_company != prev_company:\n",
    "        use_grey = not use_grey\n",
    "        prev_company = current_company\n",
    "\n",
    "    if use_grey:\n",
    "        for col in range(1, ws.max_column + 1):\n",
    "            ws.cell(row=row, column=col).fill = grey_fill\n",
    "\n",
    "# Save the final cleaned and formatted workbook\n",
    "wb.save(output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
