{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9dc236e",
   "metadata": {},
   "source": [
    "# Green Term Context Classification Analysis\n",
    "\n",
    "## Overview\n",
    "This module performs three-dimensional classification of environmental terms to assess claim substantiation and temporal orientation. It analyzes each green term's temporal context, quantification level, and evidence versus aspirational nature, directly supporting Substantiation Weakness and Temporal Orientation dimensions.\n",
    "\n",
    "## Three Classification Dimensions\n",
    "\n",
    "### 1. Temporal Classification\n",
    "- **Past context**: Completed actions (\"achieved\", \"implemented\", \"certified\")\n",
    "- **Present context**: Current operations (\"operates\", \"generates\", \"maintains\")  \n",
    "- **Future context**: Planned actions (\"will install\", \"targeting by 2030\")\n",
    "- **Method**: Dependency parsing to find governing verbs, temporal marker detection within 5-token window\n",
    "\n",
    "### 2. Quantification Classification  \n",
    "- **Highly quantified**: Strong connection to specific numbers with units (\"€50 million investment\", \"25% reduction\")\n",
    "- **Partially quantified**: Relative quantifiers without direct numbers (\"doubled capacity\", \"increased efficiency\")\n",
    "- **Non-quantified**: No meaningful quantification connections\n",
    "- **Method**: Token reconstruction for fragmented numbers, syntactic connection analysis\n",
    "\n",
    "### 3. Evidence vs. Aspirational Classification\n",
    "- **Strong evidence**: Completed, verifiable actions (\"installed 200MW\", \"achieved certification\")\n",
    "- **Moderate evidence**: Ongoing or implemented actions (\"operates facilities\", \"generates energy\")\n",
    "- **Strong aspirational**: Uncertain/conditional plans (\"should achieve\", \"might invest\", \"vision includes\")\n",
    "- **Moderate aspirational**: Committed future intentions (\"will install\", \"plan to reduce\", \"strategy targets\")\n",
    "- **Method**: Targeted marker detection around semantic governors (main controlling verbs)\n",
    "\n",
    "## Variables Produced for Communication Scoring\n",
    "According to the analysis framework:\n",
    "- **Quantified Claim Intensity** → Substantiation Weakness dimension\n",
    "- **Evidence-Based Claim Intensity** → Substantiation Weakness dimension  \n",
    "- **Aspirational Claim Intensity** → Substantiation Weakness dimension\n",
    "- **Future Orientation Ratio** → Temporal Orientation dimension\n",
    "\n",
    "## Intensity Scoring Method\n",
    "Weighted percentage calculation: ((strong count × 1.5) + (moderate count × 1.0)) ÷ total green terms × 100. This enables standardized comparison across documents while emphasizing stronger classifications.\n",
    "\n",
    "## Advanced Features\n",
    "Confidence scoring for each classification, combined cross-dimensional insights (future+aspirational, past+evidence), and semantic governor analysis for precise context determination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395af05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy_layout import spaCyLayout\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Load spaCy model and configure for large documents\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.max_length = 1_500_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2128fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Toggle between \"test\" and \"actual\"\n",
    "MODE = \"actual\"   \n",
    "\n",
    "# Define configuration based on mode\n",
    "if MODE == \"test\":\n",
    "    report_names = [ \n",
    "        \"Axpo_Holding_AG\", \"NEOEN_SA\"\n",
    "    ]\n",
    "    folders = {\n",
    "        \"2021\": Path(\"data/NLP/Testing/Reports/Clean/2021\"),\n",
    "        \"2022\": Path(\"data/NLP/Testing/Reports/Clean/2022\")\n",
    "    }\n",
    "\n",
    "elif MODE == \"actual\":\n",
    "    report_names = [ \n",
    "        \"Akenerji_Elektrik_Uretim_AS\",\n",
    "        \"Arendals_Fossekompani_ASA\",\n",
    "        \"Atlantica_Sustainable_Infrastructure_PLC\",\n",
    "        \"CEZ\",\n",
    "        \"EDF\",\n",
    "        \"EDP_Energias_de_Portugal_SA\",\n",
    "        \"Endesa\",\n",
    "        \"ERG_SpA\",\n",
    "        \"Orsted\",\n",
    "        \"Polska_Grupa_Energetyczna_PGE_SA\",\n",
    "        \"Romande_Energie_Holding_SA\",\n",
    "        \"Scatec_ASA\",\n",
    "        \"Solaria_Energia_y_Medio_Ambiente_SA\",\n",
    "        \"Terna_Energy_SA\"\n",
    "    ]\n",
    "\n",
    "    folders = {\n",
    "        \"2021\": Path(\"data/NLP/Reports/Cleanest/2021\"),\n",
    "        \"2022\": Path(\"data/NLP/Reports/Cleanest/2022\")\n",
    "    }\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid MODE. Use 'test' or 'actual'.\")\n",
    "\n",
    "# Check availability\n",
    "for name in report_names:\n",
    "    file_name = f\"{name}.txt\"\n",
    "    in_2021 = (folders[\"2021\"] / file_name).exists()\n",
    "    in_2022 = (folders[\"2022\"] / file_name).exists()\n",
    "    print(f\"{file_name}: 2021: {'YES' if in_2021 else 'NO'} | 2022: {'YES' if in_2022 else 'NO'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a05eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store processed docs\n",
    "documents = {}\n",
    "\n",
    "# Load and process all documents\n",
    "for version, folder_path in folders.items():\n",
    "    for name in report_names:\n",
    "        txt_path = folder_path / f\"{name}.txt\"\n",
    "        try:\n",
    "            with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            doc_key = f\"{name}_{version}\"\n",
    "            documents[doc_key] = nlp(text)\n",
    "            print(f\"Processed {doc_key}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {txt_path.name}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c41a0",
   "metadata": {},
   "source": [
    "## Green word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7833a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of green/sustainable nouns (lemmas) - EMISSIONS FOCUSED\n",
    "green_nouns = [\n",
    "    \"adaptation\", \"afforestation\", \"biodiversity\", \"biofuel\", \"biogas\", \"biomass\", \n",
    "    \"ccs\", \"ccus\", \"cogeneration\", \"decarbonisation\", \"decarbonization\", \"ecology\", \n",
    "    \"ecosystem\", \"electrification\", \"environment\", \"ess\", \"geothermal\", \"hydropower\", \n",
    "    \"improvement\", \"innovation\", \"mitigation\", \"optimization\", \"photovoltaic\", \n",
    "    \"preservation\", \"pv\", \"recycling\", \"reforestation\", \"regeneration\", \"renewable\", \n",
    "    \"renewables\", \"responsibility\", \"restoration\", \"solar\", \"sustainability\", \n",
    "    \"transition\", \"transparency\", \"wind\"\n",
    "]\n",
    "\n",
    "# Multi-word green nouns (lemmas) - EMISSIONS FOCUSED\n",
    "green_multiword_nouns = {\n",
    "    \"abatement\": [\"carbon\", \"co2\", \"co2e\", \"emission\", \"ghg\", \"pollution\"], \n",
    "    \"bond\": [\"climate\", \"green\", \"sustainability\"], \n",
    "    \"bonds\": [\"climate\", \"green\", \"sustainability\"], \n",
    "    \"capture\": [\"carbon\", \"co2\", \"ghg\", \"methane\"],\n",
    "    \"development\": [\"clean\", \"renewable\", \"sustainable\"],\n",
    "    \"economy\": [\"circular\", \"green\", \"hydrogen\", \"sustainable\"],\n",
    "    \"energy\": [\"alternative\", \"clean\", \"geothermal\", \"hydro\", \"renewable\", \"solar\", \"tidal\", \"wind\"],\n",
    "    \"farm\": [\"offshore\", \"solar\", \"wind\"],\n",
    "    \"farms\": [\"offshore\", \"solar\", \"wind\"],\n",
    "    \"finance\": [\"climate\", \"green\", \"sustainable\"],\n",
    "    \"financing\": [\"climate\", \"green\", \"sustainable\"],\n",
    "    \"fuel\": [\"alternative\", \"bio\", \"clean\", \"hydrogen\", \"synthetic\"],\n",
    "    \"fuels\": [\"alternative\", \"bio\", \"clean\", \"hydrogen\", \"synthetic\"], \n",
    "    \"fund\": [\"climate\", \"green\", \"sustainability\"], \n",
    "    \"funds\": [\"climate\", \"green\", \"sustainability\"], \n",
    "    \"generation\": [\"clean\", \"renewable\"],\n",
    "    \"goal\": [\"carbon\", \"climate\", \"emission\"], \n",
    "    \"goals\": [\"carbon\", \"climate\", \"emission\"],\n",
    "    \"growth\": [\"clean\", \"climate\", \"green\", \"renewable\", \"sustainable\"],\n",
    "    \"hydrogen\": [\"blue\", \"clean\", \"green\", \"renewable\"],\n",
    "    \"investment\": [\"clean\", \"climate\", \"green\", \"renewable\", \"sustainable\"],\n",
    "    \"investments\": [\"clean\", \"climate\", \"green\", \"renewable\", \"sustainable\"],\n",
    "    \"management\": [\"carbon\", \"energy\", \"environmental\", \"waste\"],\n",
    "    \"neutral\": [\"carbon\", \"climate\", \"co2\", \"emission\"],\n",
    "    \"neutrality\": [\"carbon\", \"climate\", \"co2\", \"emission\"],\n",
    "    \"panel\": [\"photovoltaic\", \"pv\", \"solar\"],\n",
    "    \"panels\": [\"photovoltaic\", \"pv\", \"solar\"],\n",
    "    \"plant\": [\"biomass\", \"geothermal\", \"hydro\", \"renewable\", \"solar\", \"wind\"],\n",
    "    \"plants\": [\"biomass\", \"geothermal\", \"hydro\", \"renewable\", \"solar\", \"wind\"],\n",
    "    \"power\": [\"clean\", \"geothermal\", \"hydro\", \"renewable\", \"solar\", \"tidal\", \"wind\"],\n",
    "    \"program\": [\"conservation\", \"efficiency\", \"renewable\", \"retrofit\"],\n",
    "    \"project\": [\"clean\", \"efficiency\", \"green\", \"renewable\"],\n",
    "    \"reduction\": [\"carbon\", \"co2\", \"emission\", \"ghg\", \"waste\"],\n",
    "    \"reductions\": [\"carbon\", \"co2\", \"emission\", \"ghg\", \"waste\"],\n",
    "    \"sequestration\": [\"carbon\", \"co2\", \"ghg\"], \n",
    "    \"solution\": [\"clean\", \"climate\", \"green\", \"renewable\"],\n",
    "    \"solutions\": [\"clean\", \"climate\", \"green\", \"renewable\"],\n",
    "    \"source\": [\"clean\", \"geothermal\", \"green\", \"hydro\", \"renewable\", \"solar\", \"wind\"], \n",
    "    \"sources\": [\"clean\", \"geothermal\", \"green\", \"hydro\", \"renewable\", \"solar\", \"wind\"],\n",
    "    \"standard\": [\"efficiency\", \"environmental\", \"green\", \"performance\"],\n",
    "    \"standards\": [\"efficiency\", \"environmental\", \"green\", \"performance\"],\n",
    "    \"station\": [\"clean\", \"geothermal\", \"green\", \"hydro\", \"renewable\", \"solar\", \"wind\"],\n",
    "    \"stations\": [\"clean\", \"geothermal\", \"green\", \"hydro\", \"renewable\", \"solar\", \"wind\"],\n",
    "    \"storage\": [\"carbon\", \"co2\"],\n",
    "    \"technology\": [\"carbon\", \"clean\", \"efficiency\", \"green\", \"renewable\"],\n",
    "    \"technologies\": [\"carbon\", \"clean\", \"efficiency\", \"green\", \"renewable\"],\n",
    "    \"transition\": [\"climate\", \"energy\", \"green\"],\n",
    "    \"turbine\": [\"offshore\", \"onshore\", \"wind\"],\n",
    "    \"turbines\": [\"offshore\", \"onshore\", \"wind\"],\n",
    "    \"zero\": [\"carbon\", \"climate\", \"emission\", \"footprint\", \"ghg\", \"net\", \"pollution\", \"waste\"]\n",
    "}\n",
    "\n",
    "# List of single-word green adjectives (lemmas) - EMISSIONS FOCUSED\n",
    "green_adjectives = [\n",
    "    \"circular\", \"clean\", \"decarbonise\", \"decarbonised\", \"decarbonising\", \"decarbonize\", \n",
    "    \"decarbonized\", \"decarbonizing\", \"durable\", \"ecological\", \"ecosystemic\", \"efficient\", \n",
    "    \"enriching\", \"environmental\", \"environmentally\", \"green\", \"hydroelectric\", \"innovative\",\n",
    "    \"optimal\", \"proenvironmental\", \"recover\", \"recoverable\", \"recovered\", \"recyclable\", \n",
    "    \"recycle\", \"recycled\", \"recycling\", \"reforested\", \"refurbish\", \"refurbished\", \n",
    "    \"regenerable\", \"regenerate\", \"regenerated\", \"renewable\", \"renewables\", \"responsible\", \n",
    "    \"restore\", \"restored\", \"reusable\", \"reuse\", \"sustainable\", \"sustainably\"\n",
    "]\n",
    "\n",
    "# Multi-word green adjectives (lemmas) - EMISSIONS FOCUSED\n",
    "green_multiword_adjectives = {\n",
    "    \"based\": [\"biomass\", \"nature\", \"plant\", \"renewable\"], \n",
    "    \"bio\": [\"energy\", \"fuel\", \"gas\", \"mass\"],\n",
    "    \"biomass\": [\"fired\", \"fueled\", \"powered\"],\n",
    "    \"carbon\": [\"captured\", \"capturing\", \"free\", \"low\", \"lower\", \"negative\", \"neutral\", \"non\", \"sequestered\", \"zero\"],\n",
    "    \"ccs\": [\"enabled\", \"equipped\", \"ready\"], \n",
    "    \"efficient\": [\"eco\", \"energy\", \"fuel\", \"high\", \"resource\"],\n",
    "    \"efficiency\": [\"eco\", \"energy\", \"fuel\", \"high\", \"resource\"],\n",
    "    \"electric\": [\"all\", \"geothermal\", \"hydro\", \"solar\", \"tidal\", \"wind\"],\n",
    "    \"emission\": [\"free\", \"low\", \"zero\"], \n",
    "    \"emissions\": [\"free\", \"low\", \"zero\"], \n",
    "    \"emitting\": [\"non\", \"zero\"],\n",
    "    \"energy\": [\"alternative\", \"clean\", \"efficient\", \"environmental\", \"renewable\", \"saved\"], \n",
    "    \"energies\": [\"alternative\", \"clean\", \"efficient\", \"environmental\", \"renewable\", \"saved\"],\n",
    "    \"environmental\": [\"certified\", \"energy\", \"management\", \"responsible\"],\n",
    "    \"free\": [\"carbon\", \"coal\", \"co2\", \"emission\", \"emissions\", \"fossil\", \"waste\"],\n",
    "    \"friendly\": [\"climate\", \"eco\", \"environment\", \"environmental\", \"planet\"], \n",
    "    \"friendlier\": [\"carbon\", \"climate\", \"eco\", \"environment\", \"environmental\", \"planet\"],\n",
    "    \"gas\": [\"bio\", \"renewable\"],\n",
    "    \"intensity\": [\"low\", \"reduced\"], \n",
    "    \"mitigating\": [\"key\"],\n",
    "    \"natural\": [\"protected\"],\n",
    "    \"negative\": [\"carbon\", \"co2\", \"emission\", \"emissions\"],\n",
    "    \"neutral\": [\"carbon\", \"climate\", \"co2\", \"emission\"],\n",
    "    \"oriented\": [\"climate\", \"ecosystem\", \"sustainability\"],\n",
    "    \"planting\": [\"forest\", \"tree\"],\n",
    "    \"pollutant\": [\"anti\", \"controlling\", \"low\", \"preventing\", \"reduced\", \"zero\"],\n",
    "    \"pollution\": [\"anti\", \"controlling\", \"low\", \"preventing\", \"reduced\", \"zero\"],\n",
    "    \"production\": [\"clean\", \"green\", \"renewable\", \"responsible\", \"sustainable\"],\n",
    "    \"proof\": [\"climate\", \"future\"],\n",
    "    \"protected\": [\"environmental\", \"natural\"],\n",
    "    \"reducing\": [\"carbon\", \"emission\", \"ghg\", \"pollution\"],\n",
    "    \"related\": [\"climate\", \"environment\", \"sustainability\"],\n",
    "    \"resilient\": [\"climate\", \"environment\"],\n",
    "    \"responsible\": [\"climate\", \"eco\", \"environmental\"],\n",
    "    \"saving\": [\"carbon\", \"energy\", \"fuel\", \"resource\"],\n",
    "    \"sustainable\": [\"certified\", \"climate\"],\n",
    "    \"zero\": [\"carbon\", \"emission\", \"net\"]\n",
    "}\n",
    "\n",
    "# List of green verbs (lemmas) - EMISSIONS FOCUSED\n",
    "green_verbs = [\n",
    "    \"afforeste\", \"afforesting\", \"conserve\", \"conserving\", \"decarbonize\", \"decarbonizing\", \n",
    "    \"decarbonise\", \"decarbonising\", \"electrify\", \"electrifying\", \"innovate\", \"innovating\",\n",
    "    \"minimize\", \"minimizing\", \"minimise\", \"minimising\", \"mitigate\", \"mitigating\", \n",
    "    \"optimize\", \"optimizing\", \"optimise\", \"optimising\", \"preserve\", \"preserving\", \n",
    "    \"recover\", \"recovering\", \"recycle\", \"recycling\", \"remediate\", \"remediating\", \n",
    "    \"reforest\", \"reforesting\", \"regenerate\", \"regenerating\", \"restore\", \"restoring\", \n",
    "    \"reuse\", \"reusing\", \"transition\", \"transitioning\", \"upgrade\", \"upgrading\"\n",
    "]\n",
    "\n",
    "# List of green adverbs (lemmas) - EMISSIONS FOCUSED\n",
    "green_adverbs = [\n",
    "    \"cleanly\", \"consciously\", \"ecologically\", \"efficiently\", \"environmentally\", \n",
    "    \"optimally\", \"renewably\", \"responsibly\", \"sustainably\"\n",
    "]\n",
    "\n",
    "# Multi-word green adverbs (lemmas) - EMISSIONS FOCUSED\n",
    "green_multiword_adverbs = {\n",
    "    \"aware\": [\"carbon\", \"climate\", \"eco\", \"environmentally\"],\n",
    "    \"based\": [\"nature\", \"plant\", \"renewable\", \"sustainability\"], \n",
    "    \"compliant\": [\"climate\", \"emission\", \"environmentally\"],\n",
    "    \"compatible\": [\"climate\", \"eco\", \"environmentally\"],\n",
    "    \"conscious\": [\"carbon\", \"climate\", \"eco\", \"environmentally\"],\n",
    "    \"designed\": [\"efficiently\", \"environmentally\", \"sustainably\"],\n",
    "    \"driven\": [\"climate\", \"renewable\", \"sustainability\"],\n",
    "    \"efficient\": [\"carbon\", \"energy\", \"fuel\", \"resource\"],\n",
    "    \"focused\": [\"climate\", \"emission\", \"environmental\", \"sustainability\"],\n",
    "    \"friendly\": [\"carbon\", \"climate\", \"co2\", \"eco\", \"environmentally\"],\n",
    "    \"managed\": [\"environmentally\", \"responsibly\", \"sustainably\"],\n",
    "    \"neutral\": [\"carbon\", \"climate\", \"emission\"],\n",
    "    \"operated\": [\"cleanly\", \"efficiently\", \"sustainably\"],\n",
    "    \"oriented\": [\"climate\", \"environment\", \"renewable\", \"sustainability\"],\n",
    "    \"produced\": [\"cleanly\", \"renewably\", \"sustainably\"],\n",
    "    \"responsible\": [\"carbon\", \"climate\", \"environmentally\"],\n",
    "    \"safe\": [\"climate\", \"eco\", \"environmentally\"],\n",
    "    \"sensitive\": [\"carbon\", \"climate\", \"environmentally\"],\n",
    "    \"sound\": [\"carbon\", \"climate\", \"environmentally\"]\n",
    "}\n",
    "\n",
    "# Neutral terms that become green with proper context (lemmas) - EMISSIONS FOCUSED\n",
    "neutral_nouns = [\n",
    "    \"co2\", \"co2e\", \"cooling\", \"emission\", \"emissions\", \"energy\", \"footprint\", \n",
    "    \"fuel\", \"ghg\", \"heating\", \"methane\", \"pollution\", \"transportation\", \"waste\"\n",
    "]\n",
    "\n",
    "# Multi-word neutral terms that become green with context (lemmas) - EMISSIONS FOCUSED\n",
    "neutral_multiword_nouns = {\n",
    "    \"consumption\": [\"coal\", \"electricity\", \"energy\", \"fuel\", \"gas\", \"oil\", \"power\"],\n",
    "    \"economy\": [\"carbon\"],\n",
    "    \"emission\": [\"annual\", \"baseline\", \"carbon\", \"co2\", \"co2e\", \"direct\", \"ghg\", \"indirect\", \"scope\", \"total\"],\n",
    "    \"emissions\": [\"annual\", \"baseline\", \"carbon\", \"co2\", \"co2e\", \"direct\", \"ghg\", \"indirect\", \"scope\", \"total\"],\n",
    "    \"footprint\": [\"carbon\", \"co2\", \"ecological\", \"emission\", \"environmental\", \"ghg\"],\n",
    "    \"impact\": [\"carbon\", \"climate\", \"ecological\", \"environmental\"],\n",
    "    \"intensity\": [\"carbon\", \"co2\", \"emission\", \"energy\", \"fuel\", \"ghg\"],\n",
    "    \"usage\": [\"electricity\", \"energy\", \"fuel\", \"power\", \"resource\"],\n",
    "    \"use\": [\"electricity\", \"energy\", \"fuel\", \"resource\"]\n",
    "}\n",
    "\n",
    "# Context words that indicate positive change - EMISSIONS FOCUSED\n",
    "improvement_adjectives = [\n",
    "    \"advanced\", \"best\", \"better\", \"boosted\", \"enhanced\", \"excellent\", \"exceptional\", \n",
    "    \"improved\", \"impressive\", \"optimized\", \"optimised\", \"outstanding\", \"positive\", \n",
    "    \"remarkable\", \"strengthened\", \"successful\", \"superior\", \"upgraded\"\n",
    "]\n",
    "\n",
    "# Improvement verbs that indicate positive change\n",
    "improvement_verbs = [\n",
    "    \"advance\", \"advancing\", \"boost\", \"boosting\", \"deliver\", \"delivering\", \"enhance\", \n",
    "    \"enhancing\", \"improve\", \"improving\", \"optimize\", \"optimizing\", \"optimise\", \n",
    "    \"optimising\", \"outperform\", \"outperforming\", \"strengthen\", \"strengthening\", \n",
    "    \"upgrade\", \"upgrading\"\n",
    "]\n",
    "# Improvement adverbs that indicate positive manner\n",
    "improvement_adverbs = [\n",
    "    \"effectively\", \"efficiently\", \"excellently\", \"meaningfully\", \"positively\", \"successfully\"\n",
    "]\n",
    "# Dependency-based green term patterns - EMISSIONS FOCUSED\n",
    "dependency_green_patterns = {\n",
    "    \"improvement_adjectives\": {\n",
    "        \"head_words\": [\"efficiency\", \"generation\", \"performance\", \"process\", \"solution\", \"system\", \"technology\"],\n",
    "        \"dependent_words\": [\"advanced\", \"better\", \"cleaner\", \"enhanced\", \"greener\", \"improved\", \"innovative\", \"optimized\"],\n",
    "        \"dependency_relations\": [\"amod\"],\n",
    "        \"description\": \"Improvement adjectives with green nouns\"\n",
    "    },\n",
    "    \"increase_positive\": {\n",
    "        \"head_words\": [\"boost\", \"develop\", \"enhance\", \"expand\", \"grow\", \"improve\", \"increase\", \"scale\"],\n",
    "        \"dependent_words\": [\"capture\", \"conservation\", \"efficiency\", \"recycling\", \"renewable\", \"sequestration\", \"sustainability\"],\n",
    "        \"dependency_relations\": [\"dobj\"],\n",
    "        \"description\": \"Positive action verbs with green objects\"\n",
    "    },\n",
    "    \"achieve_climate\": {\n",
    "        \"head_words\": [\"achieve\", \"attain\", \"deliver\", \"reach\", \"realize\"],\n",
    "        \"dependent_words\": [\"neutrality\", \"reduction\", \"transition\", \"zero\"],\n",
    "        \"dependency_relations\": [\"dobj\"],\n",
    "        \"description\": \"Achievement verbs with climate goals\"\n",
    "    },\n",
    "    \"transition_to\": {\n",
    "        \"head_words\": [\"change\", \"migrate\", \"move\", \"shift\", \"switch\", \"transition\"],\n",
    "        \"dependent_words\": [\"circular\", \"clean\", \"green\", \"renewable\", \"sustainable\", \"zero\"],\n",
    "        \"dependency_relations\": [\"prep\", \"pobj\"],\n",
    "        \"description\": \"Transition phrases\"\n",
    "    },\n",
    "    \"investment_in\": {\n",
    "        \"head_words\": [\"allocate\", \"finance\", \"funding\", \"invest\", \"investment\", \"spend\"],\n",
    "        \"dependent_words\": [\"carbon\", \"clean\", \"climate\", \"environmental\", \"green\", \"renewable\", \"sustainable\"],\n",
    "        \"dependency_relations\": [\"prep\", \"pobj\"],\n",
    "        \"description\": \"Investment in green areas\"\n",
    "    },\n",
    "    \"commitment_to\": {\n",
    "        \"head_words\": [\"commitment\", \"dedication\", \"pledge\", \"promise\"],\n",
    "        \"dependent_words\": [\"climate\", \"neutrality\", \"reduction\", \"sustainability\", \"zero\"],\n",
    "        \"dependency_relations\": [\"prep\", \"pobj\"],\n",
    "        \"description\": \"Commitments to climate goals\"\n",
    "    },\n",
    "    \"sustainable_adverbs\": {\n",
    "        \"head_words\": [\"develop\", \"generate\", \"grow\", \"manage\", \"manufacture\", \"operate\", \"produce\"],\n",
    "        \"dependent_words\": [\"cleanly\", \"efficiently\", \"environmentally\", \"responsibly\", \"sustainably\"],\n",
    "        \"dependency_relations\": [\"advmod\"],\n",
    "        \"description\": \"Sustainable manner of operations\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEGATION DETECTION SYSTEM - COMPREHENSIVE WORD LISTS\n",
    "\n",
    "# Direct negation words (alphabetically sorted)\n",
    "direct_negation_words = [\n",
    "    \"absence\", \"barely\", \"beneath\", \"below\", \"deficit\", \"devoid\", \"empty\", \"exempt\", \n",
    "    \"excluded\", \"failed\", \"gap\", \"hardly\", \"impossible\", \"inadequate\", \"insufficient\", \n",
    "    \"lacking\", \"minus\", \"never\", \"neither\", \"nil\", \"no\", \"nobody\", \"none\", \"nor\", \n",
    "    \"not\", \"nothing\", \"nowhere\", \"rarely\", \"scarcely\", \"seldom\", \"short\", \"shortfall\", \n",
    "    \"unable\", \"void\", \"without\", \"zero\"\n",
    "]\n",
    "\n",
    "# Negative descriptor words (alphabetically sorted)\n",
    "negative_descriptor_words = [\n",
    "    \"absence\", \"absent\", \"barrier\", \"blocks\", \"blocked\", \"cancel\", \"cancelled\", \n",
    "    \"cease\", \"ceased\", \"challenge\", \"concern\", \"constraint\", \"constraints\", \n",
    "    \"decline\", \"decrease\", \"deficit\", \"degrade\", \"degraded\", \"deteriorate\", \n",
    "    \"deteriorated\", \"difficulties\", \"difficulty\", \"drop\", \"downturn\", \"end\", \n",
    "    \"ended\", \"fail\", \"failed\", \"failure\", \"fall\", \"gap\", \"halt\", \"halted\", \n",
    "    \"harm\", \"hinders\", \"hindered\", \"impedes\", \"impeded\", \"inability\", \"incapable\", \n",
    "    \"inadequate\", \"ineffective\", \"inefficient\", \"insufficient\", \"issues\", \"lack\", \n",
    "    \"lacking\", \"lacks\", \"limited\", \"limiting\", \"loss\", \"missing\", \"obstacle\", \n",
    "    \"prevents\", \"prevented\", \"problem\", \"problems\", \"reduction\", \"reject\", \"rejected\", \n",
    "    \"refuse\", \"refused\", \"setback\", \"shortage\", \"shortfall\", \"shrinkage\", \"stop\", \n",
    "    \"stopped\", \"struggles\", \"struggling\", \"suspend\", \"suspended\", \"threat\", \n",
    "    \"unable\", \"unsuccessful\", \"weakening\", \"worse\", \"worsen\", \"worsening\"\n",
    "]\n",
    "\n",
    "# Phrasal negation patterns (alphabetically sorted)\n",
    "phrasal_negation_patterns = [\n",
    "    \"absence of\", \"anything but\", \"are no plans\", \"are not\", \"aren't\", \"can not\", \n",
    "    \"can't\", \"cannot\", \"cease\", \"could not\", \"couldn't\", \"devoid of\", \"did not\", \n",
    "    \"didn't\", \"do not\", \"does not\", \"doesn't\", \"don't\", \"exempt from\", \"failed to\", \n",
    "    \"failing to\", \"far from\", \"free from\", \"had not\", \"hadn't\", \"has not\", \"hasn't\", \n",
    "    \"have not\", \"haven't\", \"inability to\", \"incapable of\", \"inadequate to\", \n",
    "    \"insufficient to\", \"instead of\", \"is no plan\", \"is not\", \"isn't\", \"lack of\", \n",
    "    \"may not\", \"might not\", \"must not\", \"mustn't\", \"need not\", \"needn't\", \n",
    "    \"never\", \"no attempt to\", \"no chance to\", \"no effort to\", \"no intention to\", \n",
    "    \"no longer\", \"no means to\", \"no opportunity to\", \"no plans to\", \"no way to\", \n",
    "    \"not enough\", \"not yet\", \"other than\", \"rather than\", \"scarcity of\", \n",
    "    \"short of\", \"shortage of\", \"should not\", \"shouldn't\", \"too few\", \"too few to\", \n",
    "    \"too little\", \"too little to\", \"unable to\", \"was not\", \"wasn't\", \"were not\", \n",
    "    \"weren't\", \"will not\", \"without\", \"without any\", \"without proper\", \n",
    "    \"without sufficient\", \"without the\", \"won't\", \"would not\", \"wouldn't\"\n",
    "]\n",
    "\n",
    "# Negation prefixes (alphabetically sorted)\n",
    "negation_prefixes = [\n",
    "    \"anti\", \"counter\", \"de\", \"dis\", \"false\", \"fake\", \"il\", \"im\", \"in\", \"ir\", \n",
    "    \"mis\", \"non\", \"pseudo\", \"un\"\n",
    "]\n",
    "\n",
    "# Words that shouldn't cause negation in positive green contexts\n",
    "protected_green_context_words = [\n",
    "    \"abate\", \"curb\", \"cut\", \"declining\", \"decrease\", \"decreasing\", \"eliminate\", \n",
    "    \"fewer\", \"less\", \"low\", \"lower\", \"minimal\", \"reduce\", \"reduction\", \"slash\", \"zero\"\n",
    "]\n",
    "\n",
    "# Verbs that when negated should exclude green terms in their scope\n",
    "reduction_negative_verbs = [\n",
    "    \"abate\", \"abating\", \"control\", \"controlling\", \"curb\", \"curbing\", \"cut\", \"cutting\", \n",
    "    \"decrease\", \"decreasing\", \"eliminate\", \"eliminating\", \"limit\", \"limiting\", \n",
    "    \"lower\", \"lowering\", \"minimize\", \"minimizing\", \"minimise\", \"minimising\", \n",
    "    \"reduce\", \"reducing\", \"remove\", \"removing\", \"restrict\", \"restricting\", \n",
    "    \"slash\", \"slashing\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5afb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_context_for_neutral_term(doc, neutral_token_start, neutral_token_end, all_existing_terms):\n",
    "    \"\"\"\n",
    "    Find context words (negative/improvement) for a neutral term.\n",
    "    Returns: (context_found, context_word, context_type, context_relationship)\n",
    "    \"\"\"\n",
    "    # Get the main token of the neutral term\n",
    "    main_token = doc[neutral_token_start]\n",
    "    \n",
    "    # For multiword terms, find the syntactic head\n",
    "    if neutral_token_end > neutral_token_start:\n",
    "        tokens_in_term = [doc[i] for i in range(neutral_token_start, neutral_token_end + 1)]\n",
    "        for token in tokens_in_term:\n",
    "            if token.head not in tokens_in_term or token.head == token:\n",
    "                main_token = token\n",
    "                break\n",
    "    \n",
    "    # Apply distance filtering to head_subtree scope\n",
    "    head_subtree_all = list(main_token.head.subtree)\n",
    "    head_subtree_filtered = [\n",
    "        token for token in head_subtree_all \n",
    "        if (main_token.i - 8) <= token.i <= (main_token.i + 4)\n",
    "    ]\n",
    "    \n",
    "    # Define search scopes with restricted head_subtree\n",
    "    search_scopes = [\n",
    "        (\"subtree\", list(main_token.subtree)),\n",
    "        (\"ancestors\", list(main_token.ancestors)),\n",
    "        (\"head_subtree\", head_subtree_filtered)\n",
    "    ]\n",
    "    \n",
    "    # Collect context word lists\n",
    "    negative_context_words = (set(negative_descriptor_words) | \n",
    "                             set(protected_green_context_words) | \n",
    "                             set(reduction_negative_verbs))\n",
    "    \n",
    "    improvement_context_words = (set(improvement_adjectives) | \n",
    "                                set(improvement_verbs) | \n",
    "                                set(improvement_adverbs))\n",
    "    \n",
    "    # Search for context in each scope\n",
    "    for scope_name, scope_tokens in search_scopes:\n",
    "        for token in scope_tokens:\n",
    "            token_lemma = token.lemma_.lower()\n",
    "            \n",
    "            # Skip if this token is part of existing green terms\n",
    "            if is_word_part_of_green_terms(token, all_existing_terms):\n",
    "                continue\n",
    "            \n",
    "            # Check for negative context\n",
    "            if token_lemma in negative_context_words:\n",
    "                if is_context_related_to_term(token, main_token, scope_name):\n",
    "                    return True, token.text, \"negative\", token.dep_\n",
    "            \n",
    "            # Check for improvement context  \n",
    "            if token_lemma in improvement_context_words:\n",
    "                if is_context_related_to_term(token, main_token, scope_name):\n",
    "                    return True, token.text, \"improvement\", token.dep_\n",
    "    \n",
    "    return False, None, None, None\n",
    "\n",
    "def is_context_related_to_term(context_token, neutral_token, scope_name):\n",
    "    \"\"\"\n",
    "    Validate that context word is syntactically related to neutral term.\n",
    "    \"\"\"\n",
    "    # Direct dependency relationship\n",
    "    if context_token.head == neutral_token or neutral_token.head == context_token:\n",
    "        return True\n",
    "    \n",
    "    # Same head (siblings)\n",
    "    if context_token.head == neutral_token.head and scope_name == \"head_subtree\":\n",
    "        return True\n",
    "    \n",
    "    # Ancestor relationship\n",
    "    if scope_name == \"ancestors\":\n",
    "        neutral_ancestors = list(neutral_token.ancestors)\n",
    "        if context_token in neutral_ancestors[:3]:\n",
    "            return True\n",
    "    \n",
    "    # Subtree relationship\n",
    "    if scope_name == \"subtree\":\n",
    "        if context_token in list(neutral_token.subtree):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def find_context_dependent_terms(doc, excluded_positions, all_existing_terms):\n",
    "    \"\"\"\n",
    "    Find neutral terms that become green when paired with context words.\n",
    "    Returns: (found_terms, context_excluded_positions)\n",
    "    \"\"\"\n",
    "    found_terms = []\n",
    "    context_excluded_positions = set()\n",
    "    \n",
    "    # Find single neutral terms\n",
    "    for i, token in enumerate(doc):\n",
    "        if i in excluded_positions or i in context_excluded_positions:\n",
    "            continue\n",
    "        \n",
    "        lemma_lower = token.lemma_.lower()\n",
    "        if lemma_lower in neutral_nouns:\n",
    "            context_found, context_word, context_type, context_rel = find_context_for_neutral_term(\n",
    "                doc, i, i, all_existing_terms\n",
    "            )\n",
    "            \n",
    "            if context_found:\n",
    "                found_terms.append({\n",
    "                    'term': f\"{context_word} {token.text}\",\n",
    "                    'pos': f\"context_dependent_noun\",\n",
    "                    'start_idx': i,\n",
    "                    'end_idx': i,\n",
    "                    'sentence': token.sent,\n",
    "                    'neutral_part': token.text,\n",
    "                    'context_word': context_word,\n",
    "                    'context_type': context_type,\n",
    "                    'context_relationship': context_rel,\n",
    "                    'negated': False\n",
    "                })\n",
    "                context_excluded_positions.add(i)\n",
    "    \n",
    "    # Find multiword neutral terms\n",
    "    tokens = [token.lemma_.lower() for token in doc]\n",
    "    \n",
    "    for base_word, modifiers in neutral_multiword_nouns.items():\n",
    "        for modifier in modifiers:\n",
    "            # Pattern 1: modifier-base (e.g., \"carbon-footprint\")\n",
    "            pattern1 = f\"{modifier}-{base_word}\"\n",
    "            # Pattern 2: modifier base (e.g., \"carbon footprint\")\n",
    "            pattern2 = f\"{modifier} {base_word}\"\n",
    "            \n",
    "            # Search in original text for hyphenated version\n",
    "            text_lower = doc.text.lower()\n",
    "            for match in re.finditer(re.escape(pattern1), text_lower):\n",
    "                start_char = match.start()\n",
    "                end_char = match.end()\n",
    "                \n",
    "                # Find corresponding token positions\n",
    "                start_token_idx = None\n",
    "                end_token_idx = None\n",
    "                \n",
    "                for j, token in enumerate(doc):\n",
    "                    if token.idx <= start_char < token.idx + len(token.text):\n",
    "                        start_token_idx = j\n",
    "                    if token.idx < end_char <= token.idx + len(token.text):\n",
    "                        end_token_idx = j\n",
    "                        break\n",
    "                \n",
    "                if (start_token_idx is not None and end_token_idx is not None and\n",
    "                    start_token_idx not in excluded_positions and start_token_idx not in context_excluded_positions):\n",
    "                    \n",
    "                    context_found, context_word, context_type, context_rel = find_context_for_neutral_term(\n",
    "                        doc, start_token_idx, end_token_idx, all_existing_terms\n",
    "                    )\n",
    "                    \n",
    "                    if context_found:\n",
    "                        found_terms.append({\n",
    "                            'term': f\"{context_word} {pattern1}\",\n",
    "                            'pos': f\"context_dependent_multiword_noun\",\n",
    "                            'start_idx': start_token_idx,\n",
    "                            'end_idx': end_token_idx,\n",
    "                            'sentence': doc[start_token_idx].sent,\n",
    "                            'neutral_part': pattern1,\n",
    "                            'context_word': context_word,\n",
    "                            'context_type': context_type,\n",
    "                            'context_relationship': context_rel,\n",
    "                            'negated': False\n",
    "                        })\n",
    "                        for idx in range(start_token_idx, end_token_idx + 1):\n",
    "                            context_excluded_positions.add(idx)\n",
    "            \n",
    "            # Search for space-separated version\n",
    "            for j in range(len(tokens) - 1):\n",
    "                if (tokens[j] == modifier and tokens[j + 1] == base_word and\n",
    "                    j not in excluded_positions and j not in context_excluded_positions):\n",
    "                    \n",
    "                    context_found, context_word, context_type, context_rel = find_context_for_neutral_term(\n",
    "                        doc, j, j + 1, all_existing_terms\n",
    "                    )\n",
    "                    \n",
    "                    if context_found:\n",
    "                        found_terms.append({\n",
    "                            'term': f\"{context_word} {pattern2}\",\n",
    "                            'pos': f\"context_dependent_multiword_noun\",\n",
    "                            'start_idx': j,\n",
    "                            'end_idx': j + 1,\n",
    "                            'sentence': doc[j].sent,\n",
    "                            'neutral_part': pattern2,\n",
    "                            'context_word': context_word,\n",
    "                            'context_type': context_type,\n",
    "                            'context_relationship': context_rel,\n",
    "                            'negated': False\n",
    "                        })\n",
    "                        context_excluded_positions.add(j)\n",
    "                        context_excluded_positions.add(j + 1)\n",
    "    \n",
    "    return found_terms, context_excluded_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29005e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_word_part_of_green_terms(token, all_found_green_terms):\n",
    "    \"\"\"Check if a token is part of any existing green term.\"\"\"\n",
    "    for green_term in all_found_green_terms:\n",
    "        green_term_span = range(green_term['start_idx'], green_term['end_idx'] + 1)\n",
    "        if token.i in green_term_span:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def was_used_in_green_pattern(token, all_found_green_terms):\n",
    "    \"\"\"Check if this token was used to construct any green dependency pattern.\"\"\"\n",
    "    for green_term in all_found_green_terms:\n",
    "        if green_term['pos'].startswith('dependency_'):\n",
    "            green_term_span = range(green_term['start_idx'], green_term['end_idx'] + 1)\n",
    "            if token.i in green_term_span:\n",
    "                if token.lemma_.lower() in protected_green_context_words:\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def is_negation_related_to_term(negation_token, green_term_token, all_found_green_terms):\n",
    "    \"\"\"\n",
    "    Determine if a negation word is actually negating the green term.\n",
    "    \"\"\"\n",
    "    # Check if this negation word is part of any existing green term\n",
    "    if is_word_part_of_green_terms(negation_token, all_found_green_terms):\n",
    "        return False\n",
    "    \n",
    "    # Check if this negation word was used to construct any green pattern\n",
    "    if was_used_in_green_pattern(negation_token, all_found_green_terms):\n",
    "        return False\n",
    "    \n",
    "    # Check syntactic relationship using dependency parsing\n",
    "    if negation_token.head == green_term_token or green_term_token.head == negation_token:\n",
    "        return True\n",
    "    \n",
    "    # Check if they're in the same sentence and have close syntactic relationship\n",
    "    if negation_token.sent == green_term_token.sent:\n",
    "        negation_ancestors = [negation_token] + list(negation_token.ancestors)\n",
    "        green_ancestors = [green_term_token] + list(green_term_token.ancestors)\n",
    "        \n",
    "        # If they share close ancestors, they're likely related\n",
    "        for neg_ancestor in negation_ancestors[:3]:\n",
    "            if neg_ancestor in green_ancestors[:3]:\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def validate_phrasal_negation(sentence, phrase, green_term_token):\n",
    "    \"\"\"\n",
    "    Validate that a phrasal negation pattern actually applies to the green term.\n",
    "    \"\"\"\n",
    "    sentence_text = sentence.text.lower()\n",
    "    phrase_start = sentence_text.find(phrase)\n",
    "    \n",
    "    if phrase_start == -1:\n",
    "        return False\n",
    "    \n",
    "    # Calculate character positions\n",
    "    green_term_char_start = green_term_token.idx\n",
    "    green_term_char_end = green_term_token.idx + len(green_term_token.text)\n",
    "    \n",
    "    # Convert to sentence-relative positions\n",
    "    sentence_char_start = sentence.start_char\n",
    "    relative_green_start = green_term_char_start - sentence_char_start\n",
    "    relative_green_end = green_term_char_end - sentence_char_start\n",
    "    \n",
    "    phrase_end = phrase_start + len(phrase)\n",
    "    \n",
    "    # Check if the green term appears reasonably close after the negation phrase\n",
    "    if relative_green_start > phrase_end and relative_green_start - phrase_end < 50:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def find_negated_reduction_verb(main_token, all_found_green_terms):\n",
    "    \"\"\"\n",
    "    Check if the green term is the object/target of a negated reduction verb.\n",
    "    Returns: (is_negated, negation_type, negation_text, scope_found)\n",
    "    \"\"\"\n",
    "    # Get all context words that are already being used to create green terms\n",
    "    used_context_words = set()\n",
    "    for green_term in all_found_green_terms:\n",
    "        if 'context_word' in green_term:\n",
    "            context_word = green_term['context_word'].lower()\n",
    "            used_context_words.add(context_word)\n",
    "            used_context_words.add(context_word.rstrip('ed').rstrip('ing').rstrip('s'))\n",
    "    \n",
    "    # Get the context word for this specific green term (if it's context-dependent)\n",
    "    this_term_context_word_lemma = None\n",
    "    for green_term in all_found_green_terms:\n",
    "        term_span = range(green_term['start_idx'], green_term['end_idx'] + 1)\n",
    "        if main_token.i in term_span and 'context_word' in green_term:\n",
    "            context_word = green_term['context_word']\n",
    "            context_doc = nlp(context_word)\n",
    "            if len(context_doc) > 0:\n",
    "                this_term_context_word_lemma = context_doc[0].lemma_.lower()\n",
    "            else:\n",
    "                this_term_context_word_lemma = context_word.lower()\n",
    "            break\n",
    "    \n",
    "    # Check if this green term is the object of a reduction verb\n",
    "    for ancestor in main_token.ancestors:\n",
    "        if ancestor.lemma_.lower() in reduction_negative_verbs:\n",
    "            # Apply distance filtering to verb subtree and head_subtree scopes\n",
    "            verb_subtree_all = list(ancestor.subtree)\n",
    "            verb_subtree_filtered = [\n",
    "                token for token in verb_subtree_all \n",
    "                if (ancestor.i - 5) <= token.i < ancestor.i\n",
    "            ]\n",
    "            \n",
    "            verb_head_subtree_all = list(ancestor.head.subtree)\n",
    "            verb_head_subtree_filtered = [\n",
    "                token for token in verb_head_subtree_all \n",
    "                if (ancestor.i - 5) <= token.i < ancestor.i\n",
    "            ]\n",
    "            \n",
    "            # Check the verb's subtree, ancestors, and head.subtree for negation\n",
    "            verb_scopes = [\n",
    "                (\"verb_subtree\", verb_subtree_filtered),\n",
    "                (\"verb_ancestors\", list(ancestor.ancestors)),\n",
    "                (\"verb_head_subtree\", verb_head_subtree_filtered)\n",
    "            ]\n",
    "            \n",
    "            for scope_name, scope_tokens in verb_scopes:\n",
    "                for token in scope_tokens:\n",
    "                    if (token.dep_ == \"neg\" or \n",
    "                        token.lemma_.lower() in direct_negation_words):\n",
    "                        if not is_word_part_of_green_terms(token, all_found_green_terms):\n",
    "                            if token.lemma_.lower() not in used_context_words:\n",
    "                                if this_term_context_word_lemma is None or token.lemma_.lower() != this_term_context_word_lemma:\n",
    "                                    return True, f\"negated_reduction_verb_{scope_name}\", f\"'{token.text}' negating '{ancestor.text}'\", scope_name\n",
    "    \n",
    "    # Also check direct dependency relationships where green term is object\n",
    "    if main_token.dep_ in [\"dobj\", \"pobj\", \"attr\"]:\n",
    "        head_verb = main_token.head\n",
    "        if head_verb.lemma_.lower() in reduction_negative_verbs:\n",
    "            # Apply distance filtering to direct verb scopes\n",
    "            direct_verb_subtree_all = list(head_verb.subtree)\n",
    "            direct_verb_subtree_filtered = [\n",
    "                token for token in direct_verb_subtree_all \n",
    "                if (head_verb.i - 5) <= token.i < head_verb.i\n",
    "            ]\n",
    "            \n",
    "            direct_verb_head_subtree_all = list(head_verb.head.subtree)\n",
    "            direct_verb_head_subtree_filtered = [\n",
    "                token for token in direct_verb_head_subtree_all \n",
    "                if (head_verb.i - 5) <= token.i < head_verb.i\n",
    "            ]\n",
    "            \n",
    "            # Check if this verb is negated\n",
    "            verb_scopes = [\n",
    "                (\"direct_verb_subtree\", direct_verb_subtree_filtered),\n",
    "                (\"direct_verb_ancestors\", list(head_verb.ancestors)),\n",
    "                (\"direct_verb_head_subtree\", direct_verb_head_subtree_filtered)\n",
    "            ]\n",
    "            \n",
    "            for scope_name, scope_tokens in verb_scopes:\n",
    "                for token in scope_tokens:\n",
    "                    if (token.dep_ == \"neg\" or \n",
    "                        token.lemma_.lower() in direct_negation_words):\n",
    "                        if not is_word_part_of_green_terms(token, all_found_green_terms):\n",
    "                            if token.lemma_.lower() not in used_context_words:\n",
    "                                if this_term_context_word_lemma is None or token.lemma_.lower() != this_term_context_word_lemma:\n",
    "                                    return True, f\"negated_reduction_verb_{scope_name}\", f\"'{token.text}' negating '{head_verb.text}'\", scope_name\n",
    "    \n",
    "    return False, None, None, None\n",
    "\n",
    "def find_negation_in_multiple_scopes(main_token, all_found_green_terms):\n",
    "    \"\"\"\n",
    "    Enhanced negation detection that checks subtree, ancestors, and head.subtree.\n",
    "    Returns: (is_negated, negation_type, negation_text, scope_found)\n",
    "    \"\"\"\n",
    "    # Get all context words that are already being used to create green terms\n",
    "    used_context_words = set()\n",
    "    for green_term in all_found_green_terms:\n",
    "        if 'context_word' in green_term:\n",
    "            context_word = green_term['context_word'].lower()\n",
    "            used_context_words.add(context_word)\n",
    "            used_context_words.add(context_word.rstrip('ed').rstrip('ing').rstrip('s'))\n",
    "    \n",
    "    # Get the context word for this specific green term (if it's context-dependent)\n",
    "    this_term_context_word_lemma = None\n",
    "    for green_term in all_found_green_terms:\n",
    "        term_span = range(green_term['start_idx'], green_term['end_idx'] + 1)\n",
    "        if main_token.i in term_span and 'context_word' in green_term:\n",
    "            context_word = green_term['context_word']\n",
    "            context_doc = nlp(context_word)\n",
    "            if len(context_doc) > 0:\n",
    "                this_term_context_word_lemma = context_doc[0].lemma_.lower()\n",
    "            else:\n",
    "                this_term_context_word_lemma = context_word.lower()\n",
    "            break\n",
    "    \n",
    "    # Apply distance filtering to subtree and head_subtree scopes\n",
    "    subtree_all = list(main_token.subtree)\n",
    "    subtree_filtered = [\n",
    "        token for token in subtree_all \n",
    "        if (main_token.i - 6) <= token.i <= (main_token.i + 3)\n",
    "    ]\n",
    "    \n",
    "    head_subtree_all = list(main_token.head.subtree)\n",
    "    head_subtree_filtered = [\n",
    "        token for token in head_subtree_all \n",
    "        if (main_token.i - 6) <= token.i <= (main_token.i + 3)\n",
    "    ]\n",
    "    \n",
    "    # Define the three scopes to check\n",
    "    scopes = [\n",
    "        (\"subtree\", subtree_filtered),\n",
    "        (\"ancestors\", list(main_token.ancestors)),\n",
    "        (\"head_subtree\", head_subtree_filtered)\n",
    "    ]\n",
    "    \n",
    "    # Method 1: Check spaCy's built-in negation detection in all scopes\n",
    "    for scope_name, scope_tokens in scopes:\n",
    "        for token in scope_tokens:\n",
    "            if token.dep_ == \"neg\":\n",
    "                if not is_word_part_of_green_terms(token, all_found_green_terms):\n",
    "                    if token.lemma_.lower() not in used_context_words:\n",
    "                        if this_term_context_word_lemma is None or token.lemma_.lower() != this_term_context_word_lemma:\n",
    "                            return True, \"spacy_neg\", token.text, scope_name\n",
    "    \n",
    "    # Method 2: Check for direct negation words in all scopes\n",
    "    for scope_name, scope_tokens in scopes:\n",
    "        for token in scope_tokens:\n",
    "            if token.lemma_.lower() in direct_negation_words:\n",
    "                if (not is_word_part_of_green_terms(token, all_found_green_terms) and\n",
    "                    is_negation_related_to_term(token, main_token, all_found_green_terms)):\n",
    "                    if token.lemma_.lower() not in used_context_words:\n",
    "                        if this_term_context_word_lemma is None or token.lemma_.lower() != this_term_context_word_lemma:\n",
    "                            return True, \"direct_negation\", token.text, scope_name\n",
    "    \n",
    "    # Method 3: Check for negative descriptor words in all scopes\n",
    "    for scope_name, scope_tokens in scopes:\n",
    "        for token in scope_tokens:\n",
    "            if token.lemma_.lower() in negative_descriptor_words:\n",
    "                if (not is_word_part_of_green_terms(token, all_found_green_terms) and\n",
    "                    not was_used_in_green_pattern(token, all_found_green_terms) and\n",
    "                    is_negation_related_to_term(token, main_token, all_found_green_terms)):\n",
    "                    if token.lemma_.lower() not in used_context_words:\n",
    "                        if this_term_context_word_lemma is None or token.lemma_.lower() != this_term_context_word_lemma:\n",
    "                            return True, \"negative_descriptor\", token.text, scope_name\n",
    "    \n",
    "    return False, None, None, None\n",
    "\n",
    "def detect_negation_for_term(doc, green_term_start_idx, green_term_end_idx, all_found_green_terms):\n",
    "    \"\"\"\n",
    "    Comprehensive negation detection for a green term using multiple scopes and reduction verbs.\n",
    "    Returns: (is_negated, negation_type, negation_text, scope_found)\n",
    "    \"\"\"\n",
    "    # Get the main token of the green term (head token for multiword terms)\n",
    "    main_token = doc[green_term_start_idx]\n",
    "    \n",
    "    # For multiword terms, try to find the head token\n",
    "    if green_term_end_idx > green_term_start_idx:\n",
    "        tokens_in_term = [doc[i] for i in range(green_term_start_idx, green_term_end_idx + 1)]\n",
    "        for token in tokens_in_term:\n",
    "            if token.head not in tokens_in_term or token.head == token:\n",
    "                main_token = token\n",
    "                break\n",
    "    \n",
    "    # Check for negated reduction verbs first\n",
    "    is_negated, negation_type, negation_text, scope = find_negated_reduction_verb(main_token, all_found_green_terms)\n",
    "    if is_negated:\n",
    "        return True, negation_type, negation_text, scope\n",
    "    \n",
    "    # Enhanced method: check subtree, ancestors, and head.subtree\n",
    "    is_negated, negation_type, negation_text, scope = find_negation_in_multiple_scopes(main_token, all_found_green_terms)\n",
    "    if is_negated:\n",
    "        return True, f\"{negation_type}_{scope}\", negation_text, scope\n",
    "    \n",
    "    # Check for prefix negation on all tokens in the term\n",
    "    for token_idx in range(green_term_start_idx, green_term_end_idx + 1):\n",
    "        token = doc[token_idx]\n",
    "        \n",
    "        if token.i >= 2:\n",
    "            prev_token = doc[token.i - 1]\n",
    "            if prev_token.text == \"-\":\n",
    "                prev_prev_token = doc[token.i - 2]\n",
    "                if prev_prev_token.text.lower() in negation_prefixes:\n",
    "                    return True, \"prefix_negation\", f\"{prev_prev_token.text}-\", \"prefix\"\n",
    "    \n",
    "    # Check for phrasal negation patterns\n",
    "    sentence = main_token.sent\n",
    "    sentence_text = sentence.text.lower()\n",
    "    for phrase in phrasal_negation_patterns:\n",
    "        if phrase in sentence_text:\n",
    "            if validate_phrasal_negation(sentence, phrase, main_token):\n",
    "                return True, \"phrasal_negation\", phrase, \"sentence\"\n",
    "    \n",
    "    return False, None, None, None\n",
    "\n",
    "def filter_negated_terms(all_found_terms, doc):\n",
    "    \"\"\"\n",
    "    Filter out negated terms and return both valid and negated term lists.\n",
    "    \"\"\"\n",
    "    valid_terms = []\n",
    "    negated_terms = []\n",
    "    \n",
    "    for term_info in all_found_terms:\n",
    "        is_negated, negation_type, negation_text, scope = detect_negation_for_term(\n",
    "            doc, \n",
    "            term_info['start_idx'], \n",
    "            term_info['end_idx'], \n",
    "            all_found_terms\n",
    "        )\n",
    "        \n",
    "        if is_negated:\n",
    "            term_info['negated'] = True\n",
    "            term_info['negation_type'] = negation_type\n",
    "            term_info['negation_text'] = negation_text\n",
    "            term_info['negation_scope'] = scope\n",
    "            negated_terms.append(term_info)\n",
    "        else:\n",
    "            term_info['negated'] = False\n",
    "            valid_terms.append(term_info)\n",
    "    \n",
    "    return valid_terms, negated_terms\n",
    "\n",
    "def get_negation_statistics(negated_terms):\n",
    "    \"\"\"\n",
    "    Generate statistics about negation patterns including scope and reduction verb information.\n",
    "    \"\"\"\n",
    "    if not negated_terms:\n",
    "        return {}\n",
    "    \n",
    "    negation_types = Counter()\n",
    "    negated_by_pos = Counter()\n",
    "    negation_scopes = Counter()\n",
    "    reduction_verb_negations = Counter()\n",
    "    \n",
    "    for term in negated_terms:\n",
    "        negation_types[term['negation_type']] += 1\n",
    "        negated_by_pos[term['pos']] += 1\n",
    "        if 'negation_scope' in term:\n",
    "            negation_scopes[term['negation_scope']] += 1\n",
    "        if 'reduction_verb' in term.get('negation_type', ''):\n",
    "            reduction_verb_negations[term['negation_type']] += 1\n",
    "    \n",
    "    return {\n",
    "        'total_negated': len(negated_terms),\n",
    "        'by_type': dict(negation_types),\n",
    "        'by_pos': dict(negated_by_pos),\n",
    "        'by_scope': dict(negation_scopes),\n",
    "        'reduction_verb_negations': dict(reduction_verb_negations),\n",
    "        'examples': [(term['term'], term['negation_type'], term['negation_text'], \n",
    "                     term.get('negation_scope', 'unknown')) \n",
    "                    for term in negated_terms[:8]]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b33a98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_multiword_terms(doc, multiword_dict, pos_tag):\n",
    "    \"\"\"Find multiword terms in document and return positions to exclude from single word counting.\"\"\"\n",
    "    found_terms = []\n",
    "    excluded_positions = set()\n",
    "    \n",
    "    # Convert doc to lowercase tokens for matching\n",
    "    tokens = [token.lemma_.lower() for token in doc]\n",
    "    \n",
    "    for base_word, modifiers in multiword_dict.items():\n",
    "        for modifier in modifiers:\n",
    "            # Pattern 1: modifier-base (e.g., \"eco-friendly\")\n",
    "            pattern1 = f\"{modifier}-{base_word}\"\n",
    "            # Pattern 2: modifier base (e.g., \"eco friendly\") \n",
    "            pattern2 = f\"{modifier} {base_word}\"\n",
    "            \n",
    "            # Search in original text for hyphenated version\n",
    "            text_lower = doc.text.lower()\n",
    "            for match in re.finditer(re.escape(pattern1), text_lower):\n",
    "                start_char = match.start()\n",
    "                end_char = match.end()\n",
    "                \n",
    "                # Find corresponding token positions\n",
    "                start_token_idx = None\n",
    "                end_token_idx = None\n",
    "                \n",
    "                for i, token in enumerate(doc):\n",
    "                    if token.idx <= start_char < token.idx + len(token.text):\n",
    "                        start_token_idx = i\n",
    "                    if token.idx < end_char <= token.idx + len(token.text):\n",
    "                        end_token_idx = i\n",
    "                        break\n",
    "                \n",
    "                if start_token_idx is not None and end_token_idx is not None:\n",
    "                    found_terms.append({\n",
    "                        'term': pattern1,\n",
    "                        'pos': pos_tag,\n",
    "                        'start_idx': start_token_idx,\n",
    "                        'end_idx': end_token_idx,\n",
    "                        'sentence': doc[start_token_idx].sent,\n",
    "                        'negated': False\n",
    "                    })\n",
    "                    # Mark positions as excluded\n",
    "                    for idx in range(start_token_idx, end_token_idx + 1):\n",
    "                        excluded_positions.add(idx)\n",
    "            \n",
    "            # Search for space-separated version\n",
    "            for i in range(len(tokens) - 1):\n",
    "                if tokens[i] == modifier and tokens[i + 1] == base_word:\n",
    "                    found_terms.append({\n",
    "                        'term': pattern2,\n",
    "                        'pos': pos_tag,\n",
    "                        'start_idx': i,\n",
    "                        'end_idx': i + 1,\n",
    "                        'sentence': doc[i].sent,\n",
    "                        'negated': False\n",
    "                    })\n",
    "                    # Mark positions as excluded\n",
    "                    excluded_positions.add(i)\n",
    "                    excluded_positions.add(i + 1)\n",
    "    \n",
    "    return found_terms, excluded_positions\n",
    "\n",
    "def find_single_word_terms(doc, word_list, pos_tag, excluded_positions):\n",
    "    \"\"\"Find single word terms, excluding positions already counted in multiword terms.\"\"\"\n",
    "    found_terms = []\n",
    "    \n",
    "    for i, token in enumerate(doc):\n",
    "        if i in excluded_positions:\n",
    "            continue\n",
    "            \n",
    "        lemma_lower = token.lemma_.lower()\n",
    "        if lemma_lower in word_list:\n",
    "            # Skip \"sustainability\" if followed by \"report\"\n",
    "            if lemma_lower == \"sustainability\" and i + 1 < len(doc):\n",
    "                next_token = doc[i + 1]\n",
    "                if next_token.lemma_.lower() in {\"report\", \"reporting\"}:\n",
    "                    continue\n",
    "\n",
    "            # Skip \"PV\" if it's between brackets like \"(PV)\"\n",
    "            if lemma_lower == \"pv\":\n",
    "                left_char = doc.text[token.idx - 1] if token.idx > 0 else \"\"\n",
    "                right_char = doc.text[token.idx + len(token)] if token.idx + len(token) < len(doc.text) else \"\"\n",
    "                if left_char == \"(\" and right_char == \")\":\n",
    "                    continue\n",
    "\n",
    "            found_terms.append({\n",
    "                'term': lemma_lower,\n",
    "                'pos': pos_tag,\n",
    "                'start_idx': i,\n",
    "                'end_idx': i,\n",
    "                'sentence': token.sent,\n",
    "                'negated': False\n",
    "            })\n",
    "\n",
    "    return found_terms\n",
    "\n",
    "def find_dependency_patterns(doc, excluded_positions):\n",
    "    \"\"\"Find dependency-based green patterns.\"\"\"\n",
    "    found_terms = []\n",
    "    dependency_excluded_positions = set()\n",
    "    \n",
    "    for pattern_name, pattern_info in dependency_green_patterns.items():\n",
    "        head_words = pattern_info[\"head_words\"]\n",
    "        dependent_words = pattern_info[\"dependent_words\"]\n",
    "        dependency_relations = pattern_info[\"dependency_relations\"]\n",
    "        \n",
    "        for token in doc:\n",
    "            # Skip if token is already counted\n",
    "            if token.i in excluded_positions or token.i in dependency_excluded_positions:\n",
    "                continue\n",
    "                \n",
    "            token_lemma = token.lemma_.lower()\n",
    "            \n",
    "            # Check if current token is a head word\n",
    "            if token_lemma in head_words:\n",
    "                # Look for dependents\n",
    "                for child in token.subtree:\n",
    "                    if (child.dep_ in dependency_relations and \n",
    "                        child.lemma_.lower() in dependent_words and\n",
    "                        child.i not in excluded_positions and\n",
    "                        child.i not in dependency_excluded_positions):\n",
    "                        \n",
    "                        # Found a match\n",
    "                        term_text = f\"{child.text} {token.text}\"\n",
    "                        found_terms.append({\n",
    "                            'term': term_text,\n",
    "                            'pos': f\"dependency_{pattern_name}\",\n",
    "                            'start_idx': min(child.i, token.i),\n",
    "                            'end_idx': max(child.i, token.i),\n",
    "                            'sentence': token.sent,\n",
    "                            'pattern': pattern_name,\n",
    "                            'dependency': child.dep_,\n",
    "                            'negated': False\n",
    "                        })\n",
    "                        # Mark both positions as used\n",
    "                        dependency_excluded_positions.add(child.i)\n",
    "                        dependency_excluded_positions.add(token.i)\n",
    "            \n",
    "            # Check if current token is a dependent word\n",
    "            elif token_lemma in dependent_words:\n",
    "                # Look at its head\n",
    "                head = token.head\n",
    "                if (token.dep_ in dependency_relations and\n",
    "                    head.lemma_.lower() in head_words and\n",
    "                    head.i not in excluded_positions and\n",
    "                    head.i not in dependency_excluded_positions):\n",
    "                    \n",
    "                    # Found a match\n",
    "                    term_text = f\"{token.text} {head.text}\"\n",
    "                    found_terms.append({\n",
    "                        'term': term_text,\n",
    "                        'pos': f\"dependency_{pattern_name}\",\n",
    "                        'start_idx': min(token.i, head.i),\n",
    "                        'end_idx': max(token.i, head.i),\n",
    "                        'sentence': token.sent,\n",
    "                        'pattern': pattern_name,\n",
    "                        'dependency': token.dep_,\n",
    "                        'negated': False\n",
    "                    })\n",
    "                    # Mark both positions as used\n",
    "                    dependency_excluded_positions.add(token.i)\n",
    "                    dependency_excluded_positions.add(head.i)\n",
    "    \n",
    "    return found_terms, dependency_excluded_positions\n",
    "\n",
    "def get_context_words(doc, start_idx, end_idx, context_size=5):\n",
    "    \"\"\"Extract context words around the found term.\"\"\"\n",
    "    # Get the sentence containing the term\n",
    "    sentence = doc[start_idx].sent\n",
    "    sent_start = sentence.start\n",
    "    sent_end = sentence.end\n",
    "    \n",
    "    # Calculate context boundaries within the sentence\n",
    "    context_start = max(sent_start, start_idx - context_size)\n",
    "    context_end = min(sent_end, end_idx + context_size + 1)\n",
    "    \n",
    "    # Extract context tokens\n",
    "    context_tokens = []\n",
    "    for i in range(context_start, context_end):\n",
    "        if i == start_idx and start_idx == end_idx:\n",
    "            # Single word term - highlight it\n",
    "            context_tokens.append(f\"**{doc[i].text}**\")\n",
    "        elif i == start_idx:\n",
    "            # Start of multiword term\n",
    "            context_tokens.append(f\"**{doc[i].text}\")\n",
    "        elif i == end_idx:\n",
    "            # End of multiword term\n",
    "            context_tokens.append(f\"{doc[i].text}**\")\n",
    "        elif start_idx < i < end_idx:\n",
    "            # Middle of multiword term\n",
    "            context_tokens.append(doc[i].text)\n",
    "        else:\n",
    "            # Regular context word\n",
    "            context_tokens.append(doc[i].text)\n",
    "    \n",
    "    return \" \".join(context_tokens)\n",
    "\n",
    "def analyze_green_terms_with_negation(doc, doc_name):\n",
    "    \"\"\"\n",
    "    Analyze all green terms in a document with enhanced multi-scope negation detection and context-dependent terms.\n",
    "    Returns: (valid_counts, valid_terms, negated_terms, original_counts)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ANALYZING: {doc_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    all_found_terms = []\n",
    "    all_excluded_positions = set()\n",
    "    \n",
    "    # Step 1: Find multiword terms first (all types together)\n",
    "    multiword_noun_terms, excluded_noun_pos = find_multiword_terms(doc, green_multiword_nouns, \"multiword_noun\")\n",
    "    multiword_adj_terms, excluded_adj_pos = find_multiword_terms(doc, green_multiword_adjectives, \"multiword_adjective\")\n",
    "    multiword_adv_terms, excluded_adv_pos = find_multiword_terms(doc, green_multiword_adverbs, \"multiword_adverb\")\n",
    "    \n",
    "    # Combine all multiword terms and check for overlaps\n",
    "    all_multiword_terms = multiword_noun_terms + multiword_adj_terms + multiword_adv_terms\n",
    "    \n",
    "    # Remove duplicates based on position overlap\n",
    "    filtered_multiword_terms = []\n",
    "    used_positions = set()\n",
    "    \n",
    "    # Sort by start position to process in order\n",
    "    all_multiword_terms.sort(key=lambda x: x['start_idx'])\n",
    "    \n",
    "    for term in all_multiword_terms:\n",
    "        # Check if this term overlaps with any already used positions\n",
    "        term_positions = set(range(term['start_idx'], term['end_idx'] + 1))\n",
    "        if not term_positions.intersection(used_positions):\n",
    "            filtered_multiword_terms.append(term)\n",
    "            used_positions.update(term_positions)\n",
    "            all_excluded_positions.update(term_positions)\n",
    "    \n",
    "    all_found_terms.extend(filtered_multiword_terms)\n",
    "    \n",
    "    # Step 2: Find dependency patterns (excluding already counted positions)\n",
    "    dependency_terms, dependency_excluded_pos = find_dependency_patterns(doc, all_excluded_positions)\n",
    "    \n",
    "    # Filter dependency terms to avoid overlap with multiword terms\n",
    "    filtered_dependency_terms = []\n",
    "    for dep_term in dependency_terms:\n",
    "        dep_positions = set(range(dep_term['start_idx'], dep_term['end_idx'] + 1))\n",
    "        if not dep_positions.intersection(all_excluded_positions):\n",
    "            filtered_dependency_terms.append(dep_term)\n",
    "            all_excluded_positions.update(dep_positions)\n",
    "    \n",
    "    all_found_terms.extend(filtered_dependency_terms)\n",
    "    \n",
    "    # Step 3: Find context-dependent terms (excluding already counted positions)\n",
    "    context_dependent_terms, context_excluded_pos = find_context_dependent_terms(doc, all_excluded_positions, all_found_terms)\n",
    "    \n",
    "    # Filter context-dependent terms to avoid overlap\n",
    "    filtered_context_terms = []\n",
    "    for ctx_term in context_dependent_terms:\n",
    "        ctx_positions = set(range(ctx_term['start_idx'], ctx_term['end_idx'] + 1))\n",
    "        if not ctx_positions.intersection(all_excluded_positions):\n",
    "            filtered_context_terms.append(ctx_term)\n",
    "            all_excluded_positions.update(ctx_positions)\n",
    "    \n",
    "    all_found_terms.extend(filtered_context_terms)\n",
    "    \n",
    "    # Step 4: Find single word terms with position tracking to prevent double counting\n",
    "    # Process in priority order: verbs > nouns > adjectives > adverbs\n",
    "    \n",
    "    # Priority 1: Verbs (actions are often most important)\n",
    "    single_verb_terms = find_single_word_terms(doc, green_verbs, \"verb\", all_excluded_positions)\n",
    "    for term in single_verb_terms:\n",
    "        all_excluded_positions.add(term['start_idx'])\n",
    "    all_found_terms.extend(single_verb_terms)\n",
    "    \n",
    "    # Priority 2: Nouns (concrete green concepts)\n",
    "    single_noun_terms = find_single_word_terms(doc, green_nouns, \"noun\", all_excluded_positions)\n",
    "    for term in single_noun_terms:\n",
    "        all_excluded_positions.add(term['start_idx'])\n",
    "    all_found_terms.extend(single_noun_terms)\n",
    "    \n",
    "    # Priority 3: Adjectives (descriptive green terms)\n",
    "    single_adj_terms = find_single_word_terms(doc, green_adjectives, \"adjective\", all_excluded_positions)\n",
    "    for term in single_adj_terms:\n",
    "        all_excluded_positions.add(term['start_idx'])\n",
    "    all_found_terms.extend(single_adj_terms)\n",
    "    \n",
    "    # Priority 4: Adverbs (manner of green actions)\n",
    "    single_adv_terms = find_single_word_terms(doc, green_adverbs, \"adverb\", all_excluded_positions)\n",
    "    for term in single_adv_terms:\n",
    "        all_excluded_positions.add(term['start_idx'])\n",
    "    all_found_terms.extend(single_adv_terms)\n",
    "    \n",
    "    # Step 5: Apply enhanced multi-scope negation detection\n",
    "    print(f\"Found {len(all_found_terms)} green terms before negation filtering...\")\n",
    "    valid_terms, negated_terms = filter_negated_terms(all_found_terms, doc)\n",
    "    print(f\"After negation filtering: {len(valid_terms)} valid terms, {len(negated_terms)} negated terms\")\n",
    "    \n",
    "    # Count by type for original, valid, and negated terms\n",
    "    original_type_counts = Counter()\n",
    "    valid_type_counts = Counter()\n",
    "    negated_type_counts = Counter()\n",
    "    \n",
    "    for term_info in all_found_terms:\n",
    "        original_type_counts[term_info['pos']] += 1\n",
    "    \n",
    "    for term_info in valid_terms:\n",
    "        valid_type_counts[term_info['pos']] += 1\n",
    "    \n",
    "    for term_info in negated_terms:\n",
    "        negated_type_counts[term_info['pos']] += 1\n",
    "    \n",
    "    # Print comprehensive counts\n",
    "    print(f\"\\nGREEN TERMS COUNTS (ORIGINAL | NEGATED | VALID):\")\n",
    "    print(f\"Nouns: {original_type_counts['noun']} | {negated_type_counts['noun']} | {valid_type_counts['noun']}\")\n",
    "    print(f\"Multiword Nouns: {original_type_counts['multiword_noun']} | {negated_type_counts['multiword_noun']} | {valid_type_counts['multiword_noun']}\")\n",
    "    print(f\"Adjectives: {original_type_counts['adjective']} | {negated_type_counts['adjective']} | {valid_type_counts['adjective']}\")  \n",
    "    print(f\"Multiword Adjectives: {original_type_counts['multiword_adjective']} | {negated_type_counts['multiword_adjective']} | {valid_type_counts['multiword_adjective']}\")\n",
    "    print(f\"Verbs: {original_type_counts['verb']} | {negated_type_counts['verb']} | {valid_type_counts['verb']}\")\n",
    "    print(f\"Adverbs: {original_type_counts['adverb']} | {negated_type_counts['adverb']} | {valid_type_counts['adverb']}\")\n",
    "    print(f\"Multiword Adverbs: {original_type_counts['multiword_adverb']} | {negated_type_counts['multiword_adverb']} | {valid_type_counts['multiword_adverb']}\")\n",
    "    print(f\"Context-Dependent Nouns: {original_type_counts['context_dependent_noun']} | {negated_type_counts['context_dependent_noun']} | {valid_type_counts['context_dependent_noun']}\")\n",
    "    print(f\"Context-Dependent Multiword Nouns: {original_type_counts['context_dependent_multiword_noun']} | {negated_type_counts['context_dependent_multiword_noun']} | {valid_type_counts['context_dependent_multiword_noun']}\")\n",
    "    \n",
    "    # Count dependency patterns\n",
    "    original_dependency_counts = Counter()\n",
    "    valid_dependency_counts = Counter()\n",
    "    negated_dependency_counts = Counter()\n",
    "    \n",
    "    for term_info in all_found_terms:\n",
    "        if term_info['pos'].startswith('dependency_'):\n",
    "            original_dependency_counts[term_info['pos']] += 1\n",
    "    \n",
    "    for term_info in valid_terms:\n",
    "        if term_info['pos'].startswith('dependency_'):\n",
    "            valid_dependency_counts[term_info['pos']] += 1\n",
    "            \n",
    "    for term_info in negated_terms:\n",
    "        if term_info['pos'].startswith('dependency_'):\n",
    "            negated_dependency_counts[term_info['pos']] += 1\n",
    "    \n",
    "    print(f\"Dependency Patterns: {sum(original_dependency_counts.values())} | {sum(negated_dependency_counts.values())} | {sum(valid_dependency_counts.values())}\")\n",
    "    \n",
    "    for dep_type in original_dependency_counts.keys():\n",
    "        pattern_name = dep_type.replace('dependency_', '')\n",
    "        orig_count = original_dependency_counts[dep_type]\n",
    "        neg_count = negated_dependency_counts[dep_type]\n",
    "        val_count = valid_dependency_counts[dep_type]\n",
    "        print(f\"  {pattern_name}: {orig_count} | {neg_count} | {val_count}\")\n",
    "    \n",
    "    print(f\"TOTAL: {sum(original_type_counts.values())} | {sum(negated_type_counts.values())} | {sum(valid_type_counts.values())}\")\n",
    "    \n",
    "    # Generate negation statistics\n",
    "    negation_stats = get_negation_statistics(negated_terms)\n",
    "    if negation_stats:\n",
    "        print(f\"\\nENHANCED NEGATION ANALYSIS:\")\n",
    "        print(f\"Total negated terms: {negation_stats['total_negated']}\")\n",
    "        print(f\"Negation types: {negation_stats['by_type']}\")\n",
    "        print(f\"Negation scopes: {negation_stats['by_scope']}\")\n",
    "        print(f\"Examples of negated terms (with scope):\")\n",
    "        for term, neg_type, neg_text, scope in negation_stats['examples']:\n",
    "            print(f\"  - '{term}' (negated by: {neg_text}, type: {neg_type}, scope: {scope})\")\n",
    "    \n",
    "    # Sort valid terms by their position in the text\n",
    "    valid_terms_sorted = sorted(valid_terms, key=lambda x: x['start_idx'])\n",
    "    \n",
    "    # Print valid terms in order they appear in the text\n",
    "    print(f\"\\nVALID TERMS IN TEXT ORDER:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, term_info in enumerate(valid_terms_sorted[:20], 1):\n",
    "        # Format the term type for display\n",
    "        if term_info['pos'].startswith('dependency_'):\n",
    "            pattern_name = term_info['pos'].replace('dependency_', '').upper()\n",
    "            term_type = f\"DEPENDENCY {pattern_name} TERM\"\n",
    "        elif term_info['pos'].startswith('context_dependent_'):\n",
    "            ctx_type = term_info.get('context_type', 'unknown').upper()\n",
    "            base_type = term_info['pos'].replace('context_dependent_', '').upper().replace('_', ' ')\n",
    "            term_type = f\"CONTEXT-DEPENDENT {base_type} ({ctx_type})\"\n",
    "        else:\n",
    "            term_type = term_info['pos'].upper().replace('_', ' ') + \" TERM\"\n",
    "        \n",
    "        context = get_context_words(doc, term_info['start_idx'], term_info['end_idx'])\n",
    "        print(f\"{i}. {term_type}: {term_info['term']}\")\n",
    "        \n",
    "        # Add dependency info if it's a dependency pattern\n",
    "        if 'dependency' in term_info:\n",
    "            print(f\"   (Dependency: {term_info['dependency']})\")\n",
    "        \n",
    "        # Add context info if it's a context-dependent term\n",
    "        if 'context_word' in term_info:\n",
    "            print(f\"   (Context: '{term_info['context_word']}' -> Neutral: '{term_info['neutral_part']}')\")\n",
    "        \n",
    "        print(f\"   Context: {context}\")\n",
    "        print()\n",
    "    \n",
    "    # If there are negated terms, show some examples\n",
    "    if negated_terms:\n",
    "        print(f\"\\nEXAMPLES OF NEGATED TERMS (EXCLUDED FROM COUNT):\")\n",
    "        print(\"-\" * 40)\n",
    "        negated_terms_sorted = sorted(negated_terms, key=lambda x: x['start_idx'])\n",
    "        for i, term_info in enumerate(negated_terms_sorted[:10], 1):\n",
    "            if term_info['pos'].startswith('dependency_'):\n",
    "                pattern_name = term_info['pos'].replace('dependency_', '').upper()\n",
    "                term_type = f\"DEPENDENCY {pattern_name} TERM\"\n",
    "            elif term_info['pos'].startswith('context_dependent_'):\n",
    "                ctx_type = term_info.get('context_type', 'unknown').upper()\n",
    "                base_type = term_info['pos'].replace('context_dependent_', '').upper().replace('_', ' ')\n",
    "                term_type = f\"CONTEXT-DEPENDENT {base_type} ({ctx_type})\"\n",
    "            else:\n",
    "                term_type = term_info['pos'].upper().replace('_', ' ') + \" TERM\"\n",
    "            \n",
    "            context = get_context_words(doc, term_info['start_idx'], term_info['end_idx'])\n",
    "            print(f\"{i}. {term_type}: {term_info['term']}\")\n",
    "            scope_info = f\" in {term_info.get('negation_scope', 'unknown')} scope\" if 'negation_scope' in term_info else \"\"\n",
    "            print(f\"   Negated by: {term_info['negation_text']} (type: {term_info['negation_type']}{scope_info})\")\n",
    "            \n",
    "            # Add context info if it's a context-dependent term\n",
    "            if 'context_word' in term_info:\n",
    "                print(f\"   (Context: '{term_info['context_word']}' -> Neutral: '{term_info['neutral_part']}')\")\n",
    "            \n",
    "            print(f\"   Context: {context}\")\n",
    "            print()\n",
    "    \n",
    "    return valid_type_counts, valid_terms, negated_terms, original_type_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa79b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "for doc_name, doc in documents.items():\n",
    "    valid_counts, valid_terms, negated_terms, original_counts = analyze_green_terms_with_negation(doc, doc_name)\n",
    "    all_results[doc_name] = {\n",
    "        'valid_counts': valid_counts,\n",
    "        'valid_terms': valid_terms,\n",
    "        'negated_terms': negated_terms,\n",
    "        'original_counts': original_counts,\n",
    "        'total_tokens': len(doc),\n",
    "        'total_sentences': len(list(doc.sents))\n",
    "    }\n",
    "\n",
    "# Cell 10: Print Comprehensive Summary\n",
    "print(f\"\\n{'='*140}\")\n",
    "print(\"COMPREHENSIVE SUMMARY - GREEN TERMS ANALYSIS WITH CONTEXT-DEPENDENT TERMS + MULTI-SCOPE NEGATION\")\n",
    "print(f\"{'='*140}\")\n",
    "\n",
    "print(\"\\n1. ORIGINAL COUNTS (Before Negation Filtering)\")\n",
    "print(f\"{'Document':<25} {'Nouns':<8} {'M-Nouns':<8} {'Adj':<8} {'M-Adj':<8} {'Verbs':<8} {'Adv':<8} {'M-Adv':<8} {'Ctx-N':<8} {'Ctx-MN':<8} {'Dep-Pat':<8} {'Total':<8}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "for doc_name, results in all_results.items():\n",
    "    counts = results['original_counts']\n",
    "    dependency_total = sum(count for pos_type, count in counts.items() if pos_type.startswith('dependency_'))\n",
    "    total = sum(counts.values())\n",
    "    print(f\"{doc_name:<25} {counts['noun']:<8} {counts['multiword_noun']:<8} {counts['adjective']:<8} {counts['multiword_adjective']:<8} {counts['verb']:<8} {counts['adverb']:<8} {counts['multiword_adverb']:<8} {counts['context_dependent_noun']:<8} {counts['context_dependent_multiword_noun']:<8} {dependency_total:<8} {total:<8}\")\n",
    "\n",
    "print(\"\\n2. NEGATED TERMS (Filtered Out)\")\n",
    "print(f\"{'Document':<25} {'Nouns':<8} {'M-Nouns':<8} {'Adj':<8} {'M-Adj':<8} {'Verbs':<8} {'Adv':<8} {'M-Adv':<8} {'Ctx-N':<8} {'Ctx-MN':<8} {'Dep-Pat':<8} {'Total':<8}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "for doc_name, results in all_results.items():\n",
    "    negated_counts = Counter()\n",
    "    for term in results['negated_terms']:\n",
    "        negated_counts[term['pos']] += 1\n",
    "    \n",
    "    dependency_total = sum(count for pos_type, count in negated_counts.items() if pos_type.startswith('dependency_'))\n",
    "    total = sum(negated_counts.values())\n",
    "    print(f\"{doc_name:<25} {negated_counts['noun']:<8} {negated_counts['multiword_noun']:<8} {negated_counts['adjective']:<8} {negated_counts['multiword_adjective']:<8} {negated_counts['verb']:<8} {negated_counts['adverb']:<8} {negated_counts['multiword_adverb']:<8} {negated_counts['context_dependent_noun']:<8} {negated_counts['context_dependent_multiword_noun']:<8} {dependency_total:<8} {total:<8}\")\n",
    "\n",
    "print(\"\\n3. FINAL VALID COUNTS (After Multi-Scope + Reduction Verb Negation Filtering)\")\n",
    "print(f\"{'Document':<25} {'Nouns':<8} {'M-Nouns':<8} {'Adj':<8} {'M-Adj':<8} {'Verbs':<8} {'Adv':<8} {'M-Adv':<8} {'Ctx-N':<8} {'Ctx-MN':<8} {'Dep-Pat':<8} {'Total':<8}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "for doc_name, results in all_results.items():\n",
    "    counts = results['valid_counts']\n",
    "    dependency_total = sum(count for pos_type, count in counts.items() if pos_type.startswith('dependency_'))\n",
    "    total = sum(counts.values())\n",
    "    print(f\"{doc_name:<25} {counts['noun']:<8} {counts['multiword_noun']:<8} {counts['adjective']:<8} {counts['multiword_adjective']:<8} {counts['verb']:<8} {counts['adverb']:<8} {counts['multiword_adverb']:<8} {counts['context_dependent_noun']:<8} {counts['context_dependent_multiword_noun']:<8} {dependency_total:<8} {total:<8}\")\n",
    "\n",
    "print(\"\\n4. CONTEXT-DEPENDENT TERMS ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Analyze context-dependent terms by type\n",
    "all_context_types = Counter()\n",
    "all_context_words = Counter()\n",
    "context_examples = []\n",
    "\n",
    "for doc_name, results in all_results.items():\n",
    "    doc_context_negative = 0\n",
    "    doc_context_improvement = 0\n",
    "    \n",
    "    for term in results['valid_terms']:\n",
    "        if term['pos'].startswith('context_dependent_'):\n",
    "            context_type = term.get('context_type', 'unknown')\n",
    "            context_word = term.get('context_word', 'unknown')\n",
    "            all_context_types[context_type] += 1\n",
    "            all_context_words[context_word.lower()] += 1\n",
    "            \n",
    "            if context_type == 'negative':\n",
    "                doc_context_negative += 1\n",
    "            elif context_type == 'improvement':\n",
    "                doc_context_improvement += 1\n",
    "            \n",
    "            if len(context_examples) < 10:\n",
    "                context_examples.append((doc_name, term['term'], term['context_word'], term['neutral_part'], context_type))\n",
    "    \n",
    "    total_context = doc_context_negative + doc_context_improvement\n",
    "    if total_context > 0:\n",
    "        print(f\"{doc_name}: {total_context} context-dependent terms ({doc_context_negative} negative, {doc_context_improvement} improvement)\")\n",
    "\n",
    "print(f\"\\nOverall context type distribution: {dict(all_context_types)}\")\n",
    "print(f\"Top context words: {dict(all_context_words.most_common(10))}\")\n",
    "\n",
    "print(f\"\\nExamples of context-dependent terms:\")\n",
    "for doc, term, context, neutral, ctx_type in context_examples:\n",
    "    print(f\"  - '{term}' = '{context}' + '{neutral}' ({ctx_type} context) [{doc}]\")\n",
    "\n",
    "print(\"\\n5. NEGATION IMPACT SUMMARY\")\n",
    "print(f\"{'Document':<25} {'Original':<10} {'Negated':<10} {'Valid':<10} {'Negation %':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for doc_name, results in all_results.items():\n",
    "    original_total = sum(results['original_counts'].values())\n",
    "    negated_total = len(results['negated_terms'])\n",
    "    valid_total = sum(results['valid_counts'].values())\n",
    "    negation_pct = (negated_total / original_total * 100) if original_total > 0 else 0\n",
    "    \n",
    "    print(f\"{doc_name:<25} {original_total:<10} {negated_total:<10} {valid_total:<10} {negation_pct:<11.1f}%\")\n",
    "\n",
    "print(f\"\\n{'='*140}\")\n",
    "print(\"ANALYSIS COMPLETE - Enhanced with Context-Dependent Terms + Multi-Scope Negation Detection\")\n",
    "print(\"Key features:\")\n",
    "print(\"1. Context-dependent terms: neutral terms that become green with negative/improvement context\")\n",
    "print(\"2. Multi-scope negation detection: subtree, ancestors, head.subtree, and sentence-level\")\n",
    "print(\"3. Reduction verb negation: detects when positive verbs are negated\")\n",
    "print(\"4. Comprehensive term classification and overlap prevention\")\n",
    "print(f\"{'='*140}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e29e2d",
   "metadata": {},
   "source": [
    "# Context Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16144f8f",
   "metadata": {},
   "source": [
    "## Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6972ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def get_temporal_markers_for_year(report_year):\n",
    "    \"\"\"\n",
    "    Get temporal markers adjusted for the specific report year.\n",
    "    \"\"\"\n",
    "    if report_year == 2021:\n",
    "        return {\n",
    "            'past': {\n",
    "                'year_patterns': [r'\\b(in|during|since|until|before)\\s+(19\\d{2}|200\\d|201\\d|2020)\\b', r'\\b(19\\d{2}|200\\d|201\\d|2020)\\b'],\n",
    "                'relative_patterns': ['last year', 'previous year', 'previously', 'before', 'ago', 'earlier', 'historic', 'past', 'prior', \n",
    "                                      'former', 'preceding', 'until now', 'so far', 'to date', 'historically', 'last quarter', 'previous quarter', \n",
    "                                      'last month', 'previous month', 'last semester', 'previous semester', 'last period', 'previous period'],\n",
    "                'specific_patterns': ['since 2020', 'since 2019', 'since 2018', 'since 2017', 'since 2016', 'since 2015', 'until 2020', \n",
    "                                      'before 2021', 'up to 2020', 'through 2020', 'by 2020', 'Q1 2020', 'Q2 2020', 'Q3 2020', 'Q4 2020', \n",
    "                                      'first quarter 2020', 'second quarter 2020', 'third quarter 2020', 'fourth quarter 2020', 'H1 2020', \n",
    "                                      'H2 2020', 'first half 2020', 'second half 2020']\n",
    "            },\n",
    "            'present': {\n",
    "                'year_patterns': [r'\\b(in|during|this|current)\\s+2021\\b', r'\\b2021\\b'],\n",
    "                'relative_patterns': ['currently', 'now', 'today', 'this year', 'present', 'ongoing', 'continue', 'continues', 'at present', \n",
    "                                      'as of now', 'current', 'existing', 'active', 'in progress', 'underway', 'throughout this year', \n",
    "                                      'during this period', 'as we speak', 'at this time', 'at this point', 'right now', 'presently'],\n",
    "                'specific_patterns': ['as of now', 'at present', 'to date', 'in 2021', 'during 2021', 'this year', 'current year', 'Q1 2021', \n",
    "                                      'Q2 2021', 'Q3 2021', 'Q4 2021', 'this quarter', 'current quarter', 'H1 2021', 'H2 2021', 'first half 2021', \n",
    "                                      'second half 2021', 'as of 2021', 'as of end-2021', 'year-to-date', 'YTD 2021', 'as of December 2021', \n",
    "                                      'end of 2021']\n",
    "            },\n",
    "            'future': {\n",
    "                'year_patterns': [r'\\b(by|until|before|from|after)\\s+(202[2-9]|20[3-9]\\d)\\b', r'\\b(202[2-9]|20[3-9]\\d)\\b'],\n",
    "                'relative_patterns': ['next year', 'future', 'upcoming', 'planned', 'will', 'shall', 'intend', 'target', 'aim', 'expect', \n",
    "                                      'anticipate', 'forecast', 'project', 'outlook', 'forward', 'ahead', 'coming', 'prospective'],\n",
    "                'specific_patterns': ['by 2030', 'by 2025', 'by 2026', 'by 2027', 'by 2028', 'by 2029', 'in the future', 'from 2022', \n",
    "                                      'after 2021', 'beyond 2021', 'going forward', 'by 2024', 'by 2023', 'by 2022', 'by 2035', 'by 2040', \n",
    "                                      'by 2050', 'from 2022 onwards', 'starting 2022', 'beginning 2022', 'Q1 2022', 'Q2 2022', 'Q3 2022', \n",
    "                                      'Q4 2022', 'next quarter', 'H1 2022', 'H2 2022', 'first half 2022', 'second half 2022',]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    elif report_year == 2022:\n",
    "        return {\n",
    "            'past': {\n",
    "                'year_patterns': [r'\\b(in|during|since|until|before)\\s+(19\\d{2}|200\\d|201\\d|2020|2021)\\b', r'\\b(19\\d{2}|200\\d|201\\d|2020|2021)\\b'],\n",
    "                'relative_patterns': ['last year', 'previous year', 'previously', 'before', 'ago', 'earlier', 'historic', 'past', 'prior', \n",
    "                                      'former', 'preceding', 'until now', 'so far', 'to date', 'historically', 'last quarter', 'previous quarter', \n",
    "                                      'last month', 'previous month', 'last semester', 'previous semester', 'last period', 'previous period',],\n",
    "                'specific_patterns': ['since 2020', 'since 2019', 'since 2018', 'since 2017', 'since 2016', 'since 2015', 'until 2020', \n",
    "                                      'before 2021', 'up to 2020', 'through 2020', 'by 2020', 'Q1 2020', 'Q2 2020', 'Q3 2020', 'Q4 2020', \n",
    "                                      'first quarter 2020', 'second quarter 2020', 'third quarter 2020', 'fourth quarter 2020', 'H1 2020', \n",
    "                                      'H2 2020', 'first half 2020', 'second half 2020',]\n",
    "            },\n",
    "            'present': {\n",
    "                'year_patterns': [r'\\b(in|during|this|current)\\s+2022\\b', r'\\b2022\\b'],\n",
    "                'relative_patterns': ['currently', 'now', 'today', 'this year', 'present', 'ongoing', 'continue', 'continues', 'at present', \n",
    "                                      'as of now', 'current', 'existing', 'active', 'in progress', 'underway', 'throughout this year', \n",
    "                                      'during this period', 'as we speak', 'at this time', 'at this point', 'right now', 'presently'],\n",
    "                'specific_patterns': ['as of now', 'at present', 'to date', 'in 2021', 'during 2021', 'this year', 'current year', 'Q1 2021', \n",
    "                                      'Q2 2021', 'Q3 2021', 'Q4 2021', 'this quarter', 'current quarter', 'H1 2021', 'H2 2021', 'first half 2021', \n",
    "                                      'second half 2021', 'as of 2021', 'as of end-2021', 'year-to-date', 'YTD 2021', 'as of December 2021', \n",
    "                                      'end of 2021']\n",
    "            },\n",
    "            'future': {\n",
    "                'year_patterns': [r'\\b(by|until|before|from|after)\\s+(202[3-9]|20[3-9]\\d)\\b', r'\\b(202[3-9]|20[3-9]\\d)\\b'],\n",
    "                'relative_patterns': ['next year', 'future', 'upcoming', 'planned', 'will', 'shall', 'intend', 'target', 'aim', 'expect', \n",
    "                                      'anticipate', 'forecast', 'project', 'outlook', 'forward', 'ahead', 'coming', 'prospective'],\n",
    "                'specific_patterns': ['by 2030', 'by 2025', 'by 2026', 'by 2027', 'by 2028', 'by 2029', 'in the future', 'from 2022', \n",
    "                                      'after 2021', 'beyond 2021', 'going forward', 'by 2024', 'by 2023', 'by 2022', 'by 2035', 'by 2040', \n",
    "                                      'by 2050', 'from 2022 onwards', 'starting 2022', 'beginning 2022', 'Q1 2022', 'Q2 2022', 'Q3 2022', \n",
    "                                      'Q4 2022', 'next quarter', 'H1 2022', 'H2 2022', 'first half 2022', 'second half 2022',]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        return{\n",
    "            print(f\"Warning: No specific temporal markers defined for report year {report_year}. Using default patterns.\")\n",
    "        }\n",
    "\n",
    "# Enhanced auxiliary verb patterns for tense detection\n",
    "AUXILIARY_PATTERNS = {\n",
    "    'future': [\n",
    "        'will', 'shall', 'going to', 'plan to', 'intend to', 'aim to', 'expect to',\n",
    "        'hope to', 'seek to', 'strive to', 'endeavor to', 'commit to', 'pledge to',\n",
    "        'target to', 'set to', 'scheduled to', 'due to', 'about to', 'prepare to',\n",
    "        'ready to', 'poised to', 'bound to', 'likely to', 'expected to', 'projected to',\n",
    "        'forecast to', 'anticipated to', 'destined to', 'planning to', 'intending to', \n",
    "        'would', 'could potentially', 'might eventually', 'may ultimately'\n",
    "    ],\n",
    "    'present_perfect': [\n",
    "        'have', 'has', 'have been', 'has been', 'have done', 'has done',\n",
    "        'have achieved', 'has achieved', 'have implemented', 'has implemented',\n",
    "        'have established', 'has established', 'have developed', 'has developed'\n",
    "    ],\n",
    "    'past_perfect': [\n",
    "        'had', 'had been', 'had done', 'had achieved', 'had implemented',\n",
    "        'had established', 'had developed', 'had completed', 'had finished'\n",
    "    ],\n",
    "    'present_continuous': [\n",
    "        'am', 'is', 'are', 'am being', 'is being', 'are being',\n",
    "        'am doing', 'is doing', 'are doing', 'am working', 'is working', 'are working'\n",
    "    ],\n",
    "    'past_continuous': [\n",
    "        'was', 'were', 'was being', 'were being', 'was doing', 'were doing',\n",
    "        'was working', 'were working', 'was implementing', 'were implementing'\n",
    "    ],\n",
    "    'conditional': [\n",
    "        'would', 'could', 'should', 'might', 'may', 'must',\n",
    "        'would be', 'could be', 'should be', 'might be', 'may be', 'must be',\n",
    "        'would need to', 'could potentially', 'should ideally', 'might require',\n",
    "        'may involve', 'must ensure', 'would enable', 'could facilitate'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def determine_report_year(doc_name):\n",
    "    \"\"\"\n",
    "    Determine the report year from the document name.\n",
    "    \"\"\"\n",
    "    if '2021' in doc_name:\n",
    "        return 2021\n",
    "    elif '2022' in doc_name:\n",
    "        return 2022\n",
    "    else:\n",
    "        # Try to extract year from document name\n",
    "        year_match = re.search(r'(20\\d{2})', doc_name)\n",
    "        if year_match:\n",
    "            return int(year_match.group(1))\n",
    "        else:\n",
    "            return 2021  # Default fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a366c774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_governing_verb(green_term, doc):\n",
    "    \"\"\"\n",
    "    Find the governing verb for a green term using dependency parsing.\n",
    "    \n",
    "    Args:\n",
    "        green_term: Dictionary containing green term info with start_idx, end_idx, pos\n",
    "        doc: spaCy doc object\n",
    "    \n",
    "    Returns:\n",
    "        spaCy token object of governing verb or None\n",
    "    \"\"\"\n",
    "    start_idx = green_term['start_idx']\n",
    "    end_idx = green_term['end_idx']\n",
    "    pos_type = green_term['pos']\n",
    "    \n",
    "    # If the green term itself is a verb, return it\n",
    "    if 'verb' in pos_type.lower():\n",
    "        return doc[start_idx] if start_idx == end_idx else doc[start_idx]\n",
    "    \n",
    "    # For non-verb terms, find the governing verb through dependency parsing\n",
    "    main_token = doc[start_idx]  # Use the first token as the main token\n",
    "    \n",
    "    # Strategy 1: Look for the head verb directly\n",
    "    current_token = main_token\n",
    "    max_depth = 10  # Prevent infinite loops\n",
    "    depth = 0\n",
    "    \n",
    "    while current_token.head != current_token and depth < max_depth:\n",
    "        if current_token.head.pos_ == 'VERB' or current_token.head.pos_ == 'AUX':\n",
    "            return current_token.head\n",
    "        current_token = current_token.head\n",
    "        depth += 1\n",
    "    \n",
    "    # Strategy 2: Look for verbs in the same sentence that govern this term\n",
    "    sentence = main_token.sent\n",
    "    for token in sentence:\n",
    "        if token.pos_ == 'VERB' or token.pos_ == 'AUX':\n",
    "            # Check if this verb governs our green term through dependency relations\n",
    "            for child in token.subtree:\n",
    "                if start_idx <= child.i <= end_idx:\n",
    "                    return token\n",
    "    \n",
    "    # Strategy 3: Find the root verb of the sentence\n",
    "    sentence_root = main_token.sent.root\n",
    "    if sentence_root.pos_ == 'VERB' or sentence_root.pos_ == 'AUX':\n",
    "        return sentence_root\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_verb_tense(governing_verb):\n",
    "    \"\"\"\n",
    "    Extract tense information from a governing verb using enhanced patterns.\n",
    "    \n",
    "    Args:\n",
    "        governing_verb: spaCy token object of the verb\n",
    "    \n",
    "    Returns:\n",
    "        String: 'past', 'present', 'future', or 'unclear'\n",
    "    \"\"\"\n",
    "    if governing_verb is None:\n",
    "        return 'unclear'\n",
    "    \n",
    "    # Primary method: Use spaCy's morphological analysis\n",
    "    tense_info = governing_verb.morph.get(\"Tense\")\n",
    "    if tense_info:\n",
    "        tense_value = tense_info[0].lower() if isinstance(tense_info, list) else tense_info.lower()\n",
    "        if 'past' in tense_value:\n",
    "            return 'past'\n",
    "        elif 'pres' in tense_value:\n",
    "            return 'present'\n",
    "        elif 'fut' in tense_value:\n",
    "            return 'future'\n",
    "    \n",
    "    # Secondary method: Enhanced pattern matching for auxiliary verbs\n",
    "    verb_lemma = governing_verb.lemma_.lower()\n",
    "    verb_text = governing_verb.text.lower()\n",
    "    \n",
    "    # Check sentence context for auxiliary patterns\n",
    "    sentence = governing_verb.sent\n",
    "    sentence_text = sentence.text.lower()\n",
    "    \n",
    "    # Check for auxiliary patterns with better context awareness\n",
    "    for pattern_type, patterns in AUXILIARY_PATTERNS.items():\n",
    "        for pattern in patterns:\n",
    "            if pattern in sentence_text:\n",
    "                # Verify proximity to our verb (within 5 tokens)\n",
    "                pattern_tokens = pattern.split()\n",
    "                for i, sent_token in enumerate(sentence):\n",
    "                    if sent_token.text.lower() == pattern_tokens[0]:\n",
    "                        # Check if this auxiliary is close to our governing verb\n",
    "                        if abs(sent_token.i - governing_verb.i) <= 5:\n",
    "                            if pattern_type == 'future':\n",
    "                                return 'future'\n",
    "                            elif pattern_type in ['present_perfect', 'present_continuous']:\n",
    "                                return 'present'\n",
    "                            elif pattern_type in ['past_perfect', 'past_continuous']:\n",
    "                                return 'past'\n",
    "                            elif pattern_type == 'conditional':\n",
    "                                return 'future'  # Conditional often implies future intent\n",
    "    \n",
    "    # Enhanced tense detection based on verb form and context\n",
    "    if governing_verb.tag_ in ['VBD', 'VBN']:  # Past tense, past participle\n",
    "        return 'past'\n",
    "    elif governing_verb.tag_ in ['VBZ', 'VBP', 'VBG']:  # Present tense forms\n",
    "        return 'present'\n",
    "    elif governing_verb.tag_ == 'MD':  # Modal verb\n",
    "        return 'future'\n",
    "    elif governing_verb.tag_ == 'VB':  # Base form - often future with auxiliary\n",
    "        return 'future'\n",
    "    \n",
    "    return 'unclear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c62faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_temporal_markers(green_term, governing_verb, doc, report_year):\n",
    "    \"\"\"\n",
    "    Find relevant temporal markers near the green term or governing verb.\n",
    "    \n",
    "    Args:\n",
    "        green_term: Dictionary containing green term info\n",
    "        governing_verb: spaCy token object of governing verb\n",
    "        doc: spaCy doc object\n",
    "        report_year: Year of the report (2021 or 2022)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: (temporal_category, marker_text, proximity_score)\n",
    "    \"\"\"\n",
    "    green_start = green_term['start_idx']\n",
    "    green_end = green_term['end_idx']\n",
    "    verb_pos = governing_verb.i if governing_verb else green_start\n",
    "    \n",
    "    sentence = doc[green_start].sent\n",
    "    sentence_text = sentence.text.lower()\n",
    "    \n",
    "    # Get temporal markers for the specific report year\n",
    "    TEMPORAL_MARKERS = get_temporal_markers_for_year(report_year)\n",
    "    \n",
    "    # Search within 5 tokens of green term or governing verb\n",
    "    search_positions = [green_start, green_end]\n",
    "    if governing_verb:\n",
    "        search_positions.append(verb_pos)\n",
    "    \n",
    "    best_match = None\n",
    "    best_proximity = float('inf')\n",
    "    best_category = None\n",
    "    \n",
    "    # Check each temporal category\n",
    "    for category, patterns in TEMPORAL_MARKERS.items():\n",
    "        # Check year patterns\n",
    "        for pattern in patterns['year_patterns']:\n",
    "            matches = re.finditer(pattern, sentence_text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                # Find token position of match\n",
    "                match_start_char = match.start()\n",
    "                match_token_pos = None\n",
    "                for token in sentence:\n",
    "                    if token.idx <= sentence.start_char + match_start_char < token.idx + len(token.text):\n",
    "                        match_token_pos = token.i\n",
    "                        break\n",
    "                \n",
    "                if match_token_pos:\n",
    "                    # Calculate proximity to green term or governing verb\n",
    "                    proximity = min(abs(match_token_pos - pos) for pos in search_positions)\n",
    "                    if proximity <= 5 and proximity < best_proximity:\n",
    "                        best_match = match.group()\n",
    "                        best_proximity = proximity\n",
    "                        best_category = category\n",
    "        \n",
    "        # Check relative patterns\n",
    "        for pattern in patterns['relative_patterns']:\n",
    "            if pattern in sentence_text:\n",
    "                # Find token position\n",
    "                pattern_start = sentence_text.find(pattern)\n",
    "                match_token_pos = None\n",
    "                for token in sentence:\n",
    "                    token_start_in_sent = token.idx - sentence.start_char\n",
    "                    if token_start_in_sent <= pattern_start < token_start_in_sent + len(token.text):\n",
    "                        match_token_pos = token.i\n",
    "                        break\n",
    "                \n",
    "                if match_token_pos:\n",
    "                    proximity = min(abs(match_token_pos - pos) for pos in search_positions)\n",
    "                    if proximity <= 5 and proximity < best_proximity:\n",
    "                        best_match = pattern\n",
    "                        best_proximity = proximity\n",
    "                        best_category = category\n",
    "        \n",
    "        # Check specific patterns\n",
    "        for pattern in patterns['specific_patterns']:\n",
    "            if pattern in sentence_text:\n",
    "                pattern_start = sentence_text.find(pattern)\n",
    "                match_token_pos = None\n",
    "                for token in sentence:\n",
    "                    token_start_in_sent = token.idx - sentence.start_char\n",
    "                    if token_start_in_sent <= pattern_start < token_start_in_sent + len(token.text):\n",
    "                        match_token_pos = token.i\n",
    "                        break\n",
    "                \n",
    "                if match_token_pos:\n",
    "                    proximity = min(abs(match_token_pos - pos) for pos in search_positions)\n",
    "                    if proximity <= 5 and proximity < best_proximity:\n",
    "                        best_match = pattern\n",
    "                        best_proximity = proximity\n",
    "                        best_category = category\n",
    "    \n",
    "    return best_category, best_match, best_proximity if best_match else None\n",
    "\n",
    "def classify_temporal_context(green_term, doc, report_year):\n",
    "    \"\"\"\n",
    "    Classify the temporal context of a green term.\n",
    "    \n",
    "    Args:\n",
    "        green_term: Dictionary containing green term info\n",
    "        doc: spaCy doc object\n",
    "        report_year: Year of the report (2021 or 2022)\n",
    "    \n",
    "    Returns:\n",
    "        String: 'past', 'present', 'future', or 'unclear'\n",
    "    \"\"\"\n",
    "    # Step 1: Find governing verb\n",
    "    governing_verb = find_governing_verb(green_term, doc)\n",
    "    \n",
    "    # Step 2: Extract verb tense\n",
    "    verb_tense = extract_verb_tense(governing_verb)\n",
    "    \n",
    "    # Step 3: Find temporal markers\n",
    "    temporal_category, marker_text, proximity = find_temporal_markers(green_term, governing_verb, doc, report_year)\n",
    "    \n",
    "    # Step 4: Combine evidence with precedence rules\n",
    "    # Temporal markers override verb tense when they are syntactically connected\n",
    "    if temporal_category and proximity is not None and proximity <= 3:\n",
    "        # Close temporal markers take precedence\n",
    "        return temporal_category\n",
    "    elif temporal_category and proximity is not None and proximity <= 5:\n",
    "        # Moderate distance - consider both evidence\n",
    "        if verb_tense != 'unclear' and verb_tense != temporal_category:\n",
    "            # Conflict resolution: prefer more specific evidence\n",
    "            return temporal_category  # Temporal markers are usually more specific\n",
    "        else:\n",
    "            return temporal_category\n",
    "    elif verb_tense != 'unclear':\n",
    "        # No close temporal markers, use verb tense\n",
    "        return verb_tense\n",
    "    else:\n",
    "        # No clear evidence\n",
    "        return 'unclear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d8824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_temporal_classification(all_results, documents):\n",
    "    \"\"\"\n",
    "    Add temporal classification to all valid green terms across all documents.\n",
    "    \n",
    "    Args:\n",
    "        all_results: Dictionary containing results from green term analysis\n",
    "        documents: Dictionary containing spaCy doc objects\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary: Updated results with temporal classification added\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TEMPORAL CLASSIFICATION ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Initialize summary statistics\n",
    "    overall_temporal_stats = Counter()\n",
    "    doc_temporal_stats = {}\n",
    "    \n",
    "    for doc_name, results in all_results.items():\n",
    "        print(f\"\\nProcessing temporal classification for: {doc_name}\")\n",
    "        doc = documents[doc_name]\n",
    "        valid_terms = results['valid_terms']\n",
    "        \n",
    "        # Determine report year for this document\n",
    "        report_year = determine_report_year(doc_name)\n",
    "        print(f\"  Report year determined: {report_year}\")\n",
    "        \n",
    "        # Classify each valid term\n",
    "        temporal_classifications = []\n",
    "        doc_temporal_counter = Counter()\n",
    "        \n",
    "        for i, green_term in enumerate(valid_terms):\n",
    "            temporal_class = classify_temporal_context(green_term, doc, report_year)\n",
    "            \n",
    "            # Add temporal classification to the green term object\n",
    "            green_term['temporal_class'] = temporal_class\n",
    "            green_term['report_year'] = report_year\n",
    "            \n",
    "            # Update statistics\n",
    "            temporal_classifications.append(temporal_class)\n",
    "            doc_temporal_counter[temporal_class] += 1\n",
    "            overall_temporal_stats[temporal_class] += 1\n",
    "        \n",
    "        # Store document-level statistics\n",
    "        doc_temporal_stats[doc_name] = doc_temporal_counter\n",
    "        \n",
    "        # Print document summary\n",
    "        total_terms = len(valid_terms)\n",
    "        print(f\"  Temporal classification completed for {total_terms} terms:\")\n",
    "        for category, count in doc_temporal_counter.items():\n",
    "            percentage = (count / total_terms) * 100 if total_terms > 0 else 0\n",
    "            print(f\"    {category.capitalize()}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Print overall summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TEMPORAL CLASSIFICATION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    total_terms = sum(overall_temporal_stats.values())\n",
    "    print(f\"Total terms classified: {total_terms}\")\n",
    "    print(f\"\\nOverall distribution:\")\n",
    "    for category in ['past', 'present', 'future', 'unclear']:\n",
    "        count = overall_temporal_stats[category]\n",
    "        percentage = (count / total_terms) * 100 if total_terms > 0 else 0\n",
    "        print(f\"  {category.capitalize()}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Document-level breakdown\n",
    "    print(f\"\\nDocument-level breakdown:\")\n",
    "    print(f\"{'Document':<25} {'Past':<8} {'Present':<8} {'Future':<8} {'Unclear':<8} {'Total':<8}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for doc_name, doc_stats in doc_temporal_stats.items():\n",
    "        past = doc_stats['past']\n",
    "        present = doc_stats['present']\n",
    "        future = doc_stats['future']\n",
    "        unclear = doc_stats['unclear']\n",
    "        total = sum(doc_stats.values())\n",
    "        \n",
    "        print(f\"{doc_name:<25} {past:<8} {present:<8} {future:<8} {unclear:<8} {total:<8}\")\n",
    "    \n",
    "    # Store temporal statistics in results\n",
    "    for doc_name in all_results:\n",
    "        all_results[doc_name]['temporal_stats'] = doc_temporal_stats[doc_name]\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca9a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLY TEMPORAL CLASSIFICATION TO EXISTING RESULTS\n",
    "\n",
    "print(\"Applying temporal classification to all valid green terms...\")\n",
    "\n",
    "# Apply temporal classification to all documents\n",
    "all_results = add_temporal_classification(all_results, documents)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TEMPORAL CLASSIFICATION INTEGRATION COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"All valid green terms now include 'temporal_class' field\")\n",
    "print(\"Report year automatically detected for each document\")\n",
    "print(\"Temporal statistics added to results\")\n",
    "print(\"Ready for further analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b951dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_temporal_patterns(all_results):\n",
    "    \"\"\"\n",
    "    Analyze patterns in temporal classification results.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TEMPORAL PATTERN ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Analyze by POS type\n",
    "    pos_temporal_stats = defaultdict(Counter)\n",
    "    \n",
    "    for doc_name, results in all_results.items():\n",
    "        for term in results['valid_terms']:\n",
    "            if 'temporal_class' in term:\n",
    "                pos_type = term['pos']\n",
    "                temporal_class = term['temporal_class']\n",
    "                pos_temporal_stats[pos_type][temporal_class] += 1\n",
    "    \n",
    "    print(\"\\nTemporal classification by POS type:\")\n",
    "    print(f\"{'POS Type':<30} {'Past':<8} {'Present':<8} {'Future':<8} {'Unclear':<8}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for pos_type, temporal_counter in pos_temporal_stats.items():\n",
    "        past = temporal_counter['past']\n",
    "        present = temporal_counter['present'] \n",
    "        future = temporal_counter['future']\n",
    "        unclear = temporal_counter['unclear']\n",
    "        print(f\"{pos_type:<30} {past:<8} {present:<8} {future:<8} {unclear:<8}\")\n",
    "    \n",
    "    return pos_temporal_stats\n",
    "\n",
    "def print_temporal_examples(all_results, documents, max_examples=5):\n",
    "    \"\"\"\n",
    "    Print examples of temporal classifications for verification.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TEMPORAL CLASSIFICATION EXAMPLES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Collect examples by temporal class\n",
    "    examples_by_class = {'past': [], 'present': [], 'future': [], 'unclear': []}\n",
    "    \n",
    "    for doc_name, results in all_results.items():\n",
    "        doc = documents[doc_name]\n",
    "        for term in results['valid_terms']:\n",
    "            if 'temporal_class' in term:\n",
    "                temporal_class = term['temporal_class']\n",
    "                if len(examples_by_class[temporal_class]) < max_examples:\n",
    "                    # Get sentence context\n",
    "                    sentence = term['sentence']\n",
    "                    sentence_text = sentence.text.strip()\n",
    "                    \n",
    "                    # Highlight the green term in the sentence\n",
    "                    start_char = doc[term['start_idx']].idx\n",
    "                    end_char = doc[term['end_idx']].idx + len(doc[term['end_idx']].text)\n",
    "                    sentence_start_char = sentence.start_char\n",
    "                    \n",
    "                    relative_start = start_char - sentence_start_char\n",
    "                    relative_end = end_char - sentence_start_char\n",
    "                    \n",
    "                    highlighted_sentence = (\n",
    "                        sentence_text[:relative_start] + \n",
    "                        f\"**{sentence_text[relative_start:relative_end]}**\" + \n",
    "                        sentence_text[relative_end:]\n",
    "                    )\n",
    "                    \n",
    "                    examples_by_class[temporal_class].append({\n",
    "                        'term': term['term'],\n",
    "                        'sentence': highlighted_sentence,\n",
    "                        'doc': doc_name,\n",
    "                        'report_year': term.get('report_year', 'Unknown')\n",
    "                    })\n",
    "    \n",
    "    # Print examples\n",
    "    for temporal_class, examples in examples_by_class.items():\n",
    "        print(f\"\\n{temporal_class.upper()} EXAMPLES:\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, example in enumerate(examples[:max_examples], 1):\n",
    "            print(f\"{i}. Term: '{example['term']}'\")\n",
    "            print(f\"   Document: {example['doc']} (Report Year: {example['report_year']})\")\n",
    "            print(f\"   Context: {example['sentence']}\")\n",
    "            print()\n",
    "\n",
    "# Run the analysis\n",
    "print(\"Analyzing temporal patterns...\")\n",
    "pos_temporal_patterns = analyze_temporal_patterns(all_results)\n",
    "\n",
    "print(\"\\nDisplaying temporal classification examples...\")\n",
    "print_temporal_examples(all_results, documents, max_examples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TEMPORAL CLASSIFICATION VERIFICATION\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Check a few specific examples to verify classification accuracy\n",
    "for doc_name, results in all_results.items():\n",
    "    print(f\"\\nDocument: {doc_name}\")\n",
    "    report_year = determine_report_year(doc_name)\n",
    "    print(f\"Report Year: {report_year}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Show first 10 terms with their temporal classification\n",
    "    for i, term in enumerate(results['valid_terms'][:10]):\n",
    "        if 'temporal_class' in term:\n",
    "            sentence_text = term['sentence'].text.strip()\n",
    "            # Truncate long sentences for display\n",
    "            if len(sentence_text) > 100:\n",
    "                sentence_text = sentence_text[:100] + \"...\"\n",
    "            \n",
    "            print(f\"{i+1:2d}. '{term['term']}' -> {term['temporal_class'].upper()}\")\n",
    "            print(f\"    Context: {sentence_text}\")\n",
    "    \n",
    "    if len(results['valid_terms']) > 10:\n",
    "        print(f\"    ... and {len(results['valid_terms']) - 10} more terms\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TEMPORAL CLASSIFICATION COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"All valid green terms now have temporal classification based on report year.\")\n",
    "print(\"2021 Reports: 2021=present, ≤2020=past, ≥2022=future\")\n",
    "print(\"2022 Reports: 2022=present, ≤2021=past, ≥2023=future\")\n",
    "print(\"Use 'temporal_class' field to filter terms by temporal context.\")\n",
    "print(\"Example: past_terms = [t for t in valid_terms if t['temporal_class'] == 'past']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e761a97b",
   "metadata": {},
   "source": [
    "## Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3630a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Combined currency pattern for all currencies and amounts\n",
    "CURRENCY_SYMBOLS = ['$', '€', '£', '¥', '₹', 'USD', 'EUR', 'GBP', 'NOK', 'CZK', 'PLN', 'CHF', 'DKK', 'SEK', \n",
    "                   'dollars', 'dollar', 'euros', 'euro', 'pounds', 'pound', 'kroner', 'krone', 'kr',\n",
    "                   'korun', 'koruna', 'Kč', 'złoty', 'złotych', 'zł', 'francs', 'franc', 'Fr']\n",
    "\n",
    "CURRENCY_UNITS = ['million', 'M', 'billion', 'B', 'thousand', 'K', 'trillion', 'T']\n",
    "\n",
    "# Percentage patterns\n",
    "PERCENTAGE_PATTERNS = ['%', 'percent', 'per cent', 'percentage', 'pct', 'pc']\n",
    "\n",
    "# Energy and environmental units\n",
    "UNIT_PATTERNS = {\n",
    "    'energy': ['MW', 'GW', 'TW', 'kW', 'MWh', 'GWh', 'TWh', 'kWh', 'Wh'],\n",
    "    'emissions': ['tons', 'tonnes', 'tCO2', 'tCO2e', 'CO2', 'CO2e', 'kt', 'Mt', 'Gt'],\n",
    "    'volume': ['m3', 'm³', 'cubic meters', 'litres', 'liters', 'gallons'],\n",
    "    'weight': ['kg', 'tons', 'tonnes', 'pounds', 'lbs'],\n",
    "    'area': ['hectares', 'km2', 'km²', 'square meters', 'm2', 'm²'],\n",
    "    'distance': ['km', 'kilometers', 'miles', 'meters', 'm'],\n",
    "    'time': ['years', 'months', 'days', 'hours', 'hrs']\n",
    "}\n",
    "\n",
    "# Organizational terms to exclude with decimal numbers\n",
    "ORGANIZATIONAL_TERMS = [\n",
    "    'policy', 'management', 'governance', 'analysis', 'methodology', \n",
    "    'approach', 'framework', 'overview', 'introduction', 'background',\n",
    "    'scope', 'objectives', 'strategy', 'implementation', 'monitoring',\n",
    "    'reporting', 'compliance', 'assessment', 'evaluation', 'review',\n",
    "    'taxonomy', 'participations', 'stakeholder', 'engagement'\n",
    "    ]\n",
    "\n",
    "# Objects/patterns to EXCLUDE from meaningful quantifications\n",
    "EXCLUDED_OBJECTS = [\n",
    "    # Document structure\n",
    "    'chapter', 'chapters', 'section', 'sections', 'page', 'pages', 'appendix', 'appendices',\n",
    "    'table', 'tables', 'figure', 'figures', 'chart', 'charts', 'graph', 'graphs',\n",
    "    'note', 'notes', 'footnote', 'footnotes', 'reference', 'references',\n",
    "    \n",
    "    # Organizational structure  \n",
    "    'employees', 'staff', 'workers', 'people', 'personnel', 'team', 'teams',\n",
    "    'office', 'offices', 'location', 'locations', 'site', 'sites', 'facility', 'facilities',\n",
    "    'country', 'countries', 'region', 'regions', 'city', 'cities',\n",
    "    'department', 'departments', 'division', 'divisions', 'unit', 'units',\n",
    "    \n",
    "    # Business entities\n",
    "    'customer', 'customers', 'client', 'clients', 'supplier', 'suppliers',\n",
    "    'shareholder', 'shareholders', 'stakeholder', 'stakeholders', 'investor', 'investors',\n",
    "    'partner', 'partners', 'contractor', 'contractors',\n",
    "    \n",
    "    # Time periods (non-quantitative)\n",
    "    'year', 'years', 'month', 'months', 'quarter', 'quarters', 'week', 'weeks',\n",
    "    'day', 'days', 'hour', 'hours', 'minute', 'minutes',\n",
    "    'years of experience', 'years experience', 'experience',\n",
    "    \n",
    "    # Document numbering\n",
    "    'item', 'items', 'point', 'points', 'step', 'steps', 'phase', 'phases',\n",
    "    'level', 'levels', 'tier', 'tiers', 'grade', 'grades', 'class', 'classes',\n",
    "    \n",
    "    # Financial/legal (non-environmental)\n",
    "    'share', 'shares', 'stock', 'stocks', 'option', 'options',\n",
    "    'contract', 'contracts', 'agreement', 'agreements',\n",
    "    \n",
    "    # Generic counts that aren't meaningful\n",
    "    'number', 'numbers', 'amount', 'amounts', 'quantity', 'quantities',\n",
    "    'total', 'sum', 'count', 'instance', 'instances', 'case', 'cases'\n",
    "]\n",
    "\n",
    "# Expanded relative quantifiers\n",
    "RELATIVE_QUANTIFIERS = [\n",
    "    # Multiplication\n",
    "    'doubled', 'tripled', 'quadrupled', 'quintupled', 'multiplied',\n",
    "    'more than doubled', 'nearly doubled', 'almost doubled', 'over doubled',\n",
    "    'more than tripled', 'nearly tripled', 'almost tripled',\n",
    "    'increased twofold', 'increased threefold', 'increased fourfold',\n",
    "    'grew twofold', 'grew threefold', 'expanded twofold',\n",
    "    \n",
    "    # Division/reduction\n",
    "    'halved', 'quartered', 'cut in half', 'cut by half', 'reduced by half',\n",
    "    'decreased by half', 'slashed in half', 'divided by half',\n",
    "    'cut by three quarters', 'reduced by three quarters',\n",
    "    \n",
    "    # Fractional increases/decreases\n",
    "    'increased by half', 'grew by half', 'expanded by half',\n",
    "    'increased by a third', 'grew by a third', 'expanded by a third',\n",
    "    'decreased by a third', 'reduced by a third', 'cut by a third',\n",
    "    'increased by two thirds', 'grew by two thirds',\n",
    "    'decreased by two thirds', 'reduced by two thirds',\n",
    "    \n",
    "    # Significant changes\n",
    "    'dramatically increased', 'dramatically decreased', 'dramatically reduced',\n",
    "    'significantly increased', 'significantly decreased', 'significantly reduced',\n",
    "    'substantially increased', 'substantially decreased', 'substantially reduced',\n",
    "    'markedly increased', 'markedly decreased', 'markedly reduced',\n",
    "    'considerably increased', 'considerably decreased', 'considerably reduced',\n",
    "    \n",
    "    # Amplification terms\n",
    "    'amplified', 'magnified', 'intensified', 'escalated', 'boosted',\n",
    "    'enhanced', 'strengthened', 'reinforced', 'accelerated',\n",
    "    'diminished', 'weakened', 'scaled back', 'scaled down', 'minimized',\n",
    "    \n",
    "    # Comparative terms\n",
    "    'surpassed', 'exceeded', 'outperformed', 'outdid', 'topped',\n",
    "    'fell short of', 'underperformed', 'lagged behind',\n",
    "\n",
    "    # EXPANDED LIST - Add more common relative terms\n",
    "    'increased', 'decreased', 'reduced', 'improved', 'enhanced', 'boosted',\n",
    "    'declined', 'dropped', 'fell', 'rose', 'grew', 'expanded',\n",
    "    'contracted', 'shrank', 'diminished', 'escalated', 'accelerated',\n",
    "    'slowed', 'stabilized', 'maintained', 'sustained', 'optimized',\n",
    "    'maximized', 'minimized', 'elevated', 'lowered', 'raised',\n",
    "    'strengthened', 'weakened', 'intensified', 'amplified', 'magnified',\n",
    "    'better', 'worse', 'higher', 'lower', 'greater', 'lesser',\n",
    "    'more', 'less', 'fewer', 'additional', 'extra', 'surplus',\n",
    "    'deficit', 'shortfall', 'excess', 'beyond', 'below', 'above',\n",
    "    'up', 'down', 'upward', 'downward', 'forward', 'backward',\n",
    "    'progress', 'regress', 'advance', 'retreat', 'gain', 'loss',\n",
    "    'positive', 'negative', 'favorable', 'unfavorable', 'beneficial', 'detrimental'\n",
    "]\n",
    "\n",
    "# Year patterns to exclude (but with context checking)\n",
    "YEAR_PATTERNS = [\n",
    "    r'\\b(19|20)\\d{2}\\b',  # 4-digit years\n",
    "    r'\\b(19|20)\\d{2}-(19|20)\\d{2}\\b',  # Year ranges\n",
    "    r'\\bby\\s+(19|20)\\d{2}\\b',  # \"by YEAR\"\n",
    "    r'\\bin\\s+(19|20)\\d{2}\\b',  # \"in YEAR\"\n",
    "    r'\\bsince\\s+(19|20)\\d{2}\\b',  # \"since YEAR\"\n",
    "    r'\\buntil\\s+(19|20)\\d{2}\\b'  # \"until YEAR\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7394fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_currency_quantifications(doc):\n",
    "    \"\"\"\n",
    "    Reconstruct currency quantifications using unified pattern detection.\n",
    "    \"\"\"\n",
    "    currency_quantifications = []\n",
    "    \n",
    "    for i, token in enumerate(doc):\n",
    "        token_text = token.text.lower()\n",
    "        \n",
    "        # Check if current token is a currency symbol\n",
    "        if any(symbol.lower() == token_text for symbol in CURRENCY_SYMBOLS):\n",
    "            # Look for numbers within 3 tokens before or after the currency symbol\n",
    "            for direction in [-1, 1]:\n",
    "                for offset in range(1, 4):\n",
    "                    num_idx = i + (direction * offset)\n",
    "                    if 0 <= num_idx < len(doc):\n",
    "                        num_token = doc[num_idx]\n",
    "                        \n",
    "                        if num_token.pos_ == 'NUM' or num_token.like_num:\n",
    "                            # Found a number, now look for unit markers\n",
    "                            unit_found = None\n",
    "                            quantification_tokens = []\n",
    "                            \n",
    "                            # Determine the range of tokens to include\n",
    "                            if direction == -1:  # Number before currency\n",
    "                                start_idx = num_idx\n",
    "                                end_idx = i\n",
    "                            else:  # Number after currency\n",
    "                                start_idx = i\n",
    "                                end_idx = num_idx\n",
    "                            \n",
    "                            # Look for unit markers after the number\n",
    "                            for unit_offset in range(1, 3):\n",
    "                                unit_idx = num_idx + unit_offset\n",
    "                                if 0 <= unit_idx < len(doc):\n",
    "                                    unit_token = doc[unit_idx]\n",
    "                                    if unit_token.text in CURRENCY_UNITS:\n",
    "                                        unit_found = unit_token.text\n",
    "                                        end_idx = max(end_idx, unit_idx)\n",
    "                                        break\n",
    "                            \n",
    "                            quantification_tokens = [doc[j].text for j in range(start_idx, end_idx + 1)]\n",
    "                            quantification_text = ' '.join(quantification_tokens)\n",
    "                            \n",
    "                            currency_quantifications.append({\n",
    "                                'text': quantification_text,\n",
    "                                'start_idx': start_idx,\n",
    "                                'end_idx': end_idx,\n",
    "                                'type': 'currency',\n",
    "                                'value': num_token.text,\n",
    "                                'unit': unit_found\n",
    "                            })\n",
    "    \n",
    "    return currency_quantifications\n",
    "\n",
    "def reconstruct_percentage_quantifications(doc):\n",
    "    \"\"\"\n",
    "    Reconstruct percentage quantifications from split tokens.\n",
    "    \"\"\"\n",
    "    percentage_quantifications = []\n",
    "    \n",
    "    for i, token in enumerate(doc):\n",
    "        token_text = token.text.lower()\n",
    "        \n",
    "        if any(pct in token_text for pct in PERCENTAGE_PATTERNS):\n",
    "            # Look for numbers within 3 tokens before this percentage marker\n",
    "            for offset in range(-3, 1):\n",
    "                num_idx = i + offset\n",
    "                if 0 <= num_idx < len(doc):\n",
    "                    num_token = doc[num_idx]\n",
    "                    \n",
    "                    if num_token.pos_ == 'NUM' or num_token.like_num:\n",
    "                        quantification_text = ' '.join([doc[j].text for j in range(num_idx, i + 1)])\n",
    "                        \n",
    "                        percentage_quantifications.append({\n",
    "                            'text': quantification_text,\n",
    "                            'start_idx': num_idx,\n",
    "                            'end_idx': i,\n",
    "                            'type': 'percentage',\n",
    "                            'value': num_token.text\n",
    "                        })\n",
    "                        break\n",
    "    \n",
    "    return percentage_quantifications\n",
    "\n",
    "def reconstruct_unit_quantifications(doc):\n",
    "    \"\"\"\n",
    "    Reconstruct unit-based quantifications (MW, tons CO2, etc.).\n",
    "    \"\"\"\n",
    "    unit_quantifications = []\n",
    "    \n",
    "    for i, token in enumerate(doc):\n",
    "        token_text = token.text\n",
    "        \n",
    "        # Check against all unit patterns\n",
    "        for unit_category, units in UNIT_PATTERNS.items():\n",
    "            if token_text in units or token_text.lower() in [u.lower() for u in units]:\n",
    "                # Look for numbers within 3 tokens before this unit\n",
    "                for offset in range(-3, 1):\n",
    "                    num_idx = i + offset\n",
    "                    if 0 <= num_idx < len(doc):\n",
    "                        num_token = doc[num_idx]\n",
    "                        \n",
    "                        if num_token.pos_ == 'NUM' or num_token.like_num:\n",
    "                            # Check for additional unit words (like \"tons CO2\")\n",
    "                            end_idx = i\n",
    "                            quantification_tokens = [doc[j].text for j in range(num_idx, i + 1)]\n",
    "                            \n",
    "                            # Look ahead for additional unit components\n",
    "                            for lookahead in range(1, 3):\n",
    "                                next_idx = i + lookahead\n",
    "                                if next_idx < len(doc):\n",
    "                                    next_token = doc[next_idx]\n",
    "                                    if (next_token.text in ['CO2', 'CO2e', 'equivalent', 'eq'] or\n",
    "                                        next_token.pos_ in ['NOUN', 'PROPN']):\n",
    "                                        quantification_tokens.append(next_token.text)\n",
    "                                        end_idx = next_idx\n",
    "                                    else:\n",
    "                                        break\n",
    "                            \n",
    "                            quantification_text = ' '.join(quantification_tokens)\n",
    "                            \n",
    "                            unit_quantifications.append({\n",
    "                                'text': quantification_text,\n",
    "                                'start_idx': num_idx,\n",
    "                                'end_idx': end_idx,\n",
    "                                'type': 'unit',\n",
    "                                'category': unit_category,\n",
    "                                'value': num_token.text,\n",
    "                                'unit': token_text\n",
    "                            })\n",
    "                            break\n",
    "    \n",
    "    return unit_quantifications\n",
    "\n",
    "def find_all_quantifications(doc):\n",
    "    \"\"\"\n",
    "    Find all quantifications in a document by combining different detection methods.\n",
    "    IMPROVED: Better overlap removal logic.\n",
    "    \"\"\"\n",
    "    all_quantifications = []\n",
    "    \n",
    "    # Get different types of quantifications\n",
    "    currency_quants = reconstruct_currency_quantifications(doc)\n",
    "    percentage_quants = reconstruct_percentage_quantifications(doc)\n",
    "    unit_quants = reconstruct_unit_quantifications(doc)\n",
    "    \n",
    "    all_quantifications.extend(currency_quants)\n",
    "    all_quantifications.extend(percentage_quants)\n",
    "    all_quantifications.extend(unit_quants)\n",
    "    \n",
    "    # Remove overlapping quantifications with better logic\n",
    "    filtered_quantifications = remove_overlapping_quantifications(all_quantifications)\n",
    "    \n",
    "    return filtered_quantifications\n",
    "\n",
    "def remove_overlapping_quantifications(quantifications):\n",
    "    \"\"\"\n",
    "    Remove overlapping quantifications, preferring higher-quality ones.\n",
    "    \"\"\"\n",
    "    if not quantifications:\n",
    "        return []\n",
    "    \n",
    "    # Sort by start position first\n",
    "    sorted_quants = sorted(quantifications, key=lambda x: x['start_idx'])\n",
    "    \n",
    "    # Define type priority (higher number = higher priority)\n",
    "    type_priority = {\n",
    "        'currency': 4,\n",
    "        'percentage': 4,\n",
    "        'unit': 4,\n",
    "        'meaningful_count': 2,\n",
    "        'relative': 1\n",
    "    }\n",
    "    \n",
    "    filtered = []\n",
    "    \n",
    "    for current in sorted_quants:\n",
    "        current_positions = set(range(current['start_idx'], current['end_idx'] + 1))\n",
    "        current_priority = type_priority.get(current['type'], 0)\n",
    "        \n",
    "        # Check for overlaps with already added quantifications\n",
    "        should_add = True\n",
    "        to_remove = []\n",
    "        \n",
    "        for i, existing in enumerate(filtered):\n",
    "            existing_positions = set(range(existing['start_idx'], existing['end_idx'] + 1))\n",
    "            \n",
    "            # Check if they overlap\n",
    "            if current_positions & existing_positions:\n",
    "                existing_priority = type_priority.get(existing['type'], 0)\n",
    "                \n",
    "                # Decide which one to keep based on priority and quality\n",
    "                if current_priority > existing_priority:\n",
    "                    # Current is better, mark existing for removal\n",
    "                    to_remove.append(i)\n",
    "                elif current_priority < existing_priority:\n",
    "                    # Existing is better, don't add current\n",
    "                    should_add = False\n",
    "                    break\n",
    "                else:\n",
    "                    # Same priority, prefer the more specific one\n",
    "                    if is_more_specific(current, existing):\n",
    "                        to_remove.append(i)\n",
    "                    else:\n",
    "                        should_add = False\n",
    "                        break\n",
    "        \n",
    "        # Remove marked items (in reverse order to maintain indices)\n",
    "        for i in reversed(to_remove):\n",
    "            filtered.pop(i)\n",
    "        \n",
    "        # Add current if it should be added\n",
    "        if should_add:\n",
    "            filtered.append(current)\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "def is_more_specific(quant1, quant2):\n",
    "    \"\"\"\n",
    "    Determine if quant1 is more specific than quant2.\n",
    "    \"\"\"\n",
    "    # Prefer shorter, more precise quantifications\n",
    "    len1 = len(quant1['text'])\n",
    "    len2 = len(quant2['text'])\n",
    "    \n",
    "    # If one is much longer, prefer the shorter one\n",
    "    if len1 < len2 * 0.8:\n",
    "        return True\n",
    "    elif len2 < len1 * 0.8:\n",
    "        return False\n",
    "    \n",
    "    # If similar length, prefer the one with less extra text\n",
    "    text1_clean = quant1['text'].lower().strip()\n",
    "    text2_clean = quant2['text'].lower().strip()\n",
    "    \n",
    "    # Count non-alphanumeric characters (indicators of extra text)\n",
    "    extra_chars1 = sum(1 for c in text1_clean if not (c.isalnum() or c in '.,-%€$£¥₹'))\n",
    "    extra_chars2 = sum(1 for c in text2_clean if not (c.isalnum() or c in '.,-%€$£¥₹'))\n",
    "    \n",
    "    return extra_chars1 < extra_chars2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde08fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_meaningful_counts(doc):\n",
    "    \"\"\"\n",
    "    Find meaningful numerical counts that aren't captured by other methods.\n",
    "    \"\"\"\n",
    "    meaningful_counts = []\n",
    "    \n",
    "    for i, token in enumerate(doc):\n",
    "        if token.pos_ == 'NUM' or token.like_num:\n",
    "            # Skip decimals that are likely organizational (like version numbers)\n",
    "            if '.' in token.text:\n",
    "                context_start = max(0, i - 3)\n",
    "                context_end = min(len(doc), i + 3)\n",
    "                context_words = [doc[j].text.lower() for j in range(context_start, context_end)]\n",
    "                \n",
    "                # Check if surrounded by organizational terms\n",
    "                if any(org_term in ' '.join(context_words) for org_term in ORGANIZATIONAL_TERMS):\n",
    "                    continue\n",
    "            \n",
    "            # Look for meaningful objects after the number\n",
    "            for phrase_length in range(1, 5):\n",
    "                if i + phrase_length < len(doc):\n",
    "                    object_tokens = [doc[i + j + 1] for j in range(phrase_length)]\n",
    "                    object_phrase = ' '.join([t.text.lower() for t in object_tokens])\n",
    "                    \n",
    "                    # Check if this forms a meaningful quantification\n",
    "                    if object_tokens:\n",
    "                        # Must include at least one noun\n",
    "                        has_noun = any(token.pos_ in ['NOUN', 'PROPN'] for token in object_tokens)\n",
    "                        \n",
    "                        if has_noun:\n",
    "                            # Check exclusions\n",
    "                            should_exclude = False\n",
    "                            \n",
    "                            # Original exclusion check for EXCLUDED_OBJECTS\n",
    "                            if not should_exclude:\n",
    "                                for excluded_obj in EXCLUDED_OBJECTS:\n",
    "                                    if (excluded_obj.lower() in object_phrase or \n",
    "                                        any(word in object_phrase for word in excluded_obj.lower().split())):\n",
    "                                        should_exclude = True\n",
    "                                        break\n",
    "                            \n",
    "                            # Also exclude if it looks like a list item (e.g., \"1.\", \"2)\", \"a)\", etc.)\n",
    "                            if not should_exclude:\n",
    "                                if re.match(r'^\\d+[\\.\\)]\\s*$', token.text) or re.match(r'^[a-z][\\.\\)]\\s*$', token.text):\n",
    "                                    should_exclude = True\n",
    "                            \n",
    "                            if not should_exclude and len(object_phrase.strip()) > 2:  # Must be meaningful\n",
    "                                end_idx = i + len(object_tokens)\n",
    "                                quantification_text = f\"{token.text} {object_phrase}\"\n",
    "                                \n",
    "                                meaningful_counts.append({\n",
    "                                    'text': quantification_text,\n",
    "                                    'start_idx': i,\n",
    "                                    'end_idx': min(end_idx, len(doc) - 1),\n",
    "                                    'type': 'meaningful_count',\n",
    "                                    'value': token.text,\n",
    "                                    'object': object_phrase\n",
    "                                })\n",
    "                                break\n",
    "    \n",
    "    return meaningful_counts\n",
    "\n",
    "def find_relative_quantifiers(doc):\n",
    "    \"\"\"\n",
    "    Find relative quantifiers like 'doubled', 'tripled', 'halved'.\n",
    "    \"\"\"\n",
    "    relative_quants = []\n",
    "    \n",
    "    for i, token in enumerate(doc):\n",
    "        token_lemma = token.lemma_.lower()\n",
    "        token_text = token.text.lower()\n",
    "        \n",
    "        # Check for single-word relative quantifiers\n",
    "        if token_lemma in RELATIVE_QUANTIFIERS or token_text in RELATIVE_QUANTIFIERS:\n",
    "            relative_quants.append({\n",
    "                'text': token.text,\n",
    "                'start_idx': i,\n",
    "                'end_idx': i,\n",
    "                'type': 'relative',\n",
    "                'quantifier': token_text\n",
    "            })\n",
    "        \n",
    "        # Check for multi-word patterns\n",
    "        for phrase_length in [2, 3, 4, 5]:\n",
    "            if i + phrase_length - 1 < len(doc):\n",
    "                phrase_tokens = [doc[i + j].text.lower() for j in range(phrase_length)]\n",
    "                phrase = ' '.join(phrase_tokens)\n",
    "                \n",
    "                if phrase in RELATIVE_QUANTIFIERS:\n",
    "                    relative_quants.append({\n",
    "                        'text': phrase,\n",
    "                        'start_idx': i,\n",
    "                        'end_idx': i + phrase_length - 1,\n",
    "                        'type': 'relative',\n",
    "                        'quantifier': phrase\n",
    "                    })\n",
    "                    break  # Don't check longer phrases starting at same position\n",
    "    \n",
    "    return relative_quants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad75076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_year_quantification(quantification, doc):\n",
    "    \"\"\"\n",
    "    Determine if a quantification is actually a year that should be excluded.\n",
    "    \"\"\"\n",
    "    quant_text = quantification['text'].lower()\n",
    "    \n",
    "    # Check if the quantification matches year patterns\n",
    "    for year_pattern in YEAR_PATTERNS:\n",
    "        if re.search(year_pattern, quant_text, re.IGNORECASE):\n",
    "            # Special case: Check if year is used with units (like \"2025 MW\")\n",
    "            if quantification.get('type') == 'unit':\n",
    "                return False\n",
    "            \n",
    "            # Check context around the quantification for unit indicators\n",
    "            start_idx = quantification['start_idx']\n",
    "            end_idx = quantification['end_idx']\n",
    "            \n",
    "            # Look at surrounding tokens for unit indicators\n",
    "            for i in range(max(0, start_idx - 3), min(len(doc), end_idx + 4)):\n",
    "                if i < start_idx or i > end_idx:\n",
    "                    token = doc[i]\n",
    "                    token_text = token.text.lower()\n",
    "                    \n",
    "                    # If we find units near the year, it's probably a valid quantification\n",
    "                    for unit_category, units in UNIT_PATTERNS.items():\n",
    "                        if token_text in [u.lower() for u in units]:\n",
    "                            return False\n",
    "            \n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def filter_year_quantifications(quantifications, doc):\n",
    "    \"\"\"\n",
    "    Filter out year-based quantifications that aren't meaningful.\n",
    "    \"\"\"\n",
    "    return [quant for quant in quantifications if not is_year_quantification(quant, doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3020f7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_syntactic_connection(green_term, quantification, doc):\n",
    "    \"\"\"\n",
    "    Determine if a quantification is syntactically connected to a green term.\n",
    "    MORE SEMANTICALLY AWARE - only meaningful grammatical relationships.\n",
    "    \"\"\"\n",
    "    green_start = green_term['start_idx']\n",
    "    green_end = green_term['end_idx']\n",
    "    quant_start = quantification['start_idx']\n",
    "    quant_end = quantification['end_idx']\n",
    "    \n",
    "    # Check direct dependency relationships\n",
    "    for green_idx in range(green_start, green_end + 1):\n",
    "        green_token = doc[green_idx]\n",
    "        for quant_idx in range(quant_start, quant_end + 1):\n",
    "            quant_token = doc[quant_idx]\n",
    "            \n",
    "            # Direct dependency relationship\n",
    "            if quant_token.head == green_token or green_token.head == quant_token:\n",
    "                return True, 0.9, 'direct_dependency'\n",
    "            \n",
    "            # Check for specific meaningful relationships\n",
    "            if green_token.head == quant_token.head:\n",
    "                shared_head = green_token.head\n",
    "                \n",
    "                # Both modify the same noun (e.g., \"2,284,254 MWh of clean energy\")\n",
    "                if shared_head.pos_ == 'NOUN' and shared_head.dep_ in ['nsubj', 'dobj', 'pobj']:\n",
    "                    return True, 0.8, 'shared_noun_head'\n",
    "                \n",
    "                # Both are objects of the same verb (e.g., \"produced 2,284,254 MWh of clean energy\")\n",
    "                if shared_head.pos_ == 'VERB' and green_token.dep_ in ['dobj', 'pobj'] and quant_token.dep_ in ['dobj', 'pobj']:\n",
    "                    return True, 0.8, 'shared_verb_objects'\n",
    "    \n",
    "    # Check for quantification modifying the green term directly\n",
    "    # E.g., \"17,000 tons of recyclable materials\"\n",
    "    for green_idx in range(green_start, green_end + 1):\n",
    "        green_token = doc[green_idx]\n",
    "        for quant_idx in range(quant_start, quant_end + 1):\n",
    "            quant_token = doc[quant_idx]\n",
    "            \n",
    "            # Quantification modifies green term through \"of\" relationship\n",
    "            if quant_token.head == green_token and any(child.text.lower() == 'of' for child in quant_token.children):\n",
    "                return True, 0.9, 'quantification_of_green_term'\n",
    "            \n",
    "            # Green term modifies quantification (e.g., \"clean energy production\")\n",
    "            if green_token.head == quant_token and green_token.dep_ in ['amod', 'compound']:\n",
    "                return True, 0.8, 'green_term_modifies_quantification'\n",
    "    \n",
    "    return False, 0.0, 'no_syntactic_connection'\n",
    "\n",
    "def find_proximity_connection(green_term, quantification, doc):\n",
    "    \"\"\"\n",
    "    Determine if a quantification is close enough to a green term to be connected.\n",
    "    MORE SELECTIVE - only connects to closest quantification if multiple are nearby.\n",
    "    \"\"\"\n",
    "    green_center = (green_term['start_idx'] + green_term['end_idx']) / 2\n",
    "    quant_center = (quantification['start_idx'] + quantification['end_idx']) / 2\n",
    "    distance = abs(green_center - quant_center)\n",
    "    \n",
    "    # Check if they're in the same sentence\n",
    "    green_sent = doc[green_term['start_idx']].sent\n",
    "    quant_sent = doc[quantification['start_idx']].sent\n",
    "    \n",
    "    if green_sent == quant_sent:\n",
    "        # Same sentence - be more selective\n",
    "        if distance <= 3:\n",
    "            return True, 0.8, distance\n",
    "        elif distance <= 6:\n",
    "            return True, 0.6, distance\n",
    "        elif distance <= 10:\n",
    "            return True, 0.4, distance\n",
    "        else:\n",
    "            return False, 0.0, distance\n",
    "    \n",
    "    # Different sentences - much more restrictive\n",
    "    if abs(green_sent.start - quant_sent.start) == 1:  # Adjacent sentences\n",
    "        if distance <= 15:\n",
    "            return True, 0.3, distance\n",
    "    \n",
    "    return False, 0.0, distance\n",
    "\n",
    "def find_pattern_connection(green_term, quantification, doc):\n",
    "    \"\"\"\n",
    "    Find pattern-based connections between green terms and quantifications.\n",
    "    MUCH MORE RESTRICTIVE - only connects if quantification is directly describing the green term.\n",
    "    \"\"\"\n",
    "    green_start = green_term['start_idx']\n",
    "    green_end = green_term['end_idx']\n",
    "    quant_start = quantification['start_idx']\n",
    "    quant_end = quantification['end_idx']\n",
    "    \n",
    "    # Only check if they're in the same sentence\n",
    "    sentence = doc[green_start].sent\n",
    "    quant_sentence = doc[quant_start].sent\n",
    "    \n",
    "    if sentence != quant_sentence:\n",
    "        return False, 0.0, 'different_sentences'\n",
    "    \n",
    "    # Get the text between green term and quantification\n",
    "    if green_end < quant_start:\n",
    "        # Green term comes before quantification\n",
    "        between_start = green_end + 1\n",
    "        between_end = quant_start - 1\n",
    "        context_text = ' '.join([doc[i].text.lower() for i in range(max(green_start-2, sentence.start), min(quant_end+3, sentence.end))])\n",
    "    elif quant_end < green_start:\n",
    "        # Quantification comes before green term\n",
    "        between_start = quant_end + 1\n",
    "        between_end = green_start - 1\n",
    "        context_text = ' '.join([doc[i].text.lower() for i in range(max(quant_start-2, sentence.start), min(green_end+3, sentence.end))])\n",
    "    else:\n",
    "        # Overlapping or adjacent\n",
    "        context_text = ' '.join([doc[i].text.lower() for i in range(max(min(green_start, quant_start)-2, sentence.start), min(max(green_end, quant_end)+3, sentence.end))])\n",
    "    \n",
    "    # Strong patterns - quantification directly describes the green term\n",
    "    strong_patterns = [\n",
    "        # Direct measurement patterns\n",
    "        r'(\\d+[\\.,]?\\d*)\\s*(mwh|gwh|twh|kw|mw|gw|tw|kwh)\\s+(of|from)\\s+(clean|green|renewable)',\n",
    "        r'(clean|green|renewable)\\s+(energy|power)\\s+(of|from|produced|generated)?\\s*(\\d+[\\.,]?\\d*)',\n",
    "        r'(save|saving|saved|reduce|reducing|reduced)\\s+(approximately|about|around)?\\s*(\\d+[\\.,]?\\d*)',\n",
    "        r'(cost|costs|costing|at|less than|under|over)\\s+([€$£¥₹]?\\d+[\\.,]?\\d*)',\n",
    "        r'(invest|invested|investment|allocated|allocate|spend|spent|spending)\\s+([€$£¥₹]?\\d+[\\.,]?\\d*)',\n",
    "        r'(target|targeting|aim|aiming|goal|objective)\\s+(of|to)?\\s*(\\d+[\\.,]?\\d*)',\n",
    "        r'(achieve|achieved|reach|reached|attain|attained)\\s+(\\d+[\\.,]?\\d*)',\n",
    "        r'(increase|increased|grow|grew|rise|rose)\\s+(by|to)\\s+(\\d+[\\.,]?\\d*)',\n",
    "        r'(decrease|decreased|reduce|reduced|cut|lower|lowered)\\s+(by|to)\\s+(\\d+[\\.,]?\\d*)',\n",
    "        r'(\\d+[\\.,]?\\d*)\\s*(tons?|tonnes?|kg|mt|kt|gt)\\s+(of|from)\\s+(waste|emissions?|co2|recyclable)'\n",
    "    ]\n",
    "    \n",
    "    # Check for strong patterns\n",
    "    for pattern in strong_patterns:\n",
    "        if re.search(pattern, context_text, re.IGNORECASE):\n",
    "            return True, 0.9, 'strong_pattern'\n",
    "    \n",
    "    # Medium patterns - quantification is in close proximity with connecting words\n",
    "    medium_patterns = [\n",
    "        r'(approximately|about|around|roughly|nearly|almost|close to|up to|as much as|at least|more than|less than|over|under)\\s+(\\d+[\\.,]?\\d*)',\n",
    "        r'(\\d+[\\.,]?\\d*)\\s+(per|each|every)\\s+(year|month|day|hour)',\n",
    "        r'(total|sum|amount|quantity)\\s+(of|to)?\\s*(\\d+[\\.,]?\\d*)',\n",
    "        r'(\\d+[\\.,]?\\d*)\\s*(million|billion|thousand|m|b|k|%|percent)'\n",
    "    ]\n",
    "    \n",
    "    # Only apply medium patterns if green term and quantification are very close (within 5 tokens)\n",
    "    distance = min(abs(green_start - quant_start), abs(green_end - quant_end))\n",
    "    if distance <= 5:\n",
    "        for pattern in medium_patterns:\n",
    "            if re.search(pattern, context_text, re.IGNORECASE):\n",
    "                return True, 0.6, 'medium_pattern'\n",
    "    \n",
    "    return False, 0.0, 'no_pattern'\n",
    "\n",
    "def assess_quantification_connection(green_term, quantification, doc):\n",
    "    \"\"\"\n",
    "    Assess the overall connection between a green term and quantification.\n",
    "    MORE RESTRICTIVE - requires stronger evidence for connection.\n",
    "    \"\"\"\n",
    "    # Try all connection methods\n",
    "    syntactic_connected, syntactic_strength, syntactic_type = find_syntactic_connection(green_term, quantification, doc)\n",
    "    proximity_connected, proximity_strength, proximity_distance = find_proximity_connection(green_term, quantification, doc)\n",
    "    pattern_connected, pattern_strength, pattern_type = find_pattern_connection(green_term, quantification, doc)\n",
    "    \n",
    "    # NEW: More restrictive connection logic\n",
    "    # Need either strong syntactic connection OR strong pattern connection OR very close proximity\n",
    "    \n",
    "    if syntactic_connected and syntactic_strength >= 0.8:\n",
    "        # Strong syntactic connection is sufficient\n",
    "        return {\n",
    "            'is_connected': True,\n",
    "            'connection_strength': syntactic_strength,\n",
    "            'primary_method': 'syntactic',\n",
    "            'connection_details': {\n",
    "                'syntactic': {'connected': True, 'strength': syntactic_strength, 'type': syntactic_type},\n",
    "                'proximity': {'connected': proximity_connected, 'strength': proximity_strength, 'distance': proximity_distance},\n",
    "                'pattern': {'connected': pattern_connected, 'strength': pattern_strength, 'type': pattern_type}\n",
    "            }\n",
    "        }\n",
    "    elif pattern_connected and pattern_strength >= 0.8:\n",
    "        # Strong pattern connection is sufficient\n",
    "        return {\n",
    "            'is_connected': True,\n",
    "            'connection_strength': pattern_strength,\n",
    "            'primary_method': 'pattern',\n",
    "            'connection_details': {\n",
    "                'syntactic': {'connected': syntactic_connected, 'strength': syntactic_strength, 'type': syntactic_type},\n",
    "                'proximity': {'connected': proximity_connected, 'strength': proximity_strength, 'distance': proximity_distance},\n",
    "                'pattern': {'connected': True, 'strength': pattern_strength, 'type': pattern_type}\n",
    "            }\n",
    "        }\n",
    "    elif syntactic_connected and proximity_connected and proximity_distance <= 5:\n",
    "        # Medium syntactic + very close proximity\n",
    "        combined_strength = (syntactic_strength + proximity_strength) / 2\n",
    "        return {\n",
    "            'is_connected': True,\n",
    "            'connection_strength': combined_strength,\n",
    "            'primary_method': 'syntactic',\n",
    "            'connection_details': {\n",
    "                'syntactic': {'connected': True, 'strength': syntactic_strength, 'type': syntactic_type},\n",
    "                'proximity': {'connected': True, 'strength': proximity_strength, 'distance': proximity_distance},\n",
    "                'pattern': {'connected': pattern_connected, 'strength': pattern_strength, 'type': pattern_type}\n",
    "            }\n",
    "        }\n",
    "    elif pattern_connected and proximity_connected and proximity_distance <= 3:\n",
    "        # Medium pattern + very close proximity\n",
    "        combined_strength = (pattern_strength + proximity_strength) / 2\n",
    "        return {\n",
    "            'is_connected': True,\n",
    "            'connection_strength': combined_strength,\n",
    "            'primary_method': 'pattern',\n",
    "            'connection_details': {\n",
    "                'syntactic': {'connected': syntactic_connected, 'strength': syntactic_strength, 'type': syntactic_type},\n",
    "                'proximity': {'connected': True, 'strength': proximity_strength, 'distance': proximity_distance},\n",
    "                'pattern': {'connected': True, 'strength': pattern_strength, 'type': pattern_type}\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        # No sufficient connection\n",
    "        return {\n",
    "            'is_connected': False,\n",
    "            'connection_strength': 0.0,\n",
    "            'primary_method': 'none',\n",
    "            'connection_details': {\n",
    "                'syntactic': {'connected': syntactic_connected, 'strength': syntactic_strength, 'type': syntactic_type},\n",
    "                'proximity': {'connected': proximity_connected, 'strength': proximity_strength, 'distance': proximity_distance},\n",
    "                'pattern': {'connected': pattern_connected, 'strength': pattern_strength, 'type': pattern_type}\n",
    "            }\n",
    "        }\n",
    "    \n",
    "def filter_best_connections(green_term, all_connected_quantifications, doc):\n",
    "    \"\"\"\n",
    "    Filter to keep only the best/most relevant connections for each green term.\n",
    "    \"\"\"\n",
    "    if not all_connected_quantifications:\n",
    "        return []\n",
    "    \n",
    "    # Group by quantification type\n",
    "    by_type = {}\n",
    "    for quant_info in all_connected_quantifications:\n",
    "        quant_type = quant_info['quantification']['type']\n",
    "        if quant_type not in by_type:\n",
    "            by_type[quant_type] = []\n",
    "        by_type[quant_type].append(quant_info)\n",
    "    \n",
    "    # For each type, keep only the best connection(s)\n",
    "    filtered_connections = []\n",
    "    \n",
    "    for quant_type, quant_list in by_type.items():\n",
    "        # Sort by connection strength (descending)\n",
    "        quant_list.sort(key=lambda x: x['connection']['connection_strength'], reverse=True)\n",
    "        \n",
    "        # Keep only the strongest connection(s) for this type\n",
    "        if quant_type in ['currency', 'percentage', 'unit']:\n",
    "            # For important types, keep top 2 connections if they're both strong\n",
    "            if len(quant_list) >= 2 and quant_list[1]['connection']['connection_strength'] >= 0.7:\n",
    "                filtered_connections.extend(quant_list[:2])\n",
    "            else:\n",
    "                filtered_connections.append(quant_list[0])\n",
    "        else:\n",
    "            # For other types, keep only the best one\n",
    "            filtered_connections.append(quant_list[0])\n",
    "    \n",
    "    return filtered_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b3f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_quantification_level(green_term_quantifications):\n",
    "    \"\"\"\n",
    "    Classify the quantification level of a green term based on its connected quantifications.\n",
    "    \"\"\"\n",
    "    if not green_term_quantifications:\n",
    "        return 'non_quantified', 0.0\n",
    "    \n",
    "    # Calculate weighted confidence scores\n",
    "    highly_quantified_score = 0\n",
    "    partially_quantified_score = 0\n",
    "    \n",
    "    for quant_info in green_term_quantifications:\n",
    "        quant = quant_info['quantification']\n",
    "        connection = quant_info['connection']\n",
    "        quant_type = quant['type']\n",
    "        connection_strength = connection['connection_strength']\n",
    "        \n",
    "        if quant_type in ['currency', 'percentage', 'unit', 'meaningful_count']:\n",
    "            # Weight by quantification type importance\n",
    "            if quant_type in ['currency', 'percentage', 'unit']:\n",
    "                highly_quantified_score += 2.0 * connection_strength  # Strong evidence\n",
    "            else:  # meaningful_count\n",
    "                highly_quantified_score += 1.5 * connection_strength  # Moderate evidence\n",
    "                \n",
    "        elif quant_type == 'relative':\n",
    "            partially_quantified_score += 1.0 * connection_strength\n",
    "    \n",
    "    # Calculate final confidence\n",
    "    total_score = highly_quantified_score + partially_quantified_score\n",
    "    if total_score > 0:\n",
    "        confidence = min(total_score / 2.0, 1.0)  # Normalize to 0-1\n",
    "    else:\n",
    "        confidence = 0.0\n",
    "    \n",
    "    # Determine classification based on scores AND confidence\n",
    "    if highly_quantified_score > 0 and confidence >= 0.3:\n",
    "        return 'highly_quantified', confidence\n",
    "    elif partially_quantified_score > 0 and confidence >= 0.2:\n",
    "        return 'partially_quantified', confidence\n",
    "    else:\n",
    "        return 'non_quantified', confidence\n",
    "\n",
    "def add_quantification_classification(all_results, documents):\n",
    "    \"\"\"\n",
    "    Add quantification classification to all valid green terms.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"QUANTIFICATION CLASSIFICATION ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Initialize summary statistics\n",
    "    overall_quant_stats = Counter()\n",
    "    doc_quant_stats = {}\n",
    "    \n",
    "    for doc_name, results in all_results.items():\n",
    "        doc = documents[doc_name]\n",
    "        valid_terms = results['valid_terms']\n",
    "        \n",
    "        # Find all quantifications in the document\n",
    "        currency_quants = reconstruct_currency_quantifications(doc)\n",
    "        percentage_quants = reconstruct_percentage_quantifications(doc)\n",
    "        unit_quants = reconstruct_unit_quantifications(doc)\n",
    "        meaningful_counts = find_meaningful_counts(doc)\n",
    "        relative_quants = find_relative_quantifiers(doc)\n",
    "        \n",
    "        # Combine all quantifications\n",
    "        all_quantifications = []\n",
    "        all_quantifications.extend(currency_quants)\n",
    "        all_quantifications.extend(percentage_quants)\n",
    "        all_quantifications.extend(unit_quants)\n",
    "        all_quantifications.extend(meaningful_counts)\n",
    "        all_quantifications.extend(relative_quants)\n",
    "        \n",
    "        # Remove overlapping quantifications (this will handle the duplicates)\n",
    "        filtered_quantifications = remove_overlapping_quantifications(all_quantifications)\n",
    "        \n",
    "        # Filter out year-based quantifications\n",
    "        filtered_quantifications = filter_year_quantifications(filtered_quantifications, doc)\n",
    "        \n",
    "        # Classify each valid green term\n",
    "        doc_quant_counter = Counter()\n",
    "        \n",
    "        for green_term in valid_terms:\n",
    "            connected_quantifications = []\n",
    "            \n",
    "            # Check connection to each quantification\n",
    "            for quantification in filtered_quantifications:\n",
    "                connection_assessment = assess_quantification_connection(green_term, quantification, doc)\n",
    "                \n",
    "                if connection_assessment['is_connected']:\n",
    "                    connected_quantifications.append({\n",
    "                        'quantification': quantification,\n",
    "                        'connection': connection_assessment\n",
    "                    })\n",
    "            \n",
    "            # NEW: Filter to keep only the best connections\n",
    "            connected_quantifications = filter_best_connections(green_term, connected_quantifications, doc)\n",
    "            \n",
    "            # Continue with existing classification logic...\n",
    "            quantification_level, quantification_confidence = classify_quantification_level(connected_quantifications)\n",
    "            \n",
    "            # Classify quantification level\n",
    "            quantification_level, quantification_confidence = classify_quantification_level(connected_quantifications)\n",
    "\n",
    "            # Add to green term\n",
    "            green_term['quantification_level'] = quantification_level\n",
    "            green_term['quantification_confidence'] = quantification_confidence\n",
    "            green_term['quantification_score'] = {\n",
    "                'highly_score': sum(2.0 * q['connection']['connection_strength'] \n",
    "                                for q in connected_quantifications \n",
    "                                if q['quantification']['type'] in ['currency', 'percentage', 'unit', 'meaningful_count']),\n",
    "                'partially_score': sum(1.0 * q['connection']['connection_strength'] \n",
    "                                    for q in connected_quantifications \n",
    "                                    if q['quantification']['type'] == 'relative')\n",
    "            }\n",
    "            green_term['connected_quantifications'] = connected_quantifications\n",
    "            \n",
    "            # Update counters\n",
    "            doc_quant_counter[quantification_level] += 1\n",
    "            overall_quant_stats[quantification_level] += 1\n",
    "        \n",
    "        # Calculate quantification intensity score for document\n",
    "        total_terms = len(valid_terms)\n",
    "        if total_terms > 0:\n",
    "            highly_count = doc_quant_counter['highly_quantified']\n",
    "            partially_count = doc_quant_counter['partially_quantified']\n",
    "            intensity_score = ((highly_count * 1.5) + (partially_count * 1.0)) / total_terms * 100\n",
    "        else:\n",
    "            intensity_score = 0.0\n",
    "        \n",
    "        # Store document statistics\n",
    "        doc_quant_stats[doc_name] = {\n",
    "            'total_terms': total_terms,\n",
    "            'highly_quantified': doc_quant_counter['highly_quantified'],\n",
    "            'partially_quantified': doc_quant_counter['partially_quantified'],\n",
    "            'non_quantified': doc_quant_counter['non_quantified'],\n",
    "            'quantification_intensity_score': intensity_score\n",
    "        }\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nQuantification Classification Results:\")\n",
    "    total_terms = sum(overall_quant_stats.values())\n",
    "    for level in ['highly_quantified', 'partially_quantified', 'non_quantified']:\n",
    "        count = overall_quant_stats[level]\n",
    "        percentage = (count / total_terms * 100) if total_terms > 0 else 0\n",
    "        level_display = level.replace('_', ' ').title()\n",
    "        print(f\"  {level_display}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nDocument breakdown:\")\n",
    "    print(f\"{'Document':<30} {'Highly':<8} {'Partial':<8} {'None':<8} {'Total':<8}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for doc_name, stats in doc_quant_stats.items():\n",
    "        print(f\"{doc_name:<30} {stats['highly_quantified']:<8} {stats['partially_quantified']:<8} {stats['non_quantified']:<8} {stats['total_terms']:<8}\")\n",
    "    \n",
    "    # Store quantification statistics in results\n",
    "    for doc_name in all_results:\n",
    "        all_results[doc_name]['quantification_stats'] = doc_quant_stats[doc_name]\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c3c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply quantification classification to all documents\n",
    "all_results = add_quantification_classification(all_results, documents)\n",
    "\n",
    "def analyze_quantification_patterns(all_results):\n",
    "    \"\"\"\n",
    "    Analyze patterns in quantification classification results.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"QUANTIFICATION PATTERN ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Analyze by quantification type\n",
    "    quantification_type_stats = Counter()\n",
    "    connection_method_stats = Counter()\n",
    "    \n",
    "    for doc_name, results in all_results.items():\n",
    "        for term in results['valid_terms']:\n",
    "            for quant_info in term.get('connected_quantifications', []):\n",
    "                quant_type = quant_info['quantification']['type']\n",
    "                connection_method = quant_info['connection']['primary_method']\n",
    "                quantification_type_stats[quant_type] += 1\n",
    "                connection_method_stats[connection_method] += 1\n",
    "    \n",
    "    print(f\"\\nQuantification types found:\")\n",
    "    for quant_type, count in quantification_type_stats.most_common():\n",
    "        print(f\"  {quant_type}: {count}\")\n",
    "    \n",
    "    print(f\"\\nConnection methods used:\")\n",
    "    for method, count in connection_method_stats.most_common():\n",
    "        print(f\"  {method}: {count}\")\n",
    "    \n",
    "    return quantification_type_stats, connection_method_stats\n",
    "\n",
    "def print_quantification_examples_by_document(all_results, documents, examples_per_level=3):\n",
    "    \"\"\"\n",
    "    Print examples of quantification classifications for each document.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"QUANTIFICATION EXAMPLES BY DOCUMENT\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for doc_name, results in all_results.items():\n",
    "        doc = documents[doc_name]\n",
    "        stats = results['quantification_stats']\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"DOCUMENT: {doc_name} (Intensity Score: {stats['quantification_intensity_score']:.2f}):\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total terms: {stats['total_terms']}\")\n",
    "        print(f\"  Highly Quantified: {stats['highly_quantified']} ({stats['highly_quantified']/stats['total_terms']*100:.1f}%)\")\n",
    "        print(f\"  Partially Quantified: {stats['partially_quantified']} ({stats['partially_quantified']/stats['total_terms']*100:.1f}%)\")\n",
    "        print(f\"  Non Quantified: {stats['non_quantified']} ({stats['non_quantified']/stats['total_terms']*100:.1f}%)\")\n",
    "        \n",
    "        # Group terms by quantification level\n",
    "        terms_by_level = {}\n",
    "        for term in results['valid_terms']:\n",
    "            level = term.get('quantification_level', 'unknown')\n",
    "            if level not in terms_by_level:\n",
    "                terms_by_level[level] = []\n",
    "            terms_by_level[level].append(term)\n",
    "        \n",
    "        # Print examples for each level\n",
    "        for level in ['highly_quantified', 'partially_quantified', 'non_quantified']:\n",
    "            if level in terms_by_level and terms_by_level[level]:\n",
    "                level_display = level.replace('_', ' ').upper()\n",
    "                print(f\"\\n{level_display} EXAMPLES:\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                # Sort by confidence score (descending) and show top examples\n",
    "                sorted_terms = sorted(terms_by_level[level], \n",
    "                                    key=lambda x: x.get('quantification_confidence', 0), \n",
    "                                    reverse=True)\n",
    "                \n",
    "                for i, term in enumerate(sorted_terms[:examples_per_level]):\n",
    "                    # MODIFIED: Get 10-token context instead of entire sentence\n",
    "                    green_start = term['start_idx']\n",
    "                    green_end = term['end_idx']\n",
    "                    sentence = term['sentence']\n",
    "                    \n",
    "                    # Get sentence boundaries\n",
    "                    sent_start = sentence.start\n",
    "                    sent_end = sentence.end - 1  # spaCy sentence.end is exclusive\n",
    "                    \n",
    "                    # Calculate context boundaries (10 tokens before and after, within sentence limits)\n",
    "                    context_start = max(sent_start, green_start - 10)\n",
    "                    context_end = min(sent_end, green_end + 10)\n",
    "                    \n",
    "                    # Build context with highlighting\n",
    "                    context_tokens_list = []\n",
    "                    for token_idx in range(context_start, context_end + 1):\n",
    "                        token = doc[token_idx]\n",
    "                        if green_start <= token_idx <= green_end:\n",
    "                            # Highlight the green term\n",
    "                            context_tokens_list.append(f\"**{token.text}**\")\n",
    "                        else:\n",
    "                            context_tokens_list.append(token.text)\n",
    "                    \n",
    "                    highlighted_context = ' '.join(context_tokens_list)\n",
    "                    \n",
    "                    print(f\"{i+1}. '{term['term']}' ({term['pos']})\")\n",
    "                    print(f\"   Context: {highlighted_context}\")\n",
    "\n",
    "                    # Show confidence score\n",
    "                    confidence = term.get('quantification_confidence', 0)\n",
    "                    print(f\"   Confidence: {confidence:.3f}\")\n",
    "\n",
    "                    # Show connected quantifications with strength scores\n",
    "                    connected_quants = term.get('connected_quantifications', [])\n",
    "                    if connected_quants:\n",
    "                        quant_texts = []\n",
    "                        for quant_info in connected_quants:\n",
    "                            quant = quant_info['quantification']\n",
    "                            connection = quant_info['connection']\n",
    "                            strength = connection['connection_strength']\n",
    "                            quant_texts.append(f\"'{quant['text']}' ({quant['type']}, {connection['primary_method']}, strength={strength:.2f})\")\n",
    "                        print(f\"   Quantifications: {', '.join(quant_texts)}\")\n",
    "                        \n",
    "                        # Show score breakdown\n",
    "                        scores = term.get('quantification_score', {})\n",
    "                        if scores:\n",
    "                            print(f\"   Scores: Highly={scores.get('highly_score', 0):.2f}, Partially={scores.get('partially_score', 0):.2f}\")\n",
    "                    else:\n",
    "                        print(f\"   Quantifications: None\")\n",
    "                    print()\n",
    "\n",
    "def print_overall_quantification_summary(all_results):\n",
    "    \"\"\"\n",
    "    Print overall summary statistics for quantification classification.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"OVERALL QUANTIFICATION SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Aggregate statistics across all documents\n",
    "    overall_stats = Counter()\n",
    "    confidence_stats = []\n",
    "    \n",
    "    for doc_name, results in all_results.items():\n",
    "        for term in results['valid_terms']:\n",
    "            level = term.get('quantification_level', 'unknown')\n",
    "            overall_stats[level] += 1\n",
    "            \n",
    "            if 'quantification_confidence' in term:\n",
    "                confidence_stats.append(term['quantification_confidence'])\n",
    "    \n",
    "    # Print level statistics\n",
    "    total_terms = sum(overall_stats.values())\n",
    "    print(f\"\\nOverall Classification Results:\")\n",
    "    print(f\"  Total terms analyzed: {total_terms}\")\n",
    "    \n",
    "    for level in ['highly_quantified', 'partially_quantified', 'non_quantified']:\n",
    "        count = overall_stats[level]\n",
    "        percentage = (count / total_terms * 100) if total_terms > 0 else 0\n",
    "        level_display = level.replace('_', ' ').title()\n",
    "        print(f\"  {level_display}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Calculate quantified vs non-quantified\n",
    "    quantified_count = overall_stats['highly_quantified'] + overall_stats['partially_quantified']\n",
    "    quantified_percentage = (quantified_count / total_terms * 100) if total_terms > 0 else 0\n",
    "    print(f\"  Total quantified (highly + partially): {quantified_count} ({quantified_percentage:.1f}%)\")\n",
    "    \n",
    "    # Print POS distribution for quantified terms\n",
    "    pos_stats = defaultdict(Counter)\n",
    "    for doc_name, results in all_results.items():\n",
    "        for term in results['valid_terms']:\n",
    "            level = term.get('quantification_level', 'unknown')\n",
    "            pos = term.get('pos', 'unknown')\n",
    "            pos_stats[pos][level] += 1\n",
    "    \n",
    "    print(f\"\\nQuantification by POS tag:\")\n",
    "    print(f\"{'POS':<30} {'Highly':<8} {'Partial':<8} {'None':<8} {'Total':<8}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for pos_type in sorted(pos_stats.keys()):\n",
    "        level_counts = pos_stats[pos_type]\n",
    "        highly = level_counts['highly_quantified']\n",
    "        partial = level_counts['partially_quantified']\n",
    "        none = level_counts['non_quantified']\n",
    "        total = sum(level_counts.values())\n",
    "        print(f\"{pos_type:<30} {highly:<8} {partial:<8} {none:<8} {total:<8}\")\n",
    "\n",
    "    # Add confidence statistics (NEW SECTION)\n",
    "    print(f\"\\nConfidence Score Statistics:\")\n",
    "    confidence_stats = []\n",
    "    confidence_by_level = {'highly_quantified': [], 'partially_quantified': [], 'non_quantified': []}\n",
    "\n",
    "    for doc_name, results in all_results.items():\n",
    "        for term in results['valid_terms']:\n",
    "            if 'quantification_confidence' in term:\n",
    "                confidence = term['quantification_confidence']\n",
    "                level = term.get('quantification_level', 'unknown')\n",
    "                confidence_stats.append(confidence)\n",
    "                if level in confidence_by_level:\n",
    "                    confidence_by_level[level].append(confidence)\n",
    "\n",
    "    if confidence_stats:\n",
    "        avg_confidence = sum(confidence_stats) / len(confidence_stats)\n",
    "        high_confidence = sum(1 for c in confidence_stats if c >= 0.7)\n",
    "        medium_confidence = sum(1 for c in confidence_stats if 0.3 <= c < 0.7)\n",
    "        low_confidence = sum(1 for c in confidence_stats if c < 0.3)\n",
    "        \n",
    "        print(f\"  Overall average confidence: {avg_confidence:.3f}\")\n",
    "        print(f\"  High confidence (≥0.7): {high_confidence} ({high_confidence/len(confidence_stats)*100:.1f}%)\")\n",
    "        print(f\"  Medium confidence (0.3-0.7): {medium_confidence} ({medium_confidence/len(confidence_stats)*100:.1f}%)\")\n",
    "        print(f\"  Low confidence (<0.3): {low_confidence} ({low_confidence/len(confidence_stats)*100:.1f}%)\")\n",
    "        \n",
    "        # Confidence by quantification level\n",
    "        print(f\"\\nAverage confidence by quantification level:\")\n",
    "        for level, confidences in confidence_by_level.items():\n",
    "            if confidences:\n",
    "                avg = sum(confidences) / len(confidences)\n",
    "                print(f\"  {level.replace('_', ' ').title()}: {avg:.3f} (n={len(confidences)})\")\n",
    "    else:\n",
    "        print(\"  No confidence data available\")\n",
    "    \n",
    "    # Add intensity score statistics (NEW SECTION)\n",
    "    print(f\"\\nQuantification Intensity Scores by Document:\")\n",
    "    intensity_scores = []\n",
    "    print(f\"{'Document':<35} {'Intensity Score':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for doc_name, results in all_results.items():\n",
    "        intensity = results.get('quantification_intensity_score', 0)\n",
    "        intensity_scores.append(intensity)\n",
    "        print(f\"{doc_name:<35} {intensity:<15.2f}\")\n",
    "\n",
    "    if intensity_scores:\n",
    "        avg_intensity = sum(intensity_scores) / len(intensity_scores)\n",
    "        max_intensity = max(intensity_scores)\n",
    "        min_intensity = min(intensity_scores)\n",
    "        \n",
    "        print(f\"\\nIntensity Score Statistics:\")\n",
    "        print(f\"  Average intensity: {avg_intensity:.2f}\")\n",
    "        print(f\"  Maximum intensity: {max_intensity:.2f}\")\n",
    "        print(f\"  Minimum intensity: {min_intensity:.2f}\")\n",
    "        \n",
    "        # Intensity distribution\n",
    "        high_intensity = sum(1 for score in intensity_scores if score >= 100)\n",
    "        medium_intensity = sum(1 for score in intensity_scores if 50 <= score < 100)\n",
    "        low_intensity = sum(1 for score in intensity_scores if score < 50)\n",
    "        \n",
    "        print(f\"  High intensity (≥100): {high_intensity} documents\")\n",
    "        print(f\"  Medium intensity (50-100): {medium_intensity} documents\") \n",
    "        print(f\"  Low intensity (<50): {low_intensity} documents\")\n",
    "\n",
    "# Run analysis  \n",
    "quant_type_stats, connection_stats = analyze_quantification_patterns(all_results)\n",
    "print_quantification_examples_by_document(all_results, documents, examples_per_level=3)\n",
    "print_overall_quantification_summary(all_results)\n",
    "\n",
    "# Print final summary (OPTIONAL ADDITION)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"QUANTIFICATION CLASSIFICATION WITH CONFIDENCE & INTENSITY COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"All valid green terms now have confidence scores based on connection strength\")\n",
    "print(\"Enhanced classification considers both quantification type AND connection quality\")\n",
    "print(\"Intensity score: ((highly_count * 1.5) + (partially_count * 1)) / total_terms * 100\")\n",
    "print(\"Use 'quantification_confidence' field to filter high-quality quantifications\")\n",
    "print(\"Use 'quantification_intensity_score' field to compare document quantification density\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62e189",
   "metadata": {},
   "source": [
    "## Evidence/Claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6172d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Evidence-based markers (concrete, verifiable)\n",
    "EVIDENCE_MARKERS = {\n",
    "    'strong_evidence': {\n",
    "        'completed_actions': [\n",
    "            'achieved', 'delivered', 'implemented', 'installed', 'completed', 'finished',\n",
    "            'accomplished', 'executed', 'established', 'built', 'constructed', 'launched',\n",
    "            'deployed', 'commissioned', 'operationalized', 'finalized'\n",
    "        ],\n",
    "        'verified_outcomes': [\n",
    "            'certified', 'verified', 'audited', 'measured', 'reported', 'documented',\n",
    "            'validated', 'confirmed', 'accredited', 'assessed', 'evaluated', 'monitored',\n",
    "            'tracked', 'recorded', 'registered', 'approved'\n",
    "        ],\n",
    "        'measured_performance': [\n",
    "            'resulted in', 'demonstrated', 'showed', 'recorded', 'yielded', 'generated',\n",
    "            'produced', 'delivered', 'exceeded', 'surpassed', 'outperformed'\n",
    "        ]\n",
    "    },\n",
    "    'moderate_evidence': {\n",
    "        'implemented_actions': [\n",
    "            'introduced', 'adopted', 'initiated', 'started', 'begun', 'commenced',\n",
    "            'undertaken', 'proceeded', 'engaged', 'embarked', 'activated', 'enabled',\n",
    "            'facilitated', 'developed', 'created', 'established'\n",
    "        ],\n",
    "        'present_reality': [\n",
    "            'operates', 'generates', 'produces', 'supplies', 'provides', 'delivers',\n",
    "            'maintains', 'runs', 'functions', 'performs', 'serves', 'supports',\n",
    "            'contributes', 'consists', 'comprises', 'includes', 'contains'\n",
    "        ],\n",
    "        'ongoing_actions': [\n",
    "            'continuing', 'ongoing', 'in progress', 'underway', 'proceeding',\n",
    "            'advancing', 'progressing', 'working on', 'currently', 'actively'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Aspirational markers (future-oriented, commitments)\n",
    "ASPIRATIONAL_MARKERS = {\n",
    "    'strong_aspirational': {\n",
    "        'modal_uncertainty': [\n",
    "            'should', 'could', 'might', 'would', 'may', 'can', 'ought to',\n",
    "            'supposed to', 'expected to', 'likely to', 'probable', 'possible'\n",
    "        ],\n",
    "        'conditional_promises': [\n",
    "            'if', 'when', 'subject to', 'pending', 'contingent on', 'dependent on',\n",
    "            'provided that', 'assuming', 'unless', 'in case', 'should', 'were to'\n",
    "        ],\n",
    "        'visionary_language': [\n",
    "            'vision', 'dream', 'aspire', 'envision', 'imagine', 'hope',\n",
    "            'wish', 'desire', 'ambition', 'ultimate goal', 'long-term vision'\n",
    "        ]\n",
    "    },\n",
    "    'moderate_aspirational': {\n",
    "        'future_commitments': [\n",
    "            'will', 'plan to', 'aim to', 'intend to', 'commit to', 'pledge to',\n",
    "            'promise to', 'undertake to', 'agree to', 'decide to', 'choose to',\n",
    "            'resolve to', 'determine to', 'set out to', 'going to'\n",
    "        ],\n",
    "        'strategic_intentions': [\n",
    "            'strategy', 'roadmap', 'pathway', 'plan', 'target', 'goal', 'objective',\n",
    "            'initiative', 'program', 'project', 'scheme', 'approach', 'framework',\n",
    "            'agenda', 'blueprint', 'timeline', 'schedule'\n",
    "        ],\n",
    "        'preparatory_actions': [\n",
    "            'preparing', 'planning', 'designing', 'developing', 'working towards',\n",
    "            'moving towards', 'progressing towards', 'aiming for', 'targeting',\n",
    "            'seeking', 'pursuing', 'striving', 'endeavoring'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Words that indicate temporal context but not evidence/aspirational nature\n",
    "TEMPORAL_NEUTRALS = [\n",
    "    'in 2021', 'in 2022', 'by 2030', 'since 2020', 'during', 'throughout',\n",
    "    'over the period', 'in the past', 'historically', 'previously',\n",
    "    'currently', 'now', 'today', 'recently', 'lately'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea2aa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_green_term_type(green_term):\n",
    "    \"\"\"\n",
    "    Detect whether a green term is direct, context-dependent, or dependency-based.\n",
    "    \n",
    "    Args:\n",
    "        green_term: Green term dictionary\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: (term_type, context_info)\n",
    "    \"\"\"\n",
    "    # Check for dependency patterns first\n",
    "    if green_term['pos'].startswith('dependency_'):\n",
    "        return 'dependency', {\n",
    "            'pattern_name': green_term.get('pattern', ''),\n",
    "            'dependency_relation': green_term.get('dependency', ''),\n",
    "            'pattern_type': green_term['pos']\n",
    "        }\n",
    "    \n",
    "    # Check for context-dependent terms\n",
    "    elif 'context_word' in green_term:\n",
    "        return 'context_dependent', {\n",
    "            'context_word': green_term['context_word'],\n",
    "            'neutral_part': green_term.get('neutral_part', ''),\n",
    "            'context_type': green_term.get('context_type', ''),\n",
    "            'context_relationship': green_term.get('context_relationship', '')\n",
    "        }\n",
    "    \n",
    "    # Default to direct terms\n",
    "    else:\n",
    "        return 'direct', {}\n",
    "\n",
    "def find_semantic_governor_enhanced(green_term, doc):\n",
    "    \"\"\"\n",
    "    Find the semantic governor with special handling for all term types.\n",
    "    \n",
    "    Args:\n",
    "        green_term: Green term dictionary\n",
    "        doc: spaCy doc object\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with governor information\n",
    "    \"\"\"\n",
    "    start_idx = green_term['start_idx']\n",
    "    end_idx = green_term['end_idx']\n",
    "    term_type, context_info = detect_green_term_type(green_term)\n",
    "    \n",
    "    if term_type == 'dependency':\n",
    "        # For dependency patterns, we have two tokens in a dependency relationship\n",
    "        # We need to find the verb that governs this dependency pair\n",
    "        token1 = doc[start_idx]\n",
    "        token2 = doc[end_idx]\n",
    "        \n",
    "        # Find which token is the head and which is the dependent\n",
    "        if token1.head == token2:\n",
    "            head_token = token2\n",
    "            dependent_token = token1\n",
    "        elif token2.head == token1:\n",
    "            head_token = token1\n",
    "            dependent_token = token2\n",
    "        else:\n",
    "            # They don't have direct dependency, use the first token as primary\n",
    "            head_token = token1\n",
    "            dependent_token = token2\n",
    "        \n",
    "        # Find the verb that governs the head token of the dependency pair\n",
    "        current_token = head_token\n",
    "        max_depth = 10\n",
    "        depth = 0\n",
    "        \n",
    "        while current_token.head != current_token and depth < max_depth:\n",
    "            if current_token.head.pos_ == 'VERB' or current_token.head.pos_ == 'AUX':\n",
    "                return {\n",
    "                    'semantic_governor': current_token.head,\n",
    "                    'primary_token': head_token,\n",
    "                    'secondary_token': dependent_token,\n",
    "                    'dependency_relation': dependent_token.dep_\n",
    "                }\n",
    "            current_token = current_token.head\n",
    "            depth += 1\n",
    "        \n",
    "        # Fallback to sentence root\n",
    "        sentence_root = head_token.sent.root\n",
    "        if sentence_root.pos_ == 'VERB' or sentence_root.pos_ == 'AUX':\n",
    "            return {\n",
    "                'semantic_governor': sentence_root,\n",
    "                'primary_token': head_token,\n",
    "                'secondary_token': dependent_token,\n",
    "                'dependency_relation': dependent_token.dep_\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'semantic_governor': None,\n",
    "            'primary_token': head_token,\n",
    "            'secondary_token': dependent_token,\n",
    "            'dependency_relation': dependent_token.dep_\n",
    "        }\n",
    "    \n",
    "    elif term_type == 'context_dependent':\n",
    "        # For context-dependent terms, find the verb that governs the complete phrase\n",
    "        context_word = context_info['context_word']\n",
    "        sentence = doc[start_idx].sent\n",
    "        context_token = None\n",
    "        \n",
    "        for token in sentence:\n",
    "            if token.text.lower() == context_word.lower():\n",
    "                if abs(token.i - start_idx) <= 3:\n",
    "                    context_token = token\n",
    "                    break\n",
    "        \n",
    "        if context_token:\n",
    "            current_token = context_token\n",
    "            max_depth = 10\n",
    "            depth = 0\n",
    "            \n",
    "            while current_token.head != current_token and depth < max_depth:\n",
    "                if current_token.head.pos_ == 'VERB' or current_token.head.pos_ == 'AUX':\n",
    "                    return {\n",
    "                        'semantic_governor': current_token.head,\n",
    "                        'context_token': context_token,\n",
    "                        'neutral_tokens': [doc[i] for i in range(start_idx, end_idx + 1)]\n",
    "                    }\n",
    "                current_token = current_token.head\n",
    "                depth += 1\n",
    "    \n",
    "    # For direct terms or fallback\n",
    "    main_token = doc[start_idx]\n",
    "    current_token = main_token\n",
    "    max_depth = 10\n",
    "    depth = 0\n",
    "    \n",
    "    while current_token.head != current_token and depth < max_depth:\n",
    "        if current_token.head.pos_ == 'VERB' or current_token.head.pos_ == 'AUX':\n",
    "            return {\n",
    "                'semantic_governor': current_token.head,\n",
    "                'main_tokens': [doc[i] for i in range(start_idx, end_idx + 1)]\n",
    "            }\n",
    "        current_token = current_token.head\n",
    "        depth += 1\n",
    "    \n",
    "    # Strategy 2: Look for verbs in the same sentence that govern this term\n",
    "    sentence = main_token.sent\n",
    "    for token in sentence:\n",
    "        if token.pos_ == 'VERB' or token.pos_ == 'AUX':\n",
    "            for child in token.subtree:\n",
    "                if start_idx <= child.i <= end_idx:\n",
    "                    return {\n",
    "                        'semantic_governor': token,\n",
    "                        'main_tokens': [doc[i] for i in range(start_idx, end_idx + 1)]\n",
    "                    }\n",
    "    \n",
    "    # Fallback to sentence root\n",
    "    sentence_root = main_token.sent.root\n",
    "    if sentence_root.pos_ == 'VERB' or sentence_root.pos_ == 'AUX':\n",
    "        return {\n",
    "            'semantic_governor': sentence_root,\n",
    "            'main_tokens': [doc[i] for i in range(start_idx, end_idx + 1)]\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'semantic_governor': None,\n",
    "        'main_tokens': [doc[i] for i in range(start_idx, end_idx + 1)]\n",
    "    }\n",
    "\n",
    "def get_analysis_scope_enhanced(green_term, doc):\n",
    "    \"\"\"\n",
    "    Determine the enhanced analysis scope for evidence/aspirational markers.\n",
    "    \n",
    "    Args:\n",
    "        green_term: Green term dictionary\n",
    "        doc: spaCy doc object\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with enhanced analysis scope information\n",
    "    \"\"\"\n",
    "    term_type, context_info = detect_green_term_type(green_term)\n",
    "    governor_info = find_semantic_governor_enhanced(green_term, doc)\n",
    "    \n",
    "    # Get excluded words (green-making words that shouldn't be considered as evidence/aspirational markers)\n",
    "    excluded_words = []\n",
    "    \n",
    "    if term_type == 'context_dependent':\n",
    "        excluded_words.append(context_info['context_word'].lower())\n",
    "    elif term_type == 'dependency':\n",
    "        # For dependency patterns, both tokens are part of the green meaning\n",
    "        # so we don't exclude them unless they appear in our marker lists\n",
    "        pass\n",
    "    \n",
    "    return {\n",
    "        'term_type': term_type,\n",
    "        'context_info': context_info,\n",
    "        'governor_info': governor_info,\n",
    "        'excluded_words': excluded_words,\n",
    "        'green_term_positions': set(range(green_term['start_idx'], green_term['end_idx'] + 1))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2c6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_targeted_evidence_markers(analysis_scope, doc):\n",
    "    \"\"\"\n",
    "    Find evidence markers using targeted dependency-first approach.\n",
    "    \n",
    "    Args:\n",
    "        analysis_scope: Enhanced analysis scope dictionary\n",
    "        doc: spaCy doc object\n",
    "    \n",
    "    Returns:\n",
    "        List of evidence markers found\n",
    "    \"\"\"\n",
    "    evidence_markers_found = []\n",
    "    governor_info = analysis_scope['governor_info']\n",
    "    excluded_words = analysis_scope['excluded_words']\n",
    "    green_positions = analysis_scope['green_term_positions']\n",
    "    term_type = analysis_scope['term_type']\n",
    "    \n",
    "    # Combine all evidence markers for searching\n",
    "    all_evidence_markers = []\n",
    "    for strength_level, categories in EVIDENCE_MARKERS.items():\n",
    "        for category, markers in categories.items():\n",
    "            for marker in markers:\n",
    "                all_evidence_markers.append((marker, strength_level, category))\n",
    "    \n",
    "    # Get analysis targets based on term type\n",
    "    analysis_targets = get_analysis_targets(analysis_scope)\n",
    "    \n",
    "    # Search for markers in targeted locations\n",
    "    for target_info in analysis_targets:\n",
    "        target_token = target_info['token']\n",
    "        search_scope = target_info['scope']\n",
    "        \n",
    "        markers_in_scope = find_markers_in_scope(\n",
    "            target_token, search_scope, all_evidence_markers, excluded_words, doc\n",
    "        )\n",
    "        \n",
    "        for marker_info in markers_in_scope:\n",
    "            # Assess connection strength\n",
    "            connection_strength, connection_type = assess_targeted_marker_connection(\n",
    "                marker_info, target_info, green_positions, governor_info, doc\n",
    "            )\n",
    "            \n",
    "            if connection_strength > 0.3:\n",
    "                evidence_markers_found.append({\n",
    "                    'text': marker_info['text'],\n",
    "                    'strength_level': marker_info['strength_level'],\n",
    "                    'category': marker_info['category'],\n",
    "                    'token_position': marker_info['token_position'],\n",
    "                    'connection_strength': connection_strength,\n",
    "                    'connection_type': connection_type,\n",
    "                    'search_scope': search_scope\n",
    "                })\n",
    "    \n",
    "    return evidence_markers_found\n",
    "\n",
    "def find_targeted_aspirational_markers(analysis_scope, doc):\n",
    "    \"\"\"\n",
    "    Find aspirational markers using targeted dependency-first approach.\n",
    "    \n",
    "    Args:\n",
    "        analysis_scope: Enhanced analysis scope dictionary\n",
    "        doc: spaCy doc object\n",
    "    \n",
    "    Returns:\n",
    "        List of aspirational markers found\n",
    "    \"\"\"\n",
    "    aspirational_markers_found = []\n",
    "    governor_info = analysis_scope['governor_info']\n",
    "    excluded_words = analysis_scope['excluded_words']\n",
    "    green_positions = analysis_scope['green_term_positions']\n",
    "    term_type = analysis_scope['term_type']\n",
    "    \n",
    "    # Combine all aspirational markers for searching\n",
    "    all_aspirational_markers = []\n",
    "    for strength_level, categories in ASPIRATIONAL_MARKERS.items():\n",
    "        for category, markers in categories.items():\n",
    "            for marker in markers:\n",
    "                all_aspirational_markers.append((marker, strength_level, category))\n",
    "    \n",
    "    # Get analysis targets based on term type\n",
    "    analysis_targets = get_analysis_targets(analysis_scope)\n",
    "    \n",
    "    # Search for markers in targeted locations\n",
    "    for target_info in analysis_targets:\n",
    "        target_token = target_info['token']\n",
    "        search_scope = target_info['scope']\n",
    "        \n",
    "        markers_in_scope = find_markers_in_scope(\n",
    "            target_token, search_scope, all_aspirational_markers, excluded_words, doc\n",
    "        )\n",
    "        \n",
    "        for marker_info in markers_in_scope:\n",
    "            # Assess connection strength\n",
    "            connection_strength, connection_type = assess_targeted_marker_connection(\n",
    "                marker_info, target_info, green_positions, governor_info, doc\n",
    "            )\n",
    "            \n",
    "            if connection_strength > 0.3:\n",
    "                aspirational_markers_found.append({\n",
    "                    'text': marker_info['text'],\n",
    "                    'strength_level': marker_info['strength_level'],\n",
    "                    'category': marker_info['category'],\n",
    "                    'token_position': marker_info['token_position'],\n",
    "                    'connection_strength': connection_strength,\n",
    "                    'connection_type': connection_type,\n",
    "                    'search_scope': search_scope\n",
    "                })\n",
    "    \n",
    "    return aspirational_markers_found\n",
    "\n",
    "def get_analysis_targets(analysis_scope):\n",
    "    \"\"\"\n",
    "    Get targeted analysis locations based on term type and dependency structure.\n",
    "    \n",
    "    Args:\n",
    "        analysis_scope: Enhanced analysis scope dictionary\n",
    "    \n",
    "    Returns:\n",
    "        List of analysis targets with scope information\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "    governor_info = analysis_scope['governor_info']\n",
    "    term_type = analysis_scope['term_type']\n",
    "    semantic_governor = governor_info.get('semantic_governor')\n",
    "    \n",
    "    if term_type == 'dependency':\n",
    "        # For dependency patterns, analyze both tokens and their governor\n",
    "        primary_token = governor_info.get('primary_token')\n",
    "        secondary_token = governor_info.get('secondary_token')\n",
    "        \n",
    "        if primary_token:\n",
    "            targets.append({\n",
    "                'token': primary_token,\n",
    "                'scope': 'token_modifiers',\n",
    "                'role': 'primary_dependency_token'\n",
    "            })\n",
    "        \n",
    "        if secondary_token:\n",
    "            targets.append({\n",
    "                'token': secondary_token,\n",
    "                'scope': 'token_modifiers',\n",
    "                'role': 'secondary_dependency_token'\n",
    "            })\n",
    "        \n",
    "        if semantic_governor:\n",
    "            targets.append({\n",
    "                'token': semantic_governor,\n",
    "                'scope': 'governor_modifiers',\n",
    "                'role': 'semantic_governor'\n",
    "            })\n",
    "    \n",
    "    elif term_type == 'context_dependent':\n",
    "        # For context-dependent terms, analyze the complete phrase and governor\n",
    "        context_token = governor_info.get('context_token')\n",
    "        neutral_tokens = governor_info.get('neutral_tokens', [])\n",
    "        \n",
    "        if context_token:\n",
    "            targets.append({\n",
    "                'token': context_token,\n",
    "                'scope': 'token_modifiers',\n",
    "                'role': 'context_token'\n",
    "            })\n",
    "        \n",
    "        for token in neutral_tokens:\n",
    "            targets.append({\n",
    "                'token': token,\n",
    "                'scope': 'token_modifiers',\n",
    "                'role': 'neutral_token'\n",
    "            })\n",
    "        \n",
    "        if semantic_governor:\n",
    "            targets.append({\n",
    "                'token': semantic_governor,\n",
    "                'scope': 'governor_modifiers',\n",
    "                'role': 'semantic_governor'\n",
    "            })\n",
    "    \n",
    "    else:  # direct terms\n",
    "        # For direct terms, analyze the main tokens and governor\n",
    "        main_tokens = governor_info.get('main_tokens', [])\n",
    "        \n",
    "        for token in main_tokens:\n",
    "            targets.append({\n",
    "                'token': token,\n",
    "                'scope': 'token_modifiers',\n",
    "                'role': 'main_token'\n",
    "            })\n",
    "        \n",
    "        if semantic_governor:\n",
    "            targets.append({\n",
    "                'token': semantic_governor,\n",
    "                'scope': 'governor_modifiers',\n",
    "                'role': 'semantic_governor'\n",
    "            })\n",
    "    \n",
    "    return targets\n",
    "\n",
    "def find_markers_in_scope(target_token, search_scope, marker_list, excluded_words, doc):\n",
    "    \"\"\"\n",
    "    Find markers in the specified scope around a target token.\n",
    "    \n",
    "    Args:\n",
    "        target_token: The token to search around\n",
    "        search_scope: Type of scope ('token_modifiers', 'governor_modifiers', etc.)\n",
    "        marker_list: List of (marker, strength_level, category) tuples\n",
    "        excluded_words: Words to exclude from search\n",
    "        doc: spaCy doc object\n",
    "    \n",
    "    Returns:\n",
    "        List of marker information dictionaries\n",
    "    \"\"\"\n",
    "    markers_found = []\n",
    "    search_tokens = []\n",
    "    \n",
    "    if search_scope == 'token_modifiers':\n",
    "        # Look at tokens that modify this token (its children)\n",
    "        search_tokens.extend(target_token.children)\n",
    "        # Look at tokens this token modifies (siblings under same head)\n",
    "        if target_token.head != target_token:\n",
    "            search_tokens.extend([child for child in target_token.head.children if child != target_token])\n",
    "    \n",
    "    elif search_scope == 'governor_modifiers':\n",
    "        # Look at tokens that modify the governor\n",
    "        search_tokens.extend(target_token.children)\n",
    "        # Look at auxiliary verbs and modal verbs connected to governor\n",
    "        for child in target_token.children:\n",
    "            if child.pos_ in ['AUX', 'VERB'] or child.dep_ in ['aux', 'auxpass']:\n",
    "                search_tokens.append(child)\n",
    "    \n",
    "    # Add the target token itself to search\n",
    "    search_tokens.append(target_token)\n",
    "    \n",
    "    # Also check immediate neighbors (within 2 tokens)\n",
    "    for offset in [-2, -1, 1, 2]:\n",
    "        neighbor_idx = target_token.i + offset\n",
    "        if 0 <= neighbor_idx < len(doc):\n",
    "            neighbor_token = doc[neighbor_idx]\n",
    "            # Only add if it's in the same sentence\n",
    "            if neighbor_token.sent == target_token.sent:\n",
    "                search_tokens.append(neighbor_token)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    search_tokens = list(set(search_tokens))\n",
    "    \n",
    "    # Search for markers in these specific tokens\n",
    "    for marker, strength_level, category in marker_list:\n",
    "        marker_words = marker.lower().split()\n",
    "        \n",
    "        # Skip excluded words\n",
    "        if marker.lower() in excluded_words:\n",
    "            continue\n",
    "        \n",
    "        # Look for single-word markers\n",
    "        if len(marker_words) == 1:\n",
    "            for token in search_tokens:\n",
    "                if token.text.lower() == marker_words[0] or token.lemma_.lower() == marker_words[0]:\n",
    "                    markers_found.append({\n",
    "                        'text': marker,\n",
    "                        'strength_level': strength_level,\n",
    "                        'category': category,\n",
    "                        'token_position': token.i,\n",
    "                        'matched_token': token\n",
    "                    })\n",
    "        \n",
    "        # Look for multi-word markers in consecutive tokens\n",
    "        else:\n",
    "            for i, token in enumerate(search_tokens):\n",
    "                if token.text.lower() == marker_words[0]:\n",
    "                    # Check if subsequent tokens match\n",
    "                    match = True\n",
    "                    consecutive_tokens = [token]\n",
    "                    \n",
    "                    for j, word in enumerate(marker_words[1:], 1):\n",
    "                        next_token_idx = token.i + j\n",
    "                        if next_token_idx < len(doc):\n",
    "                            next_token = doc[next_token_idx]\n",
    "                            if next_token.text.lower() == word:\n",
    "                                consecutive_tokens.append(next_token)\n",
    "                            else:\n",
    "                                match = False\n",
    "                                break\n",
    "                        else:\n",
    "                            match = False\n",
    "                            break\n",
    "                    \n",
    "                    if match:\n",
    "                        markers_found.append({\n",
    "                            'text': marker,\n",
    "                            'strength_level': strength_level,\n",
    "                            'category': category,\n",
    "                            'token_position': token.i,\n",
    "                            'matched_token': token,\n",
    "                            'consecutive_tokens': consecutive_tokens\n",
    "                        })\n",
    "    \n",
    "    return markers_found\n",
    "\n",
    "def assess_targeted_marker_connection(marker_info, target_info, green_positions, governor_info, doc):\n",
    "    \"\"\"\n",
    "    Assess connection strength for targeted marker detection.\n",
    "    \n",
    "    Args:\n",
    "        marker_info: Dictionary with marker information\n",
    "        target_info: Dictionary with target token information\n",
    "        green_positions: Set of green term token positions\n",
    "        governor_info: Governor information dictionary\n",
    "        doc: spaCy doc object\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: (connection_strength, connection_type)\n",
    "    \"\"\"\n",
    "    marker_token = marker_info['matched_token']\n",
    "    target_token = target_info['token']\n",
    "    target_role = target_info['role']\n",
    "    \n",
    "    # Direct modification of target token\n",
    "    if marker_token.head == target_token or target_token.head == marker_token:\n",
    "        return 0.9, f'direct_modification_{target_role}'\n",
    "    \n",
    "    # Marker is child of target token\n",
    "    if marker_token in target_token.children:\n",
    "        return 0.8, f'child_of_{target_role}'\n",
    "    \n",
    "    # Marker is sibling of target token (same head)\n",
    "    if (marker_token.head == target_token.head and \n",
    "        marker_token.head != marker_token and \n",
    "        target_token.head != target_token):\n",
    "        return 0.7, f'sibling_of_{target_role}'\n",
    "    \n",
    "    # Close proximity to target token\n",
    "    distance = abs(marker_token.i - target_token.i)\n",
    "    if distance <= 2:\n",
    "        return 0.6, f'proximity_{distance}_{target_role}'\n",
    "    elif distance <= 4:\n",
    "        return 0.4, f'proximity_{distance}_{target_role}'\n",
    "    \n",
    "    # Same sentence but more distant\n",
    "    if marker_token.sent == target_token.sent:\n",
    "        return 0.2, f'same_sentence_{target_role}'\n",
    "    \n",
    "    return 0.0, 'no_connection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf815db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_evidence_aspirational(evidence_markers, aspirational_markers):\n",
    "    \"\"\"\n",
    "    Classify the evidence vs aspirational nature based on found markers.\n",
    "    \n",
    "    Args:\n",
    "        evidence_markers: List of evidence markers found\n",
    "        aspirational_markers: List of aspirational markers found\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with classification results\n",
    "    \"\"\"\n",
    "    # Calculate evidence scores\n",
    "    evidence_score = 0\n",
    "    evidence_details = []\n",
    "    \n",
    "    for marker in evidence_markers:\n",
    "        weight = marker['connection_strength']\n",
    "        if marker['strength_level'] == 'strong_evidence':\n",
    "            evidence_score += 2.0 * weight\n",
    "        elif marker['strength_level'] == 'moderate_evidence':\n",
    "            evidence_score += 1.0 * weight\n",
    "        \n",
    "        evidence_details.append({\n",
    "            'text': marker['text'],\n",
    "            'strength': marker['strength_level'],\n",
    "            'category': marker['category'],\n",
    "            'weight': weight,\n",
    "            'connection_type': marker.get('connection_type', 'unknown'),\n",
    "            'search_scope': marker.get('search_scope', 'unknown')\n",
    "        })\n",
    "    \n",
    "    # Calculate aspirational scores\n",
    "    aspirational_score = 0\n",
    "    aspirational_details = []\n",
    "    \n",
    "    for marker in aspirational_markers:\n",
    "        weight = marker['connection_strength']\n",
    "        if marker['strength_level'] == 'strong_aspirational':\n",
    "            aspirational_score += 2.0 * weight\n",
    "        elif marker['strength_level'] == 'moderate_aspirational':\n",
    "            aspirational_score += 1.0 * weight\n",
    "        \n",
    "        aspirational_details.append({\n",
    "            'text': marker['text'],\n",
    "            'strength': marker['strength_level'],\n",
    "            'category': marker['category'],\n",
    "            'weight': weight,\n",
    "            'connection_type': marker.get('connection_type', 'unknown'),\n",
    "            'search_scope': marker.get('search_scope', 'unknown')\n",
    "        })\n",
    "    \n",
    "    # Determine classification\n",
    "    classification = determine_final_classification(evidence_score, aspirational_score)\n",
    "    \n",
    "    # Calculate confidence score\n",
    "    total_score = evidence_score + aspirational_score\n",
    "    if total_score > 0:\n",
    "        confidence = min(total_score / 2.0, 1.0)  # Normalize to 0-1\n",
    "    else:\n",
    "        confidence = 0.0\n",
    "    \n",
    "    return {\n",
    "        'classification': classification,\n",
    "        'confidence': confidence,\n",
    "        'evidence_score': evidence_score,\n",
    "        'aspirational_score': aspirational_score,\n",
    "        'evidence_markers': evidence_details,\n",
    "        'aspirational_markers': aspirational_details,\n",
    "        'total_markers': len(evidence_markers) + len(aspirational_markers)\n",
    "    }\n",
    "\n",
    "def determine_final_classification(evidence_score, aspirational_score):\n",
    "    \"\"\"\n",
    "    Determine the final classification based on evidence and aspirational scores.\n",
    "    \n",
    "    Args:\n",
    "        evidence_score: Weighted evidence score\n",
    "        aspirational_score: Weighted aspirational score\n",
    "    \n",
    "    Returns:\n",
    "        String: Classification category\n",
    "    \"\"\"\n",
    "    # Thresholds for classification\n",
    "    strong_threshold = 1.5\n",
    "    moderate_threshold = 0.5\n",
    "    \n",
    "    if evidence_score >= strong_threshold and aspirational_score < moderate_threshold:\n",
    "        return 'strong_evidence'\n",
    "    elif evidence_score >= moderate_threshold and aspirational_score < evidence_score:\n",
    "        return 'moderate_evidence'\n",
    "    elif aspirational_score >= strong_threshold and evidence_score < moderate_threshold:\n",
    "        return 'strong_aspirational'\n",
    "    elif aspirational_score >= moderate_threshold and evidence_score < aspirational_score:\n",
    "        return 'moderate_aspirational'\n",
    "    elif evidence_score > 0 and aspirational_score > 0:\n",
    "        # Mixed context - classify based on stronger signal\n",
    "        if evidence_score > aspirational_score:\n",
    "            return 'moderate_evidence'\n",
    "        else:\n",
    "            return 'moderate_aspirational'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "def classify_single_green_term_enhanced(green_term, doc):\n",
    "    \"\"\"\n",
    "    Classify a single green term with enhanced analysis for all term types.\n",
    "    \n",
    "    Args:\n",
    "        green_term: Green term dictionary\n",
    "        doc: spaCy doc object\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with enhanced classification results\n",
    "    \"\"\"\n",
    "    # Get enhanced analysis scope\n",
    "    analysis_scope = get_analysis_scope_enhanced(green_term, doc)\n",
    "    \n",
    "    # Find evidence and aspirational markers using targeted approach\n",
    "    evidence_markers = find_targeted_evidence_markers(analysis_scope, doc)\n",
    "    aspirational_markers = find_targeted_aspirational_markers(analysis_scope, doc)\n",
    "    \n",
    "    # Classify based on markers\n",
    "    classification_result = classify_evidence_aspirational(evidence_markers, aspirational_markers)\n",
    "    \n",
    "    # Add enhanced context information\n",
    "    classification_result.update({\n",
    "        'term_type': analysis_scope['term_type'],\n",
    "        'context_info': analysis_scope['context_info'],\n",
    "        'governor_info': {\n",
    "            'semantic_governor': analysis_scope['governor_info'].get('semantic_governor').text if analysis_scope['governor_info'].get('semantic_governor') else None,\n",
    "            'governor_pos': analysis_scope['governor_info'].get('semantic_governor').pos_ if analysis_scope['governor_info'].get('semantic_governor') else None,\n",
    "            'additional_info': {k: v for k, v in analysis_scope['governor_info'].items() \n",
    "                             if k not in ['semantic_governor']}\n",
    "        },\n",
    "        'excluded_words': analysis_scope['excluded_words'],\n",
    "        'analysis_method': 'targeted_dependency_first'\n",
    "    })\n",
    "    \n",
    "    return classification_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b6cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_evidence_aspirational_classification_enhanced(all_results, documents):\n",
    "    \"\"\"\n",
    "    Add enhanced evidence vs aspirational classification to all valid green terms.\n",
    "    \n",
    "    Args:\n",
    "        all_results: Dictionary containing results from green term analysis\n",
    "        documents: Dictionary containing spaCy doc objects\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary: Updated results with enhanced evidence vs aspirational classification\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ENHANCED EVIDENCE VS ASPIRATIONAL CLASSIFICATION ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"Features: Dependency patterns support + Targeted marker detection\")\n",
    "    \n",
    "    # Initialize summary statistics\n",
    "    overall_ea_stats = Counter()\n",
    "    doc_ea_stats = {}\n",
    "    term_type_stats = Counter()\n",
    "    \n",
    "    for doc_name, results in all_results.items():\n",
    "        doc = documents[doc_name]\n",
    "        valid_terms = results['valid_terms']\n",
    "        \n",
    "        # Classify each valid green term with enhanced analysis\n",
    "        doc_ea_counter = Counter()\n",
    "        doc_term_type_counter = Counter()\n",
    "        \n",
    "        for green_term in valid_terms:\n",
    "            # Classify this green term with enhanced method\n",
    "            classification_result = classify_single_green_term_enhanced(green_term, doc)\n",
    "            \n",
    "            # Add enhanced classification to green term\n",
    "            green_term['evidence_aspirational_class'] = classification_result['classification']\n",
    "            green_term['ea_confidence'] = classification_result['confidence']\n",
    "            green_term['ea_evidence_score'] = classification_result['evidence_score']\n",
    "            green_term['ea_aspirational_score'] = classification_result['aspirational_score']\n",
    "            green_term['ea_evidence_markers'] = classification_result['evidence_markers']\n",
    "            green_term['ea_aspirational_markers'] = classification_result['aspirational_markers']\n",
    "            green_term['ea_term_type'] = classification_result['term_type']\n",
    "            green_term['ea_context_info'] = classification_result['context_info']\n",
    "            green_term['ea_governor_info'] = classification_result['governor_info']\n",
    "            green_term['ea_excluded_words'] = classification_result['excluded_words']\n",
    "            green_term['ea_analysis_method'] = classification_result['analysis_method']\n",
    "            \n",
    "            # Update statistics\n",
    "            ea_class = classification_result['classification']\n",
    "            term_type = classification_result['term_type']\n",
    "            \n",
    "            doc_ea_counter[ea_class] += 1\n",
    "            doc_term_type_counter[term_type] += 1\n",
    "            overall_ea_stats[ea_class] += 1\n",
    "            term_type_stats[term_type] += 1\n",
    "        \n",
    "        # Calculate evidence and aspirational intensity scores\n",
    "        total_terms = len(valid_terms)\n",
    "        strong_evidence_count = doc_ea_counter['strong_evidence']\n",
    "        moderate_evidence_count = doc_ea_counter['moderate_evidence']\n",
    "        strong_aspirational_count = doc_ea_counter['strong_aspirational']\n",
    "        moderate_aspirational_count = doc_ea_counter['moderate_aspirational']\n",
    "\n",
    "        if total_terms > 0:\n",
    "            evidence_intensity_score = (((strong_evidence_count * 1.5) + (moderate_evidence_count * 1)) / total_terms) * 100\n",
    "            aspirational_intensity_score = (((strong_aspirational_count * 1.5) + (moderate_aspirational_count * 1)) / total_terms) * 100\n",
    "        else:\n",
    "            evidence_intensity_score = 0.0\n",
    "            aspirational_intensity_score = 0.0\n",
    "\n",
    "        # Store document-level statistics\n",
    "        doc_ea_stats[doc_name] = {\n",
    "            'ea_distribution': doc_ea_counter,\n",
    "            'term_type_distribution': doc_term_type_counter\n",
    "        }\n",
    "        \n",
    "        # Store intensity scores in results\n",
    "        all_results[doc_name]['evidence_intensity_score'] = evidence_intensity_score\n",
    "        all_results[doc_name]['aspirational_intensity_score'] = aspirational_intensity_score\n",
    "    \n",
    "    # Print overall summary\n",
    "    print(f\"\\nEvidence vs Aspirational Classification Results:\")\n",
    "    total_terms = sum(overall_ea_stats.values())\n",
    "    \n",
    "    # Print in logical order\n",
    "    classification_order = ['strong_evidence', 'moderate_evidence', 'neutral', \n",
    "                          'moderate_aspirational', 'strong_aspirational']\n",
    "    \n",
    "    for ea_class in classification_order:\n",
    "        count = overall_ea_stats[ea_class]\n",
    "        percentage = (count / total_terms) * 100 if total_terms > 0 else 0\n",
    "        print(f\"  {ea_class.replace('_', ' ').title()}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Print term type distribution\n",
    "    print(f\"\\nTerm Type Distribution:\")\n",
    "    for term_type, count in term_type_stats.items():\n",
    "        percentage = (count / total_terms) * 100 if total_terms > 0 else 0\n",
    "        print(f\"  {term_type.replace('_', ' ').title()}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Document-level breakdown\n",
    "    print(f\"\\nDocument breakdown:\")\n",
    "    print(f\"{'Document':<25} {'StrEvid':<8} {'ModEvid':<8} {'Neutral':<8} {'ModAsp':<8} {'StrAsp':<8} {'Total':<8}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for doc_name, doc_stats in doc_ea_stats.items():\n",
    "        ea_dist = doc_stats['ea_distribution']\n",
    "        strong_ev = ea_dist['strong_evidence']\n",
    "        mod_ev = ea_dist['moderate_evidence']\n",
    "        neutral = ea_dist['neutral']\n",
    "        mod_asp = ea_dist['moderate_aspirational']\n",
    "        strong_asp = ea_dist['strong_aspirational']\n",
    "        total = sum(ea_dist.values())\n",
    "        \n",
    "        print(f\"{doc_name:<25} {strong_ev:<8} {mod_ev:<8} {neutral:<8} {mod_asp:<8} {strong_asp:<8} {total:<8}\")\n",
    "    \n",
    "    # Store enhanced EA statistics in results\n",
    "    for doc_name in all_results:\n",
    "        all_results[doc_name]['ea_stats_enhanced'] = doc_ea_stats[doc_name]\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa509e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply enhanced evidence vs aspirational classification to all documents\n",
    "all_results = add_evidence_aspirational_classification_enhanced(all_results, documents)\n",
    "\n",
    "def analyze_enhanced_ea_patterns(all_results):\n",
    "    \"\"\"\n",
    "    Analyze patterns in enhanced evidence vs aspirational classification results.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ENHANCED EVIDENCE VS ASPIRATIONAL PATTERN ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Analyze by term type and classification\n",
    "    term_type_ea_stats = defaultdict(Counter)\n",
    "    marker_category_stats = Counter()\n",
    "    confidence_stats = []\n",
    "    connection_type_stats = Counter()\n",
    "    search_scope_stats = Counter()\n",
    "    \n",
    "    for doc_name, results in all_results.items():\n",
    "        for term in results['valid_terms']:\n",
    "            if 'evidence_aspirational_class' in term:\n",
    "                ea_class = term['evidence_aspirational_class']\n",
    "                term_type = term.get('ea_term_type', 'unknown')\n",
    "                confidence = term.get('ea_confidence', 0)\n",
    "                \n",
    "                term_type_ea_stats[term_type][ea_class] += 1\n",
    "                confidence_stats.append(confidence)\n",
    "                \n",
    "                # Count marker categories, connection types, and search scopes\n",
    "                for marker in term.get('ea_evidence_markers', []):\n",
    "                    marker_category_stats[f\"evidence_{marker['category']}\"] += 1\n",
    "                    connection_type_stats[marker.get('connection_type', 'unknown')] += 1\n",
    "                    search_scope_stats[marker.get('search_scope', 'unknown')] += 1\n",
    "                \n",
    "                for marker in term.get('ea_aspirational_markers', []):\n",
    "                    marker_category_stats[f\"aspirational_{marker['category']}\"] += 1\n",
    "                    connection_type_stats[marker.get('connection_type', 'unknown')] += 1\n",
    "                    search_scope_stats[marker.get('search_scope', 'unknown')] += 1\n",
    "    \n",
    "    print(f\"\\nClassification by term type:\")\n",
    "    print(f\"{'Term Type':<20} {'StrEvid':<8} {'ModEvid':<8} {'Neutral':<8} {'ModAsp':<8} {'StrAsp':<8} {'Total':<8}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for term_type, ea_counter in term_type_ea_stats.items():\n",
    "        strong_ev = ea_counter['strong_evidence']\n",
    "        mod_ev = ea_counter['moderate_evidence']\n",
    "        neutral = ea_counter['neutral']\n",
    "        mod_asp = ea_counter['moderate_aspirational']\n",
    "        strong_asp = ea_counter['strong_aspirational']\n",
    "        total = sum(ea_counter.values())\n",
    "        \n",
    "        print(f\"{term_type:<20} {strong_ev:<8} {mod_ev:<8} {neutral:<8} {mod_asp:<8} {strong_asp:<8} {total:<8}\")\n",
    "    \n",
    "    print(f\"\\nTop marker categories used:\")\n",
    "    for category, count in marker_category_stats.most_common(10):\n",
    "        print(f\"  {category}: {count}\")\n",
    "    \n",
    "    print(f\"\\nTop connection types:\")\n",
    "    for conn_type, count in connection_type_stats.most_common(10):\n",
    "        print(f\"  {conn_type}: {count}\")\n",
    "    \n",
    "    print(f\"\\nSearch scope distribution:\")\n",
    "    for scope, count in search_scope_stats.most_common():\n",
    "        print(f\"  {scope}: {count}\")\n",
    "    \n",
    "    if confidence_stats:\n",
    "        avg_confidence = sum(confidence_stats) / len(confidence_stats)\n",
    "        high_confidence = sum(1 for c in confidence_stats if c >= 0.7)\n",
    "        print(f\"\\nConfidence Statistics:\")\n",
    "        print(f\"  Average confidence: {avg_confidence:.3f}\")\n",
    "        print(f\"  High confidence (≥0.7): {high_confidence}/{len(confidence_stats)} ({100*high_confidence/len(confidence_stats):.1f}%)\")\n",
    "    \n",
    "    # Add evidence and aspirational intensity score statistics (NEW SECTION)\n",
    "    print(f\"\\nEvidence & Aspirational Intensity Scores by Document:\")\n",
    "    evidence_scores = []\n",
    "    aspirational_scores = []\n",
    "    print(f\"{'Document':<35} {'Evidence':<12} {'Aspirational':<12}\")\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "    for doc_name, results in all_results.items():\n",
    "        evidence_intensity = results.get('evidence_intensity_score', 0)\n",
    "        aspirational_intensity = results.get('aspirational_intensity_score', 0)\n",
    "        evidence_scores.append(evidence_intensity)\n",
    "        aspirational_scores.append(aspirational_intensity)\n",
    "        print(f\"{doc_name:<35} {evidence_intensity:<12.2f} {aspirational_intensity:<12.2f}\")\n",
    "\n",
    "    if evidence_scores and aspirational_scores:\n",
    "        avg_evidence = sum(evidence_scores) / len(evidence_scores)\n",
    "        avg_aspirational = sum(aspirational_scores) / len(aspirational_scores)\n",
    "        max_evidence = max(evidence_scores)\n",
    "        max_aspirational = max(aspirational_scores)\n",
    "        min_evidence = min(evidence_scores)\n",
    "        min_aspirational = min(aspirational_scores)\n",
    "        \n",
    "        print(f\"\\nEvidence Intensity Statistics:\")\n",
    "        print(f\"  Average evidence intensity: {avg_evidence:.2f}\")\n",
    "        print(f\"  Maximum evidence intensity: {max_evidence:.2f}\")\n",
    "        print(f\"  Minimum evidence intensity: {min_evidence:.2f}\")\n",
    "        \n",
    "        print(f\"\\nAspirational Intensity Statistics:\")\n",
    "        print(f\"  Average aspirational intensity: {avg_aspirational:.2f}\")\n",
    "        print(f\"  Maximum aspirational intensity: {max_aspirational:.2f}\")\n",
    "        print(f\"  Minimum aspirational intensity: {min_aspirational:.2f}\")\n",
    "        \n",
    "        # Evidence intensity distribution\n",
    "        high_evidence = sum(1 for score in evidence_scores if score >= 30)\n",
    "        medium_evidence = sum(1 for score in evidence_scores if 10 <= score < 30)\n",
    "        low_evidence = sum(1 for score in evidence_scores if score < 10)\n",
    "        \n",
    "        # Aspirational intensity distribution  \n",
    "        high_aspirational = sum(1 for score in aspirational_scores if score >= 30)\n",
    "        medium_aspirational = sum(1 for score in aspirational_scores if 10 <= score < 30)\n",
    "        low_aspirational = sum(1 for score in aspirational_scores if score < 10)\n",
    "        \n",
    "        print(f\"\\nEvidence Intensity Distribution:\")\n",
    "        print(f\"  High evidence (≥30): {high_evidence} documents\")\n",
    "        print(f\"  Medium evidence (10-30): {medium_evidence} documents\")\n",
    "        print(f\"  Low evidence (<10): {low_evidence} documents\")\n",
    "        \n",
    "        print(f\"\\nAspirational Intensity Distribution:\")\n",
    "        print(f\"  High aspirational (≥30): {high_aspirational} documents\")\n",
    "        print(f\"  Medium aspirational (10-30): {medium_aspirational} documents\")\n",
    "        print(f\"  Low aspirational (<10): {low_aspirational} documents\")\n",
    "    \n",
    "    return term_type_ea_stats, marker_category_stats, connection_type_stats\n",
    "\n",
    "def print_enhanced_ea_examples_by_document(all_results, documents, examples_per_class=2):\n",
    "    \"\"\"\n",
    "    Print enhanced examples of evidence vs aspirational classifications for each document.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ENHANCED EVIDENCE VS ASPIRATIONAL EXAMPLES BY DOCUMENT\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    classification_order = ['strong_evidence', 'moderate_evidence', 'neutral', \n",
    "                          'moderate_aspirational', 'strong_aspirational']\n",
    "    \n",
    "    for doc_name, results in all_results.items():\n",
    "        evidence_intensity = results.get('evidence_intensity_score', 0)\n",
    "        aspirational_intensity = results.get('aspirational_intensity_score', 0)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"DOCUMENT: {doc_name} (Evidence: {evidence_intensity:.2f}, Aspirational: {aspirational_intensity:.2f})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        doc = documents[doc_name]\n",
    "        valid_terms = results['valid_terms']\n",
    "        \n",
    "        # Group terms by EA classification\n",
    "        terms_by_class = defaultdict(list)\n",
    "        for term in valid_terms:\n",
    "            if 'evidence_aspirational_class' in term:\n",
    "                ea_class = term['evidence_aspirational_class']\n",
    "                terms_by_class[ea_class].append(term)\n",
    "        \n",
    "        # Print summary with term type breakdown\n",
    "        total_terms = len(valid_terms)\n",
    "        print(f\"Total terms: {total_terms}\")\n",
    "        \n",
    "        # Term type summary\n",
    "        term_type_counts = Counter()\n",
    "        for term in valid_terms:\n",
    "            term_type = term.get('ea_term_type', 'unknown')\n",
    "            term_type_counts[term_type] += 1\n",
    "        \n",
    "        print(f\"Term types: \", end=\"\")\n",
    "        for term_type, count in term_type_counts.items():\n",
    "            percentage = (count / total_terms) * 100 if total_terms > 0 else 0\n",
    "            print(f\"{term_type}={count}({percentage:.1f}%) \", end=\"\")\n",
    "        print()\n",
    "        \n",
    "        # EA classification summary\n",
    "        for ea_class in classification_order:\n",
    "            count = len(terms_by_class[ea_class])\n",
    "            percentage = (count / total_terms) * 100 if total_terms > 0 else 0\n",
    "            print(f\"  {ea_class.replace('_', ' ').title()}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Show examples for each classification\n",
    "        for ea_class in classification_order:\n",
    "            class_terms = terms_by_class[ea_class]\n",
    "            if class_terms:\n",
    "                print(f\"\\n{ea_class.replace('_', ' ').upper()} EXAMPLES:\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                examples_to_show = min(examples_per_class, len(class_terms))\n",
    "                \n",
    "                for i, term in enumerate(class_terms[:examples_to_show]):\n",
    "                    sentence = term['sentence']\n",
    "                    sentence_text = sentence.text.strip()\n",
    "                    \n",
    "                    # Highlight the green term\n",
    "                    try:\n",
    "                        start_char = doc[term['start_idx']].idx\n",
    "                        end_char = doc[term['end_idx']].idx + len(doc[term['end_idx']].text)\n",
    "                        sentence_start_char = sentence.start_char\n",
    "                        \n",
    "                        relative_start = start_char - sentence_start_char\n",
    "                        relative_end = end_char - sentence_start_char\n",
    "                        \n",
    "                        if 0 <= relative_start <= len(sentence_text) and 0 <= relative_end <= len(sentence_text):\n",
    "                            highlighted_sentence = (\n",
    "                                sentence_text[:relative_start] + \n",
    "                                f\"**{sentence_text[relative_start:relative_end]}**\" + \n",
    "                                sentence_text[relative_end:]\n",
    "                            )\n",
    "                        else:\n",
    "                            highlighted_sentence = sentence_text\n",
    "                    except:\n",
    "                        highlighted_sentence = sentence_text\n",
    "                    \n",
    "                    print(f\"{i+1}. '{term['term']}' ({term.get('ea_term_type', 'unknown')})\")\n",
    "                    print(f\"   Context: {highlighted_sentence}\")\n",
    "                    print(f\"   Confidence: {term.get('ea_confidence', 0):.3f}\")\n",
    "                    \n",
    "                    # Show enhanced information\n",
    "                    governor_info = term.get('ea_governor_info', {})\n",
    "                    if governor_info.get('semantic_governor'):\n",
    "                        print(f\"   Semantic Governor: '{governor_info['semantic_governor']}' ({governor_info.get('governor_pos', 'unknown')})\")\n",
    "                    \n",
    "                    # Show excluded words for context-dependent terms\n",
    "                    excluded_words = term.get('ea_excluded_words', [])\n",
    "                    if excluded_words:\n",
    "                        print(f\"   Excluded Words: {excluded_words}\")\n",
    "                    \n",
    "                    # Show markers with enhanced details\n",
    "                    evidence_markers = term.get('ea_evidence_markers', [])\n",
    "                    aspirational_markers = term.get('ea_aspirational_markers', [])\n",
    "                    \n",
    "                    if evidence_markers:\n",
    "                        marker_details = []\n",
    "                        for m in evidence_markers:\n",
    "                            scope = m.get('search_scope', 'unknown')\n",
    "                            conn_type = m.get('connection_type', 'unknown')\n",
    "                            marker_details.append(f\"'{m['text']}' ({m['strength']}, {scope}, {conn_type})\")\n",
    "                        print(f\"   Evidence markers: {', '.join(marker_details)}\")\n",
    "                    \n",
    "                    if aspirational_markers:\n",
    "                        marker_details = []\n",
    "                        for m in aspirational_markers:\n",
    "                            scope = m.get('search_scope', 'unknown')\n",
    "                            conn_type = m.get('connection_type', 'unknown')\n",
    "                            marker_details.append(f\"'{m['text']}' ({m['strength']}, {scope}, {conn_type})\")\n",
    "                        print(f\"   Aspirational markers: {', '.join(marker_details)}\")\n",
    "                    \n",
    "                    if not evidence_markers and not aspirational_markers:\n",
    "                        print(f\"   Markers: None (classified as neutral)\")\n",
    "                    \n",
    "                    # Show dependency pattern details for dependency terms\n",
    "                    if term.get('ea_term_type') == 'dependency':\n",
    "                        context_info = term.get('ea_context_info', {})\n",
    "                        pattern_name = context_info.get('pattern_name', 'unknown')\n",
    "                        dependency_relation = context_info.get('dependency_relation', 'unknown')\n",
    "                        print(f\"   Dependency Pattern: {pattern_name} ({dependency_relation})\")\n",
    "                    \n",
    "                    print()\n",
    "                \n",
    "                if len(class_terms) > examples_to_show:\n",
    "                    print(f\"   ... and {len(class_terms) - examples_to_show} more {ea_class.replace('_', ' ')} terms\")\n",
    "\n",
    "def print_classification_comparison(all_results):\n",
    "    \"\"\"\n",
    "    Print comparison between different classification dimensions.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"MULTI-DIMENSIONAL CLASSIFICATION COMPARISON\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Combine temporal, quantification, and evidence/aspirational classifications\n",
    "    combined_stats = defaultdict(Counter)\n",
    "    \n",
    "    for doc_name, results in all_results.items():\n",
    "        for term in results['valid_terms']:\n",
    "            temporal_class = term.get('temporal_class', 'unknown')\n",
    "            quant_level = term.get('quantification_level', 'unknown')\n",
    "            ea_class = term.get('evidence_aspirational_class', 'unknown')\n",
    "            \n",
    "            # Cross-tabulations\n",
    "            combined_stats['temporal_ea'][f\"{temporal_class}_{ea_class}\"] += 1\n",
    "            combined_stats['quant_ea'][f\"{quant_level}_{ea_class}\"] += 1\n",
    "            combined_stats['temporal_quant'][f\"{temporal_class}_{quant_level}\"] += 1\n",
    "    \n",
    "    print(f\"\\nTemporal vs Evidence/Aspirational (Top 10):\")\n",
    "    for combo, count in combined_stats['temporal_ea'].most_common(10):\n",
    "        print(f\"  {combo.replace('_', ' + ')}: {count}\")\n",
    "    \n",
    "    print(f\"\\nQuantification vs Evidence/Aspirational (Top 10):\")\n",
    "    for combo, count in combined_stats['quant_ea'].most_common(10):\n",
    "        print(f\"  {combo.replace('_', ' + ')}: {count}\")\n",
    "    \n",
    "    print(f\"\\nTemporal vs Quantification (Top 10):\")\n",
    "    for combo, count in combined_stats['temporal_quant'].most_common(10):\n",
    "        print(f\"  {combo.replace('_', ' + ')}: {count}\")\n",
    "\n",
    "# Run enhanced analysis\n",
    "term_type_patterns, marker_stats, connection_stats = analyze_enhanced_ea_patterns(all_results)\n",
    "print_enhanced_ea_examples_by_document(all_results, documents, examples_per_class=2)\n",
    "print_classification_comparison(all_results)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ENHANCED EVIDENCE VS ASPIRATIONAL CLASSIFICATION WITH INTENSITY COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"All valid green terms classified with enhanced analysis\")\n",
    "print(\"Special handling for direct, context-dependent, AND dependency pattern terms\")\n",
    "print(\"Targeted dependency-first marker detection (no sentence-wide search)\")\n",
    "print(\"Enhanced connection assessment with role-aware scoring\")\n",
    "print(\"Comprehensive multi-dimensional analysis\")\n",
    "print(\"Individual term classification with detailed confidence and connection info\")\n",
    "print(\"Evidence intensity: ((strong_count * 1.5) + (moderate_count * 1)) / total_terms * 100\")\n",
    "print(\"Aspirational intensity: ((strong_count * 1.5) + (moderate_count * 1)) / total_terms * 100\")\n",
    "print(\"Use intensity scores to compare evidence vs aspirational balance across documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f5866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_classification_dataframe(all_results):\n",
    "    \"\"\"\n",
    "    Create a focused DataFrame for context classification analysis only.\n",
    "    Rows: Organizations (company-year combinations)\n",
    "    Columns: Context classification metrics (temporal, quantification, evidence/aspirational)\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for doc_name, term_data in all_results.items():\n",
    "        \n",
    "        # Extract organization and year from document name\n",
    "        parts = doc_name.split('_')\n",
    "        year = parts[-1]\n",
    "        org_name = '_'.join(parts[:-1])\n",
    "        \n",
    "        # Get valid terms for classification analysis\n",
    "        valid_terms = term_data['valid_terms']\n",
    "        valid_terms_count = len(valid_terms)\n",
    "        \n",
    "        # TEMPORAL CLASSIFICATION METRICS\n",
    "        temporal_stats = {}\n",
    "        for temporal_class in ['past', 'present', 'future', 'unclear']:\n",
    "            count = sum(1 for term in valid_terms if term.get('temporal_class') == temporal_class)\n",
    "            temporal_stats[f'temporal_{temporal_class}'] = count\n",
    "            temporal_stats[f'temporal_{temporal_class}_pct'] = round((count / valid_terms_count * 100) if valid_terms_count > 0 else 0, 2)\n",
    "        \n",
    "        # QUANTIFICATION CLASSIFICATION METRICS\n",
    "        quant_stats = {}\n",
    "        for quant_level in ['highly_quantified', 'partially_quantified', 'non_quantified']:\n",
    "            count = sum(1 for term in valid_terms if term.get('quantification_level') == quant_level)\n",
    "            quant_stats[f'quant_{quant_level}'] = count\n",
    "            quant_stats[f'quant_{quant_level}_pct'] = round((count / valid_terms_count * 100) if valid_terms_count > 0 else 0, 2)\n",
    "        \n",
    "        # QUANTIFICATION CONFIDENCE AND INTENSITY SCORES\n",
    "        quantified_terms = [term for term in valid_terms \n",
    "                          if term.get('quantification_level') in ['highly_quantified', 'partially_quantified']]\n",
    "        if quantified_terms:\n",
    "            quant_confidences = [term.get('quantification_confidence', 0) for term in quantified_terms]\n",
    "            quant_avg_confidence = round(sum(quant_confidences) / len(quant_confidences), 3)\n",
    "        else:\n",
    "            quant_avg_confidence = 0.0\n",
    "        \n",
    "        quant_stats['quant_avg_confidence'] = quant_avg_confidence\n",
    "        quant_stats['quantification_intensity_score'] = round(term_data.get('quantification_stats', {}).get('quantification_intensity_score', 0), 2)        \n",
    "        \n",
    "        # EVIDENCE/ASPIRATIONAL CLASSIFICATION METRICS\n",
    "        ea_stats = {}\n",
    "        for ea_class in ['strong_evidence', 'moderate_evidence', 'neutral', 'moderate_aspirational', 'strong_aspirational']:\n",
    "            count = sum(1 for term in valid_terms if term.get('evidence_aspirational_class') == ea_class)\n",
    "            ea_stats[f'ea_{ea_class}'] = count\n",
    "            ea_stats[f'ea_{ea_class}_pct'] = round((count / valid_terms_count * 100) if valid_terms_count > 0 else 0, 2)\n",
    "        \n",
    "        # EVIDENCE/ASPIRATIONAL CONFIDENCE SCORES\n",
    "        evidence_terms = [term for term in valid_terms \n",
    "                         if term.get('evidence_aspirational_class') in ['strong_evidence', 'moderate_evidence']]\n",
    "        aspirational_terms = [term for term in valid_terms \n",
    "                            if term.get('evidence_aspirational_class') in ['strong_aspirational', 'moderate_aspirational']]\n",
    "        \n",
    "        if evidence_terms:\n",
    "            evidence_confidences = [term.get('ea_confidence', 0) for term in evidence_terms]\n",
    "            evidence_avg_confidence = round(sum(evidence_confidences) / len(evidence_confidences), 3)\n",
    "        else:\n",
    "            evidence_avg_confidence = 0.0\n",
    "            \n",
    "        if aspirational_terms:\n",
    "            aspirational_confidences = [term.get('ea_confidence', 0) for term in aspirational_terms]\n",
    "            aspirational_avg_confidence = round(sum(aspirational_confidences) / len(aspirational_confidences), 3)\n",
    "        else:\n",
    "            aspirational_avg_confidence = 0.0\n",
    "        \n",
    "        ea_stats['evidence_avg_confidence'] = evidence_avg_confidence\n",
    "        ea_stats['aspirational_avg_confidence'] = aspirational_avg_confidence\n",
    "        ea_stats['evidence_intensity_score'] = round(term_data.get('evidence_intensity_score', 0), 2)\n",
    "        ea_stats['aspirational_intensity_score'] = round(term_data.get('aspirational_intensity_score', 0), 2)\n",
    "        \n",
    "        # COMBINED CLASSIFICATION INSIGHTS\n",
    "        # High-impact terms (highly quantified + strong evidence)\n",
    "        high_impact_terms = sum(1 for term in valid_terms \n",
    "                               if term.get('quantification_level') == 'highly_quantified' \n",
    "                               and term.get('evidence_aspirational_class') == 'strong_evidence')\n",
    "        \n",
    "        # Future aspirational terms (future + aspirational)\n",
    "        future_aspirational = sum(1 for term in valid_terms \n",
    "                                 if term.get('temporal_class') == 'future' \n",
    "                                 and term.get('evidence_aspirational_class') in ['moderate_aspirational', 'strong_aspirational'])\n",
    "        \n",
    "        # Past evidence terms (past + evidence)\n",
    "        past_evidence = sum(1 for term in valid_terms \n",
    "                           if term.get('temporal_class') == 'past' \n",
    "                           and term.get('evidence_aspirational_class') in ['moderate_evidence', 'strong_evidence'])\n",
    "        \n",
    "        # Present quantified terms (present + quantified)\n",
    "        present_quantified = sum(1 for term in valid_terms \n",
    "                                if term.get('temporal_class') == 'present' \n",
    "                                and term.get('quantification_level') in ['highly_quantified', 'partially_quantified'])\n",
    "        \n",
    "        # Calculate overall average confidence scores\n",
    "        all_ea_confidences = [term.get('ea_confidence', 0) for term in valid_terms if 'ea_confidence' in term]\n",
    "        avg_ea_confidence = round(sum(all_ea_confidences) / len(all_ea_confidences), 3) if all_ea_confidences else 0\n",
    "        \n",
    "        # Create the row dictionary\n",
    "        row = {\n",
    "            # Basic identifiers\n",
    "            'organization': org_name,\n",
    "            'year': int(year),\n",
    "            \n",
    "            # Temporal classification metrics\n",
    "            **temporal_stats,\n",
    "            \n",
    "            # Quantification classification metrics (counts, percentages, confidence, intensity)\n",
    "            **quant_stats,\n",
    "            \n",
    "            # Evidence/Aspirational classification metrics (counts, percentages, confidences, intensities)\n",
    "            **ea_stats,\n",
    "            \n",
    "            # Combined insights\n",
    "            'high_impact_terms': high_impact_terms,\n",
    "            'future_aspirational': future_aspirational,\n",
    "            'past_evidence': past_evidence,\n",
    "            'present_quantified': present_quantified,\n",
    "            'avg_ea_confidence': avg_ea_confidence\n",
    "        }\n",
    "        \n",
    "        data.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    context_classification_df = pd.DataFrame(data)\n",
    "    \n",
    "    # Sort by organization and year\n",
    "    context_classification_df = context_classification_df.sort_values(['organization', 'year'])\n",
    "    \n",
    "    return context_classification_df\n",
    "\n",
    "# Create the DataFrame\n",
    "context_classification_df = create_context_classification_dataframe(all_results)\n",
    "\n",
    "# Save to Excel\n",
    "excel_path = \"data/NLP/Results/Communication_Score_df_Context.xlsx\"\n",
    "context_classification_df.to_excel(excel_path, index=False)\n",
    "\n",
    "print(\"CONTEXT CLASSIFICATION DATAFRAME CREATED\")\n",
    "print(\"=\"*80)\n",
    "print(context_classification_df.head())\n",
    "\n",
    "print(f\"\\nDataFrame shape: {context_classification_df.shape}\")\n",
    "print(f\"Columns ({len(context_classification_df.columns)}):\")\n",
    "\n",
    "# Column descriptions in logical order\n",
    "column_descriptions = {\n",
    "    # Basic info\n",
    "    'organization': 'Organization name',\n",
    "    'year': 'Report year',\n",
    "    \n",
    "    # Temporal classification (counts and percentages)\n",
    "    'temporal_past': 'Past temporal context terms',\n",
    "    'temporal_past_pct': 'Past temporal terms percentage',\n",
    "    'temporal_present': 'Present temporal context terms', \n",
    "    'temporal_present_pct': 'Present temporal terms percentage',\n",
    "    'temporal_future': 'Future temporal context terms',\n",
    "    'temporal_future_pct': 'Future temporal terms percentage',\n",
    "    'temporal_unclear': 'Terms with unclear temporal context',\n",
    "    'temporal_unclear_pct': 'Unclear temporal terms percentage',\n",
    "    \n",
    "    # Quantification classification (counts, percentages, confidence, intensity)\n",
    "    'quant_highly_quantified': 'Highly quantified terms',\n",
    "    'quant_highly_quantified_pct': 'Highly quantified percentage',\n",
    "    'quant_partially_quantified': 'Partially quantified terms',\n",
    "    'quant_partially_quantified_pct': 'Partially quantified percentage',\n",
    "    'quant_non_quantified': 'Non-quantified terms',\n",
    "    'quant_non_quantified_pct': 'Non-quantified percentage',\n",
    "    'quant_avg_confidence': 'Average confidence for quantified terms (highly + partially)',\n",
    "    'quantification_intensity_score': 'Quantification intensity score ((highly*1.5 + partially*1)/total*100)',\n",
    "    \n",
    "    # Evidence/Aspirational classification (counts, percentages, confidences, intensities)\n",
    "    'ea_strong_evidence': 'Strong evidence terms',\n",
    "    'ea_strong_evidence_pct': 'Strong evidence percentage',\n",
    "    'ea_moderate_evidence': 'Moderate evidence terms',\n",
    "    'ea_moderate_evidence_pct': 'Moderate evidence percentage',\n",
    "    'ea_neutral': 'Neutral terms',\n",
    "    'ea_neutral_pct': 'Neutral terms percentage',\n",
    "    'ea_moderate_aspirational': 'Moderate aspirational terms',\n",
    "    'ea_moderate_aspirational_pct': 'Moderate aspirational percentage',\n",
    "    'ea_strong_aspirational': 'Strong aspirational terms',\n",
    "    'ea_strong_aspirational_pct': 'Strong aspirational percentage',\n",
    "    'evidence_avg_confidence': 'Average confidence for evidence terms (strong + moderate)',\n",
    "    'aspirational_avg_confidence': 'Average confidence for aspirational terms (strong + moderate)',\n",
    "    'evidence_intensity_score': 'Evidence intensity score ((strong*1.5 + moderate*1)/total*100)',\n",
    "    'aspirational_intensity_score': 'Aspirational intensity score ((strong*1.5 + moderate*1)/total*100)',\n",
    "    \n",
    "    # Combined insights\n",
    "    'high_impact_terms': 'Highly quantified + strong evidence terms',\n",
    "    'future_aspirational': 'Future + aspirational terms',\n",
    "    'past_evidence': 'Past + evidence terms',\n",
    "    'present_quantified': 'Present + quantified terms',\n",
    "    'avg_ea_confidence': 'Average evidence/aspirational confidence (all terms)'\n",
    "}\n",
    "\n",
    "for col, desc in column_descriptions.items():\n",
    "    if col in context_classification_df.columns:\n",
    "        print(f\"  {col:<35}: {desc}\")\n",
    "\n",
    "print(f\"\\nData saved as: {excel_path}\")\n",
    "print(f\"Variable available as: context_classification_df\")\n",
    "print(\"Contains ONLY context classification insights (temporal, quantification, evidence/aspirational)\")\n",
    "print(\"NEW: Added confidence scores for quantified and evidence/aspirational terms\")\n",
    "print(\"NEW: Added intensity scores for quantification, evidence, and aspirational classifications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c520d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Define file path and output path\n",
    "output_path = \"data/NLP/Results/Communication_Score_df_Context.xlsx\"\n",
    "\n",
    "# Save the DataFrame to Excel\n",
    "context_classification_df.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "# Load the workbook and sheet\n",
    "wb = load_workbook(output_path)\n",
    "ws = wb.active  # There's only one sheet since we saved just one DataFrame\n",
    "\n",
    "# Auto-adjust column widths based on the longest string in each column\n",
    "for col in ws.columns:\n",
    "    max_length = 0\n",
    "    col_letter = get_column_letter(col[0].column)\n",
    "    for cell in col:\n",
    "        if cell.value:\n",
    "            max_length = max(max_length, len(str(cell.value)))\n",
    "    ws.column_dimensions[col_letter].width = max_length + 3  # Add padding\n",
    "\n",
    "# Define grey fill for alternating rows\n",
    "grey_fill = PatternFill(start_color=\"D9D9D9\", end_color=\"D9D9D9\", fill_type=\"solid\")\n",
    "\n",
    "# Alternate row colors by company\n",
    "prev_company = None\n",
    "use_grey = False\n",
    "for row in range(2, ws.max_row + 1):\n",
    "    current_company = ws[f\"A{row}\"].value  # Column A has the company names\n",
    "    if current_company != prev_company:\n",
    "        use_grey = not use_grey\n",
    "        prev_company = current_company\n",
    "\n",
    "    if use_grey:\n",
    "        for col in range(1, ws.max_column + 1):\n",
    "            ws.cell(row=row, column=col).fill = grey_fill\n",
    "\n",
    "# Save the final cleaned and formatted workbook\n",
    "wb.save(output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
